{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27d0c8b-ebf8-43d9-a35e-f0e78c743f19",
   "metadata": {},
   "source": [
    "Environment Check and Prerequisites\n",
    "==========================================\n",
    "Before we start, let's understand what we're building:\n",
    "\n",
    "1. We have some mock financial data CSV files)\n",
    "2. We need to move this data into a \"real\" database (PostgreSQL)\n",
    "3. We'll learn SQL by querying our own financial data\n",
    "4. This mirrors real FinTech companies' data pipelines\n",
    "\n",
    "What you'll learn:\n",
    "- How to connect Python to PostgreSQL\n",
    "- How to import CSV files into database tables\n",
    "- How to validate data quality\n",
    "- How to write SQL queries for financial analysis\n",
    "\n",
    "Prerequisites Check:\n",
    "- PostgreSQL installed (version 14+)\n",
    "- pgAdmin installed (comes with PostgreSQL)\n",
    "- Python packages: pandas, numpy, psycopg2, sqlalchemy\n",
    "- Your generated data in 'mock_financial_data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01d8c19-dd27-47c0-b59d-5435c842572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python version: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n",
      "üìä Pandas version: 2.2.3\n",
      "üî¢ NumPy version: 2.2.6\n",
      "\n",
      "‚ùå Data directory not found!\n",
      "Please run Week 0 data generation first\n",
      "Run: generator.save_all_datasets()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Check Python version\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "\n",
    "# Check if we have the synthetic data \n",
    "data_dir = 'mock_financial_data'\n",
    "if os.path.exists(data_dir):\n",
    "    print(f\"\\n‚úÖ Found data directory: {data_dir}\")\n",
    "    files = os.listdir(data_dir)\n",
    "    print(f\"üìÅ Files available: {len(files)}\")\n",
    "    for file in sorted(files):\n",
    "        if file.endswith('.csv'):\n",
    "            size_mb = os.path.getsize(os.path.join(data_dir, file)) / (1024*1024)\n",
    "            print(f\"   - {file}: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Data directory not found!\")\n",
    "    print(\"Please run Week 0 data generation first\")\n",
    "    print(\"Run: generator.save_all_datasets()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97723fd5-3efb-46c0-a40b-07362903cf7b",
   "metadata": {},
   "source": [
    "Installing Database Connection Libraries\n",
    "===============================================\n",
    "We need special Python packages to talk to PostgreSQL:\n",
    "\n",
    "1. psycopg2: Low-level PostgreSQL adapter (like a translator)\n",
    "2. sqlalchemy: High-level database toolkit (makes complex operations easier)\n",
    "\n",
    "Think of it like:\n",
    "- psycopg2 = manual transmission (more control, more complex)\n",
    "- sqlalchemy = automatic transmission (easier to use, less control)\n",
    "\n",
    "If installation fails:\n",
    "- Windows: You might need Visual C++ build tools\n",
    "- Mac: You might need to install PostgreSQL first\n",
    "- Linux: You might need postgresql-dev package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33883c2f-4f41-41e1-9e04-01abc8d890f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Checking required packages...\n",
      "\n",
      "üì¶ Installing psycopg2-binary...\n",
      "‚úÖ psycopg2-binary installed successfully\n",
      "‚úÖ sqlalchemy already installed\n",
      "üì¶ Installing python-dotenv...\n",
      "‚úÖ python-dotenv installed successfully\n",
      "\n",
      "‚úÖ All required packages are ready!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Helper function to install packages\"\"\"\n",
    "    try:\n",
    "        __import__(package_name.replace('-', '_'))\n",
    "        print(f\"‚úÖ {package_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"‚úÖ {package_name} installed successfully\")\n",
    "\n",
    "# Install required packages\n",
    "packages_needed = [\n",
    "    'psycopg2-binary',  # PostgreSQL adapter\n",
    "    'sqlalchemy',       # SQL toolkit\n",
    "    'python-dotenv'     # For secure password storage\n",
    "]\n",
    "\n",
    "print(\"üîß Checking required packages...\\n\")\n",
    "for package in packages_needed:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n‚úÖ All required packages are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc46e5-ed8b-4023-941e-37f2281432c2",
   "metadata": {},
   "source": [
    "Setting Up Database Connection\n",
    "=====================================\n",
    "\n",
    "IMPORTANT: Database connections are like phone calls:\n",
    "1. You need the right \"phone number\" (host, port, database name)\n",
    "2. You need to \"authenticate\" yourself (username, password)\n",
    "3. The connection can \"drop\" if not used properly\n",
    "4. Always \"hang up\" (close connection) when done\n",
    "\n",
    "Security Note:\n",
    "- NEVER hardcode passwords in your code\n",
    "- NEVER commit passwords to GitHub\n",
    "- Use environment variables or config files\n",
    "\n",
    "Let's create a safe connection setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a16ab92-dd2a-4a42-bc20-767385c75e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê PostgreSQL Connection Setup\n",
      "Host: localhost\n",
      "Port: 5432\n",
      "Database: fintech_db\n",
      "User: postgres\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter PostgreSQL password:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Connected to PostgreSQL!\n",
      "üìä Version: PostgreSQL 17.5 on x86_64-windows\n",
      "\n",
      "üéâ Database connection successful!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import getpass  # For secure password input\n",
    "\n",
    "# Database configuration class\n",
    "class DatabaseConfig:\n",
    "    \"\"\"\n",
    "    Stores database connection parameters.\n",
    "    In production, these would come from environment variables.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.host = 'localhost'      # Your computer\n",
    "        self.port = 5432            # Default PostgreSQL port\n",
    "        self.database = 'fintech_db' # Database name we'll create\n",
    "        self.user = 'postgres'       # Default PostgreSQL user\n",
    "        \n",
    "        # Secure password input\n",
    "        print(\"üîê PostgreSQL Connection Setup\")\n",
    "        print(f\"Host: {self.host}\")\n",
    "        print(f\"Port: {self.port}\")\n",
    "        print(f\"Database: {self.database}\")\n",
    "        print(f\"User: {self.user}\")\n",
    "        \n",
    "        # Get password securely (won't show on screen)\n",
    "        self.password = getpass.getpass(\"Enter PostgreSQL password: \")\n",
    "        \n",
    "    def get_connection_string(self):\n",
    "        \"\"\"Create connection string for SQLAlchemy\"\"\"\n",
    "        return f\"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}\"\n",
    "    \n",
    "    def get_connection_params(self):\n",
    "        \"\"\"Get parameters for psycopg2\"\"\"\n",
    "        return {\n",
    "            'host': self.host,\n",
    "            'port': self.port,\n",
    "            'database': self.database,\n",
    "            'user': self.user,\n",
    "            'password': self.password\n",
    "        }\n",
    "\n",
    "# Create configuration\n",
    "db_config = DatabaseConfig()\n",
    "\n",
    "# Test connection\n",
    "def test_connection(config):\n",
    "    \"\"\"Test if we can connect to PostgreSQL\"\"\"\n",
    "    try:\n",
    "        # First, connect to PostgreSQL server (not specific database)\n",
    "        conn_params = config.get_connection_params()\n",
    "        conn_params['database'] = 'postgres'  # Default database\n",
    "        \n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Check PostgreSQL version\n",
    "        cur.execute(\"SELECT version();\")\n",
    "        version = cur.fetchone()[0]\n",
    "        print(f\"\\n‚úÖ Connected to PostgreSQL!\")\n",
    "        print(f\"üìä Version: {version.split(',')[0]}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Connection failed: {e}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Is PostgreSQL running? Check pgAdmin\")\n",
    "        print(\"2. Is the password correct?\")\n",
    "        print(\"3. Is the port 5432 free?\")\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "if test_connection(db_config):\n",
    "    print(\"\\nüéâ Database connection successful!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please fix connection issues before proceeding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df99d3-c1c6-4b55-9cd8-46e265968070",
   "metadata": {},
   "source": [
    "Creating Our FinTech Database\n",
    "====================================\n",
    "\n",
    "Databases are like filing cabinets:\n",
    "- PostgreSQL server = The entire office\n",
    "- Database = One filing cabinet  \n",
    "- Tables = Drawers in the cabinet\n",
    "- Rows = Individual files in drawers\n",
    "\n",
    "We'll create a dedicated database for our FinTech project.\n",
    "This keeps our data organized and separate from other projects.\n",
    "\n",
    "Important Concepts:\n",
    "- CREATE DATABASE: Makes a new database\n",
    "- DROP DATABASE: Deletes a database (careful!)\n",
    "- IF EXISTS / IF NOT EXISTS: Prevents errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d5d5a9-bd9d-429d-804b-b6f5306865a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Database 'fintech_db' already exists\n",
      "üìè Current size: 27 MB\n",
      "\n",
      "üìã All databases on this server:\n",
      "   - fintech_db: 27 MB\n",
      "   - postgres: 7987 kB\n",
      "\n",
      "‚úÖ Database setup complete!\n"
     ]
    }
   ],
   "source": [
    "def create_fintech_database(config):\n",
    "    \"\"\"\n",
    "    Create the fintech_db database if it doesn't exist.\n",
    "    This is like creating a new filing cabinet for our project.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to PostgreSQL server (not a specific database)\n",
    "        conn_params = config.get_connection_params()\n",
    "        conn_params['database'] = 'postgres'  # Default database\n",
    "        \n",
    "        # Important: autocommit=True for CREATE DATABASE\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        conn.autocommit = True  # Required for CREATE DATABASE\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Check if database exists\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT 1 FROM pg_database WHERE datname = %s\n",
    "        \"\"\", (config.database,))\n",
    "        \n",
    "        exists = cur.fetchone()\n",
    "        \n",
    "        if exists:\n",
    "            print(f\"üìä Database '{config.database}' already exists\")\n",
    "            \n",
    "            # Get database size\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT pg_size_pretty(pg_database_size('{config.database}'))\n",
    "            \"\"\")\n",
    "            size = cur.fetchone()[0]\n",
    "            print(f\"üìè Current size: {size}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"üèóÔ∏è Creating database '{config.database}'...\")\n",
    "            cur.execute(f\"CREATE DATABASE {config.database}\")\n",
    "            print(f\"‚úÖ Database '{config.database}' created successfully!\")\n",
    "        \n",
    "        # List all databases (for learning purposes)\n",
    "        print(\"\\nüìã All databases on this server:\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT datname, pg_size_pretty(pg_database_size(datname)) as size\n",
    "            FROM pg_database \n",
    "            WHERE datistemplate = false\n",
    "            ORDER BY datname;\n",
    "        \"\"\")\n",
    "        \n",
    "        for db_name, db_size in cur.fetchall():\n",
    "            print(f\"   - {db_name}: {db_size}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating database: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create our database\n",
    "if create_fintech_database(db_config):\n",
    "    print(\"\\n‚úÖ Database setup complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Database creation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391717b4-ad60-41cb-a4e1-cccdd08a5774",
   "metadata": {},
   "source": [
    "Designing Database Tables (Schemas)\n",
    "==========================================\n",
    "\n",
    "A table schema is like a blueprint for data storage.\n",
    "Just like a form has specific fields, tables have columns with specific types.\n",
    "\n",
    "Data Types in PostgreSQL:\n",
    "- INTEGER: Whole numbers (-2B to +2B)\n",
    "- BIGINT: Very large whole numbers\n",
    "- NUMERIC(10,2): Decimal with 10 total digits, 2 after decimal\n",
    "- VARCHAR(50): Text up to 50 characters\n",
    "- TEXT: Unlimited text\n",
    "- DATE: Calendar date (no time)\n",
    "- TIMESTAMP: Date + time\n",
    "- BOOLEAN: True/False\n",
    "\n",
    "Primary Keys:\n",
    "- Unique identifier for each row\n",
    "- Like a social security number for data\n",
    "- Can be one column or combination of columns\n",
    "\n",
    "Indexes:\n",
    "- Like a book's index - helps find data faster\n",
    "- Trade-off: Faster reads, slower writes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655ffa8b-490e-4781-894b-9875b93342dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing CSV files to design table schemas...\n",
      "\n",
      "üìÑ Analyzing: crypto_prices.csv\n",
      "--------------------------------------------------\n",
      "Columns: 7\n",
      "Sample rows: 5\n",
      "\n",
      "Column Analysis:\n",
      "  - Timestamp            object     ‚Üí TIMESTAMP       (sample: 2020-01-01 00:00:00)\n",
      "  - Symbol               object     ‚Üí VARCHAR(4)      (sample: BTC)\n",
      "  - Open                 float64    ‚Üí NUMERIC(20,4)   (sample: 55610.37)\n",
      "  - High                 float64    ‚Üí NUMERIC(20,4)   (sample: 61577.1)\n",
      "  - Low                  float64    ‚Üí NUMERIC(20,4)   (sample: 55610.37)\n",
      "  - Close                float64    ‚Üí NUMERIC(20,4)   (sample: 60842.74)\n",
      "  - Volume               int64      ‚Üí INTEGER         (sample: 57397)\n",
      "\n",
      "\n",
      "üìÑ Analyzing: customer_data.csv\n",
      "--------------------------------------------------\n",
      "Columns: 11\n",
      "Sample rows: 5\n",
      "\n",
      "Column Analysis:\n",
      "  - CustomerID           object     ‚Üí VARCHAR(16)     (sample: CUST_000001)\n",
      "  - Age                  int64      ‚Üí INTEGER         (sample: 18)\n",
      "  - Income               float64    ‚Üí NUMERIC(20,4)   (sample: 46881.45)\n",
      "  - CreditScore          int64      ‚Üí INTEGER         (sample: 712)\n",
      "  - AccountAgeDays       int64      ‚Üí INTEGER         (sample: 1858)\n",
      "  - AccountBalance       float64    ‚Üí NUMERIC(20,4)   (sample: 6722.32)\n",
      "  - MonthlyTransactions  int64      ‚Üí INTEGER         (sample: 31)\n",
      "  - AvgTransactionAmount float64    ‚Üí NUMERIC(20,4)   (sample: 20.54)\n",
      "  - NumProducts          int64      ‚Üí INTEGER         (sample: 1)\n",
      "  - HasLoan              bool       ‚Üí BOOLEAN         (sample: False)\n",
      "  - RiskSegment          object     ‚Üí VARCHAR(9)      (sample: Medium)\n",
      "\n",
      "\n",
      "üìÑ Analyzing: economic_indicators.csv\n",
      "--------------------------------------------------\n",
      "Columns: 3\n",
      "Sample rows: 5\n",
      "\n",
      "Column Analysis:\n",
      "  - Date                 object     ‚Üí DATE            (sample: 2020-01-31)\n",
      "  - Indicator            object     ‚Üí VARCHAR(28)     (sample: GDP_GROWTH)\n",
      "  - Value                float64    ‚Üí NUMERIC(20,8)   (sample: 2.78)\n",
      "\n",
      "\n",
      "üìÑ Analyzing: portfolio_data.csv\n",
      "--------------------------------------------------\n",
      "Columns: 8\n",
      "Sample rows: 5\n",
      "\n",
      "Column Analysis:\n",
      "  - Date                 object     ‚Üí DATE            (sample: 2020-01-31)\n",
      "  - PortfolioID          object     ‚Üí VARCHAR(9)      (sample: PF_001)\n",
      "  - RiskLevel            object     ‚Üí VARCHAR(18)     (sample: Conservative)\n",
      "  - TotalValue           float64    ‚Üí NUMERIC(20,8)   (sample: 1864562.35)\n",
      "  - StockWeight          float64    ‚Üí NUMERIC(7,4)    (sample: 0.337)\n",
      "  - BondWeight           float64    ‚Üí NUMERIC(7,4)    (sample: 0.599)\n",
      "  - CashWeight           float64    ‚Üí NUMERIC(7,4)    (sample: 0.064)\n",
      "  - MonthlyReturn        float64    ‚Üí NUMERIC(7,4)    (sample: -0.0228)\n",
      "\n",
      "\n",
      "üìÑ Analyzing: stock_prices.csv\n",
      "--------------------------------------------------\n",
      "Columns: 7\n",
      "Sample rows: 5\n",
      "\n",
      "Column Analysis:\n",
      "  - Date                 object     ‚Üí DATE            (sample: 2020-01-01)\n",
      "  - Symbol               object     ‚Üí VARCHAR(6)      (sample: AAPL)\n",
      "  - Open                 float64    ‚Üí NUMERIC(20,4)   (sample: 24.56)\n",
      "  - High                 float64    ‚Üí NUMERIC(20,4)   (sample: 24.86)\n",
      "  - Low                  float64    ‚Üí NUMERIC(20,4)   (sample: 24.56)\n",
      "  - Close                float64    ‚Üí NUMERIC(20,4)   (sample: 24.84)\n",
      "  - Volume               int64      ‚Üí INTEGER         (sample: 1701200)\n",
      "\n",
      "\n",
      "‚úÖ Schema analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Let's examine what tables we need\n",
    "def analyze_csv_structure(data_dir='mock_financial_data'):\n",
    "    \"\"\"\n",
    "    Analyze our CSV files to understand what tables we need.\n",
    "    This helps us design appropriate database schemas.\n",
    "    \"\"\"\n",
    "    print(\"üìä Analyzing CSV files to design table schemas...\\n\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    table_designs = {}\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        print(f\"üìÑ Analyzing: {csv_file}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Read first few rows to understand structure\n",
    "        df = pd.read_csv(os.path.join(data_dir, csv_file), nrows=5)\n",
    "        \n",
    "        # Analyze columns\n",
    "        print(f\"Columns: {len(df.columns)}\")\n",
    "        print(f\"Sample rows: {len(df)}\")\n",
    "        print(\"\\nColumn Analysis:\")\n",
    "        \n",
    "        table_name = csv_file.replace('.csv', '').replace('_data', 's')\n",
    "        columns_info = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # Determine PostgreSQL data type\n",
    "            dtype = str(df[col].dtype)\n",
    "            sample_value = df[col].iloc[0] if len(df) > 0 else None\n",
    "            \n",
    "            if 'date' in col.lower() or 'time' in col.lower():\n",
    "                if 'timestamp' in col.lower():\n",
    "                    pg_type = 'TIMESTAMP'\n",
    "                else:\n",
    "                    pg_type = 'DATE'\n",
    "            elif dtype == 'object':\n",
    "                # Check if it's a string column\n",
    "                max_len = df[col].astype(str).str.len().max()\n",
    "                if max_len <= 50:\n",
    "                    pg_type = f'VARCHAR({int(max_len * 1.5)})'  # Add buffer\n",
    "                else:\n",
    "                    pg_type = 'TEXT'\n",
    "            elif 'int' in dtype:\n",
    "                if df[col].max() > 2147483647:  # Max INT value\n",
    "                    pg_type = 'BIGINT'\n",
    "                else:\n",
    "                    pg_type = 'INTEGER'\n",
    "            elif 'float' in dtype:\n",
    "                # Determine precision needed\n",
    "                if 'price' in col.lower() or 'value' in col.lower():\n",
    "                    pg_type = 'NUMERIC(20,8)'  # High precision for prices\n",
    "                elif 'weight' in col.lower() or 'return' in col.lower():\n",
    "                    pg_type = 'NUMERIC(7,4)'   # Percentages\n",
    "                else:\n",
    "                    pg_type = 'NUMERIC(20,4)'  # General decimal\n",
    "            elif 'bool' in dtype:\n",
    "                pg_type = 'BOOLEAN'\n",
    "            else:\n",
    "                pg_type = 'TEXT'  # Fallback\n",
    "            \n",
    "            columns_info.append({\n",
    "                'name': col,\n",
    "                'pandas_type': dtype,\n",
    "                'postgres_type': pg_type,\n",
    "                'sample': sample_value\n",
    "            })\n",
    "            \n",
    "            print(f\"  - {col:20} {dtype:10} ‚Üí {pg_type:15} (sample: {sample_value})\")\n",
    "        \n",
    "        table_designs[table_name] = columns_info\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return table_designs\n",
    "\n",
    "# Analyze our data\n",
    "table_designs = analyze_csv_structure()\n",
    "print(\"‚úÖ Schema analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca273c-bc25-4cf0-a507-db46b032e7e0",
   "metadata": {},
   "source": [
    "Cell 6: Creating Tables in PostgreSQL\n",
    "====================================\n",
    "\n",
    "Now we'll create the actual tables in our database.\n",
    "This is like setting up the drawers in our filing cabinet.\n",
    "\n",
    "SQL Commands we'll use:\n",
    "- CREATE TABLE: Makes a new table\n",
    "- PRIMARY KEY: Unique identifier for rows\n",
    "- NOT NULL: This column cannot be empty\n",
    "- CREATE INDEX: Speed up searches\n",
    "\n",
    "Best Practices:\n",
    "1. Always include created_at timestamp\n",
    "2. Use meaningful column names\n",
    "3. Choose appropriate data types\n",
    "4. Add indexes for frequently searched columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b1038e-4ef1-4683-b4c5-a60049fbcf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating database tables...\n",
      "\n",
      "üìä Creating table: stock_prices\n",
      "   Description: Daily stock price data (OHLCV format)\n",
      "   ‚úÖ Table created successfully\n",
      "   üìã Columns: 8\n",
      "\n",
      "üìä Creating table: crypto_prices\n",
      "   Description: 6-hourly cryptocurrency price data\n",
      "   ‚úÖ Table created successfully\n",
      "   üìã Columns: 8\n",
      "\n",
      "üìä Creating table: economic_indicators\n",
      "   Description: Monthly macroeconomic indicators\n",
      "   ‚úÖ Table created successfully\n",
      "   üìã Columns: 5\n",
      "\n",
      "üìä Creating table: portfolio_holdings\n",
      "   Description: Monthly portfolio snapshots\n",
      "   ‚úÖ Table created successfully\n",
      "   üìã Columns: 9\n",
      "\n",
      "üìä Creating table: customers\n",
      "   Description: Customer demographics and account information\n",
      "   ‚úÖ Table created successfully\n",
      "   üìã Columns: 12\n",
      "\n",
      "\n",
      "üìä Database Schema Summary:\n",
      "------------------------------------------------------------\n",
      "Table: crypto_prices        Columns: 8\n",
      "Table: customers            Columns: 12\n",
      "Table: daily_returns        Columns: 6\n",
      "Table: economic_cycle_indicators Columns: 6\n",
      "Table: economic_indicators  Columns: 5\n",
      "Table: economic_indicators_pivot Columns: 11\n",
      "Table: economic_monthly_summary Columns: 4\n",
      "Table: market_summary       Columns: 10\n",
      "Table: market_summary_fixed Columns: 10\n",
      "Table: moving_averages_fixed Columns: 8\n",
      "Table: portfolio_holdings   Columns: 9\n",
      "Table: return_pairs         Columns: 5\n",
      "Table: stock_correlation_view Columns: 4\n",
      "Table: stock_correlations   Columns: 7\n",
      "Table: stock_prices         Columns: 8\n",
      "Table: volatility_metrics_fixed Columns: 5\n",
      "\n",
      "‚úÖ All tables created successfully!\n",
      "üéâ Database schema is ready!\n"
     ]
    }
   ],
   "source": [
    "def create_tables(config):\n",
    "    \"\"\"\n",
    "    Create all necessary tables for our FinTech project.\n",
    "    Each table is designed for specific financial data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Table definitions with explanations\n",
    "    tables = {\n",
    "        'stock_prices': {\n",
    "            'description': 'Daily stock price data (OHLCV format)',\n",
    "            'sql': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS stock_prices (\n",
    "                    -- Primary key columns (unique identifier)\n",
    "                    date DATE NOT NULL,\n",
    "                    symbol VARCHAR(10) NOT NULL,\n",
    "                    \n",
    "                    -- Price data (OHLCV)\n",
    "                    open NUMERIC(10,2),      -- Opening price\n",
    "                    high NUMERIC(10,2),      -- Highest price of day\n",
    "                    low NUMERIC(10,2),       -- Lowest price of day\n",
    "                    close NUMERIC(10,2),     -- Closing price\n",
    "                    volume BIGINT,           -- Number of shares traded\n",
    "                    \n",
    "                    -- Metadata\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    \n",
    "                    -- Composite primary key (date + symbol must be unique)\n",
    "                    PRIMARY KEY (date, symbol)\n",
    "                );\n",
    "                \n",
    "                -- Indexes for faster queries\n",
    "                CREATE INDEX IF NOT EXISTS idx_stock_symbol ON stock_prices(symbol);\n",
    "                CREATE INDEX IF NOT EXISTS idx_stock_date ON stock_prices(date);\n",
    "                \n",
    "                -- Add comments for documentation\n",
    "                COMMENT ON TABLE stock_prices IS 'Daily stock price data in OHLCV format';\n",
    "                COMMENT ON COLUMN stock_prices.symbol IS 'Stock ticker symbol (e.g., AAPL, GOOGL)';\n",
    "                COMMENT ON COLUMN stock_prices.volume IS 'Number of shares traded during the day';\n",
    "            \"\"\"\n",
    "        },\n",
    "        \n",
    "        'crypto_prices': {\n",
    "            'description': '6-hourly cryptocurrency price data',\n",
    "            'sql': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS crypto_prices (\n",
    "                    -- Crypto trades 24/7, so we use timestamp\n",
    "                    timestamp TIMESTAMP NOT NULL,\n",
    "                    symbol VARCHAR(10) NOT NULL,\n",
    "                    \n",
    "                    -- Price data (higher precision for crypto)\n",
    "                    open NUMERIC(20,8),      -- 8 decimals for small altcoins\n",
    "                    high NUMERIC(20,8),\n",
    "                    low NUMERIC(20,8),\n",
    "                    close NUMERIC(20,8),\n",
    "                    volume BIGINT,\n",
    "                    \n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    PRIMARY KEY (timestamp, symbol)\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_crypto_symbol ON crypto_prices(symbol);\n",
    "                CREATE INDEX IF NOT EXISTS idx_crypto_timestamp ON crypto_prices(timestamp);\n",
    "                \n",
    "                COMMENT ON TABLE crypto_prices IS '6-hourly cryptocurrency price data';\n",
    "                COMMENT ON COLUMN crypto_prices.timestamp IS 'UTC timestamp of the price snapshot';\n",
    "            \"\"\"\n",
    "        },\n",
    "        \n",
    "        'economic_indicators': {\n",
    "            'description': 'Monthly macroeconomic indicators',\n",
    "            'sql': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS economic_indicators (\n",
    "                    date DATE NOT NULL,\n",
    "                    indicator VARCHAR(50) NOT NULL,\n",
    "                    value NUMERIC(20,4),\n",
    "                    unit VARCHAR(20),        -- %, USD, points, etc.\n",
    "                    \n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    PRIMARY KEY (date, indicator)\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_econ_indicator ON economic_indicators(indicator);\n",
    "                CREATE INDEX IF NOT EXISTS idx_econ_date ON economic_indicators(date);\n",
    "                \n",
    "                COMMENT ON TABLE economic_indicators IS 'Monthly macroeconomic indicators';\n",
    "                COMMENT ON COLUMN economic_indicators.indicator IS 'Indicator name (e.g., GDP_GROWTH, INFLATION_RATE)';\n",
    "                COMMENT ON COLUMN economic_indicators.unit IS 'Unit of measurement';\n",
    "            \"\"\"\n",
    "        },\n",
    "        \n",
    "        'portfolio_holdings': {\n",
    "            'description': 'Monthly portfolio snapshots',\n",
    "            'sql': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS portfolio_holdings (\n",
    "                    date DATE NOT NULL,\n",
    "                    portfolio_id VARCHAR(20) NOT NULL,\n",
    "                    \n",
    "                    -- Portfolio characteristics\n",
    "                    risk_level VARCHAR(20),   -- Conservative/Moderate/Aggressive\n",
    "                    total_value NUMERIC(20,2),\n",
    "                    \n",
    "                    -- Asset allocation (must sum to 1.0)\n",
    "                    stock_weight NUMERIC(5,4),  -- 0.0000 to 1.0000\n",
    "                    bond_weight NUMERIC(5,4),\n",
    "                    cash_weight NUMERIC(5,4),\n",
    "                    \n",
    "                    -- Performance\n",
    "                    monthly_return NUMERIC(7,4),  -- -0.9999 to 9.9999\n",
    "                    \n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    PRIMARY KEY (date, portfolio_id)\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_portfolio_id ON portfolio_holdings(portfolio_id);\n",
    "                CREATE INDEX IF NOT EXISTS idx_portfolio_risk ON portfolio_holdings(risk_level);\n",
    "                \n",
    "                COMMENT ON TABLE portfolio_holdings IS 'Monthly portfolio performance and allocation data';\n",
    "            \"\"\"\n",
    "        },\n",
    "        \n",
    "        'customers': {\n",
    "            'description': 'Customer demographics and account information',\n",
    "            'sql': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS customers (\n",
    "                    -- Customer identity\n",
    "                    customer_id VARCHAR(20) PRIMARY KEY,\n",
    "                    \n",
    "                    -- Demographics\n",
    "                    age INTEGER CHECK (age >= 18 AND age <= 120),\n",
    "                    income NUMERIC(12,2) CHECK (income >= 0),\n",
    "                    credit_score INTEGER CHECK (credit_score >= 300 AND credit_score <= 850),\n",
    "                    \n",
    "                    -- Account information\n",
    "                    account_age_days INTEGER CHECK (account_age_days >= 0),\n",
    "                    account_balance NUMERIC(20,2),\n",
    "                    \n",
    "                    -- Behavior metrics\n",
    "                    monthly_transactions INTEGER CHECK (monthly_transactions >= 0),\n",
    "                    avg_transaction_amount NUMERIC(12,2),\n",
    "                    num_products INTEGER CHECK (num_products >= 0),\n",
    "                    has_loan BOOLEAN,\n",
    "                    \n",
    "                    -- Risk classification\n",
    "                    risk_segment VARCHAR(20),\n",
    "                    \n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_customer_risk ON customers(risk_segment);\n",
    "                CREATE INDEX IF NOT EXISTS idx_customer_credit ON customers(credit_score);\n",
    "                \n",
    "                COMMENT ON TABLE customers IS 'Customer profiles for analytics and risk assessment';\n",
    "                COMMENT ON COLUMN customers.credit_score IS 'FICO credit score (300-850 range)';\n",
    "            \"\"\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Connect to our fintech database\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"üèóÔ∏è Creating database tables...\\n\")\n",
    "        \n",
    "        for table_name, table_info in tables.items():\n",
    "            print(f\"üìä Creating table: {table_name}\")\n",
    "            print(f\"   Description: {table_info['description']}\")\n",
    "            \n",
    "            # Execute the CREATE TABLE statement\n",
    "            cur.execute(table_info['sql'])\n",
    "            \n",
    "            # Check if table was created successfully\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT COUNT(*) \n",
    "                FROM information_schema.tables \n",
    "                WHERE table_schema = 'public' \n",
    "                AND table_name = %s\n",
    "            \"\"\", (table_name,))\n",
    "            \n",
    "            if cur.fetchone()[0] > 0:\n",
    "                print(f\"   ‚úÖ Table created successfully\")\n",
    "                \n",
    "                # Get column count\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT COUNT(*) \n",
    "                    FROM information_schema.columns \n",
    "                    WHERE table_name = %s\n",
    "                \"\"\", (table_name,))\n",
    "                col_count = cur.fetchone()[0]\n",
    "                print(f\"   üìã Columns: {col_count}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Table creation failed\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        # Commit all changes\n",
    "        conn.commit()\n",
    "        \n",
    "        # Show summary of created tables\n",
    "        print(\"\\nüìä Database Schema Summary:\")\n",
    "        print(\"-\" * 60)\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                table_name,\n",
    "                COUNT(*) as column_count\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema = 'public'\n",
    "            GROUP BY table_name\n",
    "            ORDER BY table_name;\n",
    "        \"\"\")\n",
    "        \n",
    "        for table, col_count in cur.fetchall():\n",
    "            print(f\"Table: {table:20} Columns: {col_count}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(\"\\n‚úÖ All tables created successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating tables: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        return False\n",
    "\n",
    "# Create all tables\n",
    "if create_tables(db_config):\n",
    "    print(\"üéâ Database schema is ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please fix table creation errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5610bff-efa7-4459-8810-c90e33456b9f",
   "metadata": {},
   "source": [
    "Understanding the Data Import Process\n",
    "============================================\n",
    "\n",
    "Before we import data, let's understand what we're doing:\n",
    "\n",
    "CSV Files ‚Üí PostgreSQL Tables\n",
    "\n",
    "Challenges:\n",
    "1. Data types might not match\n",
    "2. Dates need special formatting\n",
    "3. NULL values need handling\n",
    "4. Large files need batch processing\n",
    "\n",
    "Methods for importing data:\n",
    "1. COPY command (fastest, but less flexible)\n",
    "2. INSERT statements (slow, but most control)\n",
    "3. pandas.to_sql (balanced approach)\n",
    "\n",
    "We'll use pandas.to_sql because:\n",
    "- It handles data type conversion\n",
    "- It can append to existing tables\n",
    "- It provides progress feedback\n",
    "- It's easier to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae88414-86b7-4e62-ae96-01d628cff7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Previewing all CSV files before import...\n",
      "\n",
      "üìÑ Previewing: stock_prices.csv\n",
      "============================================================\n",
      "Total rows: 26,100\n",
      "Columns: ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "\n",
      "First 5 rows:\n",
      "         Date Symbol   Open   High    Low  Close   Volume\n",
      "0  2020-01-01   AAPL  24.56  24.86  24.56  24.84  1701200\n",
      "1  2020-01-02   AAPL  24.93  24.93  23.82  24.11  3978279\n",
      "2  2020-01-03   AAPL  24.11  24.36  24.11  24.33  7701730\n",
      "3  2020-01-06   AAPL  24.20  24.35  24.18  24.19  7669587\n",
      "4  2020-01-07   AAPL  24.11  24.11  23.60  23.61  1400184\n",
      "\n",
      "Data Quality Checks:\n",
      "‚úÖ No NULL values in sample\n",
      "\n",
      "Data types:\n",
      "  Date: object\n",
      "  Symbol: object\n",
      "  Open: float64\n",
      "  High: float64\n",
      "  Low: float64\n",
      "  Close: float64\n",
      "  Volume: int64\n",
      "\n",
      "Estimated memory needed: 4.44 MB\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next file... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Previewing: crypto_prices.csv\n",
      "============================================================\n",
      "Total rows: 73,050\n",
      "Columns: ['Timestamp', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "\n",
      "First 5 rows:\n",
      "             Timestamp Symbol      Open      High       Low     Close  Volume\n",
      "0  2020-01-01 00:00:00    BTC  55610.37  61577.10  55610.37  60842.74   57397\n",
      "1  2020-01-01 06:00:00    BTC  58654.92  61403.00  58654.92  61398.09  178177\n",
      "2  2020-01-01 12:00:00    BTC  60507.59  60507.59  59462.07  59701.21   59879\n",
      "3  2020-01-01 18:00:00    BTC  61069.08  61069.08  57124.83  58855.20  178901\n",
      "4  2020-01-02 00:00:00    BTC  57630.54  61751.48  57630.54  59970.54   67811\n",
      "\n",
      "Data Quality Checks:\n",
      "‚úÖ No NULL values in sample\n",
      "\n",
      "Data types:\n",
      "  Timestamp: object\n",
      "  Symbol: object\n",
      "  Open: float64\n",
      "  High: float64\n",
      "  Low: float64\n",
      "  Close: float64\n",
      "  Volume: int64\n",
      "\n",
      "Estimated memory needed: 12.99 MB\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next file... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Previewing: economic_indicators.csv\n",
      "============================================================\n",
      "Total rows: 600\n",
      "Columns: ['Date', 'Indicator', 'Value']\n",
      "\n",
      "First 5 rows:\n",
      "         Date            Indicator   Value\n",
      "0  2020-01-31           GDP_GROWTH    2.78\n",
      "1  2020-01-31       INFLATION_RATE    1.75\n",
      "2  2020-01-31    UNEMPLOYMENT_RATE    5.24\n",
      "3  2020-01-31        INTEREST_RATE    1.66\n",
      "4  2020-01-31  CONSUMER_CONFIDENCE  100.13\n",
      "\n",
      "Data Quality Checks:\n",
      "‚úÖ No NULL values in sample\n",
      "\n",
      "Data types:\n",
      "  Date: object\n",
      "  Indicator: object\n",
      "  Value: float64\n",
      "\n",
      "Estimated memory needed: 0.09 MB\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next file... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Previewing: portfolio_data.csv\n",
      "============================================================\n",
      "Total rows: 6,000\n",
      "Columns: ['Date', 'PortfolioID', 'RiskLevel', 'TotalValue', 'StockWeight', 'BondWeight', 'CashWeight', 'MonthlyReturn']\n",
      "\n",
      "First 5 rows:\n",
      "         Date PortfolioID     RiskLevel  TotalValue  StockWeight  BondWeight  \\\n",
      "0  2020-01-31      PF_001  Conservative  1864562.35        0.337       0.599   \n",
      "1  2020-02-29      PF_001  Conservative  1850585.81        0.331       0.606   \n",
      "2  2020-03-31      PF_001  Conservative  1825414.18        0.348       0.589   \n",
      "3  2020-04-30      PF_001  Conservative  1786687.35        0.352       0.588   \n",
      "4  2020-05-31      PF_001  Conservative  1854394.63        0.355       0.583   \n",
      "\n",
      "   CashWeight  MonthlyReturn  \n",
      "0       0.064        -0.0228  \n",
      "1       0.063        -0.0075  \n",
      "2       0.064        -0.0136  \n",
      "3       0.060        -0.0212  \n",
      "4       0.062         0.0379  \n",
      "\n",
      "Data Quality Checks:\n",
      "‚úÖ No NULL values in sample\n",
      "\n",
      "Data types:\n",
      "  Date: object\n",
      "  PortfolioID: object\n",
      "  RiskLevel: object\n",
      "  TotalValue: float64\n",
      "  StockWeight: float64\n",
      "  BondWeight: float64\n",
      "  CashWeight: float64\n",
      "  MonthlyReturn: float64\n",
      "\n",
      "Estimated memory needed: 1.38 MB\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next file... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Previewing: customer_data.csv\n",
      "============================================================\n",
      "Total rows: 10,000\n",
      "Columns: ['CustomerID', 'Age', 'Income', 'CreditScore', 'AccountAgeDays', 'AccountBalance', 'MonthlyTransactions', 'AvgTransactionAmount', 'NumProducts', 'HasLoan', 'RiskSegment']\n",
      "\n",
      "First 5 rows:\n",
      "    CustomerID  Age    Income  CreditScore  AccountAgeDays  AccountBalance  \\\n",
      "0  CUST_000001   18  46881.45          712            1858         6722.32   \n",
      "1  CUST_000002   28  32164.17          570             473         6837.84   \n",
      "2  CUST_000003   28  80756.05          822            1470         7014.52   \n",
      "3  CUST_000004   18  55980.56          708            1204         3961.80   \n",
      "4  CUST_000005   18  28709.23          476             902          554.23   \n",
      "\n",
      "   MonthlyTransactions  AvgTransactionAmount  NumProducts  HasLoan RiskSegment  \n",
      "0                   31                 20.54            1    False      Medium  \n",
      "1                   33                358.30            1    False        High  \n",
      "2                   45                 53.58            2    False         Low  \n",
      "3                   43                 78.24            1    False      Medium  \n",
      "4                   34                 25.67            1    False        High  \n",
      "\n",
      "Data Quality Checks:\n",
      "‚úÖ No NULL values in sample\n",
      "\n",
      "Data types:\n",
      "  CustomerID: object\n",
      "  Age: int64\n",
      "  Income: float64\n",
      "  CreditScore: int64\n",
      "  AccountAgeDays: int64\n",
      "  AccountBalance: float64\n",
      "  MonthlyTransactions: int64\n",
      "  AvgTransactionAmount: float64\n",
      "  NumProducts: int64\n",
      "  HasLoan: bool\n",
      "  RiskSegment: object\n",
      "\n",
      "Estimated memory needed: 1.96 MB\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next file... \n"
     ]
    }
   ],
   "source": [
    "def preview_csv_data(csv_file, data_dir='mock_financial_data', rows=5):\n",
    "    \"\"\"\n",
    "    Preview CSV data before import to spot potential issues.\n",
    "    \n",
    "    \"\"\"\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"üìÑ Previewing: {csv_file}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Read first few rows\n",
    "    df = pd.read_csv(filepath, nrows=rows)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_rows = sum(1 for line in open(filepath)) - 1  # Subtract header\n",
    "    \n",
    "    print(f\"Total rows: {total_rows:,}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst {rows} rows:\")\n",
    "    print(df)\n",
    "    \n",
    "    # Check for potential issues\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_counts = df.isnull().sum()\n",
    "    if null_counts.any():\n",
    "        print(\"‚ö†Ô∏è Found NULL values:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "    else:\n",
    "        print(\"‚úÖ No NULL values in sample\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\nData types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    estimated_total = memory_usage * (total_rows / rows)\n",
    "    print(f\"\\nEstimated memory needed: {estimated_total:.2f} MB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preview each CSV file\n",
    "csv_files = [\n",
    "    'stock_prices.csv',\n",
    "    'crypto_prices.csv', \n",
    "    'economic_indicators.csv',\n",
    "    'portfolio_data.csv',\n",
    "    'customer_data.csv'\n",
    "]\n",
    "\n",
    "print(\"üîç Previewing all CSV files before import...\\n\")\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if os.path.exists(os.path.join('mock_financial_data', csv_file)):\n",
    "        preview_df = preview_csv_data(csv_file)\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "        # Wait for user to review\n",
    "        input(\"Press Enter to continue to next file...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06faa3eb-6424-4b24-884c-51ba2d2274d4",
   "metadata": {},
   "source": [
    "Importing Stock Price Data\n",
    "==================================\n",
    "\n",
    "Let's start with stock prices - our most important dataset.\n",
    "We'll import it step by step with proper error handling.\n",
    "\n",
    "Key considerations:\n",
    "1. Date formatting (pandas ‚Üí PostgreSQL)\n",
    "2. Handling duplicates\n",
    "3. Transaction management\n",
    "4. Progress tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4689c41-a6e1-413c-99cf-38ecd3b55681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Importing stock prices (skip existing)...\n",
      "üìà Importing Stock Price Data\n",
      "============================================================\n",
      "Step 1: Loading CSV file...\n",
      "‚úÖ Loaded 26,100 rows\n",
      "\n",
      "Step 2: Preprocessing data...\n",
      "\n",
      "Step 3: Validating data...\n",
      "‚úÖ All OHLC values are valid\n",
      "‚úÖ No negative prices found\n",
      "\n",
      "Step 4: Checking for existing data...\n",
      "üìä Found 26,100 existing records in database\n",
      "üîç Checking for overlapping data...\n",
      "‚è≠Ô∏è Skipping 26,100 records that already exist\n",
      "üì• Will import 0 new records\n",
      "‚ÑπÔ∏è No new data to import\n",
      "\n",
      "üéâ Stock price import complete!\n",
      "\n",
      "üîÑ Alternative: Replace existing data...\n",
      "Uncomment the line below if you want to replace all existing data:\n",
      "# import_stock_prices(db_config, replace_existing=True)\n",
      "\n",
      "üîç Quick Data Verification:\n",
      "----------------------------------------\n",
      "Records: 26,100\n",
      "Symbols: 20\n",
      "Date range: 2020-01-01 to 2024-12-31\n",
      "Average closing price: $310.35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "def import_stock_prices(config, data_dir='mock_financial_data', replace_existing=False):\n",
    "    \"\"\"\n",
    "    Import stock price data with proper duplicate handling.\n",
    "    \n",
    "    Args:\n",
    "        config: Database configuration object\n",
    "        data_dir: Directory containing CSV files\n",
    "        replace_existing: If True, clear existing data before import\n",
    "    \"\"\"\n",
    "    csv_file = 'stock_prices.csv'\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"üìà Importing Stock Price Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load the CSV file\n",
    "        print(\"Step 1: Loading CSV file...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "        \n",
    "        # Step 2: Data preprocessing\n",
    "        print(\"\\nStep 2: Preprocessing data...\")\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        df['date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.drop('Date', axis=1)  # Remove original column\n",
    "        \n",
    "        # Ensure column names match database (lowercase)\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Check for duplicates in the CSV\n",
    "        duplicates = df.duplicated(subset=['date', 'symbol'])\n",
    "        if duplicates.any():\n",
    "            print(f\"‚ö†Ô∏è Found {duplicates.sum()} duplicate rows in CSV\")\n",
    "            df = df[~duplicates]\n",
    "            print(f\"‚úÖ Removed duplicates, {len(df):,} rows remaining\")\n",
    "        \n",
    "        # Step 3: Data validation\n",
    "        print(\"\\nStep 3: Validating data...\")\n",
    "        \n",
    "        # Check OHLC logic (High >= Low, etc.)\n",
    "        invalid_ohlc = df[\n",
    "            (df['high'] < df['low']) |\n",
    "            (df['high'] < df['open']) |\n",
    "            (df['high'] < df['close']) |\n",
    "            (df['low'] > df['open']) |\n",
    "            (df['low'] > df['close'])\n",
    "        ]\n",
    "        \n",
    "        if len(invalid_ohlc) > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {len(invalid_ohlc)} rows with invalid OHLC\")\n",
    "            print(\"First few invalid rows:\")\n",
    "            print(invalid_ohlc.head())\n",
    "        else:\n",
    "            print(\"‚úÖ All OHLC values are valid\")\n",
    "        \n",
    "        # Check for negative prices\n",
    "        negative_prices = df[(df[['open', 'high', 'low', 'close']] < 0).any(axis=1)]\n",
    "        if len(negative_prices) > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {len(negative_prices)} rows with negative prices\")\n",
    "        else:\n",
    "            print(\"‚úÖ No negative prices found\")\n",
    "        \n",
    "        # Step 4: Handle existing data\n",
    "        print(f\"\\nStep 4: Checking for existing data...\")\n",
    "        \n",
    "        # Connect to database to check existing records\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Check if table exists and has data\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT COUNT(*) FROM stock_prices\n",
    "        \"\"\")\n",
    "        existing_count = cur.fetchone()[0]\n",
    "        \n",
    "        if existing_count > 0:\n",
    "            print(f\"üìä Found {existing_count:,} existing records in database\")\n",
    "            \n",
    "            if replace_existing:\n",
    "                print(\"üóëÔ∏è Clearing existing data (replace_existing=True)...\")\n",
    "                cur.execute(\"DELETE FROM stock_prices\")\n",
    "                conn.commit()\n",
    "                print(\"‚úÖ Existing data cleared\")\n",
    "            else:\n",
    "                print(\"üîç Checking for overlapping data...\")\n",
    "                \n",
    "                # Get existing date/symbol combinations\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT DISTINCT date, symbol \n",
    "                    FROM stock_prices\n",
    "                \"\"\")\n",
    "                existing_combinations = set(cur.fetchall())\n",
    "                \n",
    "                # Filter out records that already exist\n",
    "                df['temp_tuple'] = list(zip(df['date'].dt.date, df['symbol']))\n",
    "                df_filtered = df[~df['temp_tuple'].isin(existing_combinations)]\n",
    "                df_filtered = df_filtered.drop('temp_tuple', axis=1)\n",
    "                \n",
    "                if len(df_filtered) < len(df):\n",
    "                    skipped = len(df) - len(df_filtered)\n",
    "                    print(f\"‚è≠Ô∏è Skipping {skipped:,} records that already exist\")\n",
    "                    print(f\"üì• Will import {len(df_filtered):,} new records\")\n",
    "                    df = df_filtered\n",
    "                else:\n",
    "                    print(\"‚úÖ No overlapping data found\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        # Skip import if no new data\n",
    "        if len(df) == 0:\n",
    "            print(\"‚ÑπÔ∏è No new data to import\")\n",
    "            return True\n",
    "        \n",
    "        # Step 5: Import to database\n",
    "        print(f\"\\nStep 5: Importing {len(df):,} records to PostgreSQL...\")\n",
    "        \n",
    "        # Create SQLAlchemy engine\n",
    "        engine = create_engine(config.get_connection_string())\n",
    "        \n",
    "        # Import in chunks for better performance\n",
    "        chunk_size = 5000\n",
    "        total_chunks = (len(df) // chunk_size) + 1\n",
    "        \n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            \n",
    "            try:\n",
    "                chunk.to_sql(\n",
    "                    'stock_prices',\n",
    "                    engine,\n",
    "                    if_exists='append',\n",
    "                    index=False,\n",
    "                    method='multi'\n",
    "                )\n",
    "                \n",
    "                # Progress update\n",
    "                current_chunk = (i // chunk_size) + 1\n",
    "                progress = (i + len(chunk)) / len(df) * 100\n",
    "                print(f\"  Chunk {current_chunk}/{total_chunks}: {progress:.1f}% complete\")\n",
    "                \n",
    "            except Exception as chunk_error:\n",
    "                print(f\"‚ùå Error in chunk {current_chunk}: {chunk_error}\")\n",
    "                # Continue with next chunk or handle as needed\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully imported {len(df):,} stock price records\")\n",
    "        \n",
    "        # Step 6: Verify import\n",
    "        print(\"\\nStep 6: Verifying import...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Count total rows\n",
    "        cur.execute(\"SELECT COUNT(*) FROM stock_prices\")\n",
    "        db_count = cur.fetchone()[0]\n",
    "        \n",
    "        # Get date range and symbol count\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT MIN(date), MAX(date), COUNT(DISTINCT symbol)\n",
    "            FROM stock_prices\n",
    "        \"\"\")\n",
    "        min_date, max_date, symbol_count = cur.fetchone()\n",
    "        \n",
    "        print(f\"Database summary:\")\n",
    "        print(f\"  Total rows: {db_count:,}\")\n",
    "        print(f\"  Date range: {min_date} to {max_date}\")\n",
    "        print(f\"  Unique symbols: {symbol_count}\")\n",
    "        \n",
    "        # Sample recent data\n",
    "        print(\"\\nSample recent data from database:\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT date, symbol, open, high, low, close, volume\n",
    "            FROM stock_prices\n",
    "            ORDER BY date DESC, symbol\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            print(f\"  {row}\")\n",
    "            \n",
    "        # Data quality check\n",
    "        print(\"\\nData quality verification:\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(CASE WHEN volume = 0 THEN 1 END) as zero_volume,\n",
    "                COUNT(CASE WHEN high < low THEN 1 END) as invalid_ohlc\n",
    "            FROM stock_prices\n",
    "        \"\"\")\n",
    "        total_records, zero_volume, invalid_ohlc = cur.fetchone()\n",
    "        \n",
    "        print(f\"  Total records: {total_records:,}\")\n",
    "        print(f\"  Zero volume records: {zero_volume:,}\")\n",
    "        print(f\"  Invalid OHLC records: {invalid_ohlc:,}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Import failed: {e}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Check if stock_prices table exists\")\n",
    "        print(\"2. Verify column names match database schema\")\n",
    "        print(\"3. Check date format compatibility\")\n",
    "        print(\"4. Ensure sufficient disk space\")\n",
    "        print(\"5. Try with replace_existing=True to clear existing data\")\n",
    "        return False\n",
    "\n",
    "# Usage examples:\n",
    "\n",
    "# Option 1: Skip existing records (default behavior)\n",
    "print(\"üîÑ Importing stock prices (skip existing)...\")\n",
    "if import_stock_prices(db_config):\n",
    "    print(\"\\nüéâ Stock price import complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please check stock import errors\")\n",
    "\n",
    "# Option 2: Replace all existing data\n",
    "print(\"\\nüîÑ Alternative: Replace existing data...\")\n",
    "print(\"Uncomment the line below if you want to replace all existing data:\")\n",
    "print(\"# import_stock_prices(db_config, replace_existing=True)\")\n",
    "\n",
    "# Quick verification query\n",
    "def verify_stock_data(config):\n",
    "    \"\"\"Quick verification of imported stock data\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"\\nüîç Quick Data Verification:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Basic stats\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as records,\n",
    "                COUNT(DISTINCT symbol) as symbols,\n",
    "                MIN(date) as first_date,\n",
    "                MAX(date) as last_date,\n",
    "                ROUND(AVG(close), 2) as avg_close_price\n",
    "            FROM stock_prices\n",
    "        \"\"\")\n",
    "        \n",
    "        result = cur.fetchone()\n",
    "        print(f\"Records: {result[0]:,}\")\n",
    "        print(f\"Symbols: {result[1]}\")\n",
    "        print(f\"Date range: {result[2]} to {result[3]}\")\n",
    "        print(f\"Average closing price: ${result[4]}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run verification\n",
    "verify_stock_data(db_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccaa824-28ae-47be-8103-49e723f79d3d",
   "metadata": {},
   "source": [
    "Importing Cryptocurrency Price Data\n",
    "==========================================\n",
    "\n",
    "Cryptocurrency data has special characteristics:\n",
    "1. Timestamp instead of date (24/7 trading)\n",
    "2. Higher price precision (8 decimal places)\n",
    "3. More volatile (larger price swings)\n",
    "4. Different symbols (BTC, ETH vs AAPL, GOOGL)\n",
    "\n",
    "Let's handle these differences properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf1c934-43f4-49c6-9558-630bab255117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Importing crypto prices (skip existing)...\n",
      "üíé Importing Cryptocurrency Price Data\n",
      "============================================================\n",
      "Step 1: Loading crypto data...\n",
      "‚úÖ Loaded 73,050 rows\n",
      "\n",
      "Sample data:\n",
      "             Timestamp Symbol      Open      High       Low     Close  Volume\n",
      "0  2020-01-01 00:00:00    BTC  55610.37  61577.10  55610.37  60842.74   57397\n",
      "1  2020-01-01 06:00:00    BTC  58654.92  61403.00  58654.92  61398.09  178177\n",
      "2  2020-01-01 12:00:00    BTC  60507.59  60507.59  59462.07  59701.21   59879\n",
      "\n",
      "Step 2: Preprocessing crypto data...\n",
      "Most common time interval: 0 days 06:00:00\n",
      "\n",
      "Step 3: Checking for existing data...\n",
      "üìä Found 73,050 existing crypto records in database\n",
      "üîç Checking for overlapping crypto data...\n",
      "‚è≠Ô∏è Skipping 73,050 crypto records that already exist\n",
      "üì• Will import 0 new crypto records\n",
      "‚ÑπÔ∏è No new crypto data to import\n",
      "\n",
      "üéâ Crypto price import complete!\n",
      "\n",
      "üîÑ Alternative: Replace existing crypto data...\n",
      "Uncomment the line below if you want to replace all existing crypto data:\n",
      "# import_crypto_prices(db_config, replace_existing=True)\n",
      "\n",
      "üîç Quick Crypto Data Verification:\n",
      "---------------------------------------------\n",
      "Total Records: 73,050\n",
      "Unique Symbols: 10\n",
      "Time Range: 2020-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "Trading Days: 1,827\n",
      "Average Price (All Cryptos): $2737.4930\n",
      "\n",
      "24/7 Trading Verification:\n",
      "Hours with data: 4/24\n",
      "‚ö†Ô∏è Missing some hourly data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def import_crypto_prices(config, data_dir='mock_financial_data', replace_existing=False):\n",
    "    \"\"\"\n",
    "    Import cryptocurrency price data with proper duplicate handling\n",
    "    for timestamps and high precision values.\n",
    "    \n",
    "    Args:\n",
    "        config: Database configuration object\n",
    "        data_dir: Directory containing CSV files\n",
    "        replace_existing: If True, clear existing data before import\n",
    "    \"\"\"\n",
    "    csv_file = 'crypto_prices.csv'\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"üíé Importing Cryptocurrency Price Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load CSV\n",
    "        print(\"Step 1: Loading crypto data...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "        \n",
    "        # Show sample to understand structure\n",
    "        print(\"\\nSample data:\")\n",
    "        print(df.head(3))\n",
    "        \n",
    "        # Step 2: Preprocessing\n",
    "        print(\"\\nStep 2: Preprocessing crypto data...\")\n",
    "        \n",
    "        # Convert timestamp and ensure lowercase columns\n",
    "        df['timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df = df.drop('Timestamp', axis=1)\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Check for duplicates in CSV\n",
    "        csv_duplicates = df.duplicated(subset=['timestamp', 'symbol'])\n",
    "        if csv_duplicates.any():\n",
    "            print(f\"‚ö†Ô∏è Found {csv_duplicates.sum()} duplicate rows in CSV\")\n",
    "            df = df[~csv_duplicates]\n",
    "            print(f\"‚úÖ Removed CSV duplicates, {len(df):,} rows remaining\")\n",
    "        \n",
    "        # Check timestamp frequency (should be 6-hourly)\n",
    "        time_diffs = df.groupby('symbol')['timestamp'].diff()\n",
    "        most_common_diff = time_diffs.mode()\n",
    "        if len(most_common_diff) > 0:\n",
    "            print(f\"Most common time interval: {most_common_diff[0]}\")\n",
    "        \n",
    "        # Step 3: Handle existing data\n",
    "        print(f\"\\nStep 3: Checking for existing data...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Check if table exists and has data\n",
    "        try:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM crypto_prices\")\n",
    "            existing_count = cur.fetchone()[0]\n",
    "        except psycopg2.Error:\n",
    "            existing_count = 0\n",
    "            print(\"‚ÑπÔ∏è crypto_prices table doesn't exist yet\")\n",
    "        \n",
    "        if existing_count > 0:\n",
    "            print(f\"üìä Found {existing_count:,} existing crypto records in database\")\n",
    "            \n",
    "            if replace_existing:\n",
    "                print(\"üóëÔ∏è Clearing existing crypto data (replace_existing=True)...\")\n",
    "                cur.execute(\"DELETE FROM crypto_prices\")\n",
    "                conn.commit()\n",
    "                print(\"‚úÖ Existing crypto data cleared\")\n",
    "            else:\n",
    "                print(\"üîç Checking for overlapping crypto data...\")\n",
    "                \n",
    "                # Get existing timestamp/symbol combinations\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT DISTINCT timestamp, symbol \n",
    "                    FROM crypto_prices\n",
    "                \"\"\")\n",
    "                existing_combinations = set(cur.fetchall())\n",
    "                \n",
    "                # Filter out records that already exist\n",
    "                df['temp_tuple'] = list(zip(df['timestamp'], df['symbol']))\n",
    "                df_filtered = df[~df['temp_tuple'].isin(existing_combinations)]\n",
    "                df_filtered = df_filtered.drop('temp_tuple', axis=1)\n",
    "                \n",
    "                if len(df_filtered) < len(df):\n",
    "                    skipped = len(df) - len(df_filtered)\n",
    "                    print(f\"‚è≠Ô∏è Skipping {skipped:,} crypto records that already exist\")\n",
    "                    print(f\"üì• Will import {len(df_filtered):,} new crypto records\")\n",
    "                    df = df_filtered\n",
    "                else:\n",
    "                    print(\"‚úÖ No overlapping crypto data found\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        # Skip import if no new data\n",
    "        if len(df) == 0:\n",
    "            print(\"‚ÑπÔ∏è No new crypto data to import\")\n",
    "            return True\n",
    "        \n",
    "        # Step 4: Crypto-specific validation\n",
    "        print(f\"\\nStep 4: Crypto-specific validation on {len(df):,} records...\")\n",
    "        \n",
    "        # Check for extreme volatility (>50% in 6 hours)\n",
    "        df['price_change_pct'] = df.groupby('symbol')['close'].pct_change()\n",
    "        extreme_moves = df[df['price_change_pct'].abs() > 0.5]\n",
    "        \n",
    "        if len(extreme_moves) > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {len(extreme_moves)} extreme price moves (>50% in 6 hours)\")\n",
    "            # Show top 3 extreme moves\n",
    "            top_moves = extreme_moves.nlargest(3, 'price_change_pct')[['timestamp', 'symbol', 'close', 'price_change_pct']]\n",
    "            for _, row in top_moves.iterrows():\n",
    "                print(f\"   {row['symbol']} on {row['timestamp']}: {row['price_change_pct']:.1%} change\")\n",
    "        else:\n",
    "            print(\"‚úÖ No extreme price movements detected\")\n",
    "        \n",
    "        # Remove temporary column\n",
    "        df = df.drop('price_change_pct', axis=1)\n",
    "        \n",
    "        # Check price precision requirements\n",
    "        print(\"\\nPrice precision analysis:\")\n",
    "        precision_check = []\n",
    "        for symbol in df['symbol'].unique()[:5]:  # Check first 5 symbols\n",
    "            symbol_data = df[df['symbol'] == symbol]\n",
    "            max_decimals = symbol_data['close'].apply(\n",
    "                lambda x: len(str(x).split('.')[-1]) if '.' in str(x) else 0\n",
    "            ).max()\n",
    "            avg_price = symbol_data['close'].mean()\n",
    "            precision_check.append((symbol, avg_price, max_decimals))\n",
    "            print(f\"  {symbol}: avg price ${avg_price:.2f}, max decimals: {max_decimals}\")\n",
    "        \n",
    "        # Step 5: Import to database\n",
    "        print(f\"\\nStep 5: Importing {len(df):,} records to PostgreSQL...\")\n",
    "        \n",
    "        engine = create_engine(config.get_connection_string())\n",
    "        \n",
    "        # Import in chunks (crypto data can be large)\n",
    "        chunk_size = 10000\n",
    "        total_rows = len(df)\n",
    "        total_chunks = (total_rows // chunk_size) + 1\n",
    "        \n",
    "        for i in range(0, total_rows, chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            \n",
    "            try:\n",
    "                chunk.to_sql(\n",
    "                    'crypto_prices',\n",
    "                    engine,\n",
    "                    if_exists='append',\n",
    "                    index=False,\n",
    "                    method='multi'\n",
    "                )\n",
    "                \n",
    "                # Progress bar\n",
    "                progress = min((i + len(chunk)) / total_rows * 100, 100)\n",
    "                bar_length = 30\n",
    "                filled = int(bar_length * progress / 100)\n",
    "                bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)\n",
    "                current_chunk = (i // chunk_size) + 1\n",
    "                print(f\"\\r  Progress: [{bar}] {progress:.1f}% (Chunk {current_chunk}/{total_chunks})\", end='')\n",
    "                \n",
    "            except Exception as chunk_error:\n",
    "                print(f\"\\n‚ùå Error in chunk {current_chunk}: {chunk_error}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully imported {len(df):,} crypto price records\")\n",
    "        \n",
    "        # Step 6: Verification and analysis\n",
    "        print(\"\\nStep 6: Verification and analysis...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Get summary statistics by symbol\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                symbol,\n",
    "                COUNT(*) as data_points,\n",
    "                MIN(timestamp) as first_data,\n",
    "                MAX(timestamp) as last_data,\n",
    "                ROUND(AVG(close), 2) as avg_price,\n",
    "                ROUND(STDDEV(close), 2) as price_volatility,\n",
    "                MIN(close) as min_price,\n",
    "                MAX(close) as max_price\n",
    "            FROM crypto_prices\n",
    "            GROUP BY symbol\n",
    "            ORDER BY avg_price DESC\n",
    "            LIMIT 10\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nCrypto Summary by Symbol (Top 10 by Avg Price):\")\n",
    "        print(f\"{'Symbol':>6} {'Points':>7} {'First Data':>12} {'Last Data':>12} {'Avg Price':>12} {'Volatility':>12}\")\n",
    "        print(\"-\" * 85)\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            symbol, points, first, last, avg_price, volatility, min_price, max_price = row\n",
    "            first_str = first.strftime('%Y-%m-%d') if first else 'N/A'\n",
    "            last_str = last.strftime('%Y-%m-%d') if last else 'N/A'\n",
    "            print(f\"{symbol:>6} {points:>7,} {first_str:>12} {last_str:>12} ${avg_price:>11,.2f} ${volatility or 0:>11,.2f}\")\n",
    "        \n",
    "        # Additional crypto-specific metrics\n",
    "        print(\"\\nCrypto Market Analysis:\")\n",
    "        \n",
    "        # 24/7 trading verification\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(DISTINCT DATE(timestamp)) as unique_days,\n",
    "                COUNT(DISTINCT EXTRACT(hour FROM timestamp)) as unique_hours\n",
    "            FROM crypto_prices\n",
    "        \"\"\")\n",
    "        total_records, unique_days, unique_hours = cur.fetchone()\n",
    "        \n",
    "        print(f\"  Total records: {total_records:,}\")\n",
    "        print(f\"  Unique trading days: {unique_days:,}\")\n",
    "        print(f\"  Unique hours of day: {unique_hours} (24/7 trading verification)\")\n",
    "        \n",
    "        # Price range analysis\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                symbol,\n",
    "                MIN(close) as min_price,\n",
    "                MAX(close) as max_price,\n",
    "                ROUND((MAX(close) / MIN(close) - 1) * 100, 1) as total_return_pct\n",
    "            FROM crypto_prices\n",
    "            GROUP BY symbol\n",
    "            ORDER BY total_return_pct DESC\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nTop 5 Crypto Total Returns (Min to Max Price):\")\n",
    "        for row in cur.fetchall():\n",
    "            symbol, min_price, max_price, total_return = row\n",
    "            print(f\"  {symbol}: {total_return}% (${min_price:.2f} ‚Üí ${max_price:.2f})\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Crypto import failed: {e}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Check if crypto_prices table exists with correct schema\")\n",
    "        print(\"2. Verify timestamp format compatibility\")\n",
    "        print(\"3. Check for sufficient disk space\")\n",
    "        print(\"4. Try with replace_existing=True to clear existing data\")\n",
    "        print(\"5. Verify high-precision numeric fields can handle crypto prices\")\n",
    "        return False\n",
    "\n",
    "# Usage examples:\n",
    "\n",
    "# Option 1: Skip existing records (default behavior)\n",
    "print(\"üîÑ Importing crypto prices (skip existing)...\")\n",
    "if import_crypto_prices(db_config):\n",
    "    print(\"\\nüéâ Crypto price import complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please check crypto import errors\")\n",
    "\n",
    "# Option 2: Replace all existing data  \n",
    "print(\"\\nüîÑ Alternative: Replace existing crypto data...\")\n",
    "print(\"Uncomment the line below if you want to replace all existing crypto data:\")\n",
    "print(\"# import_crypto_prices(db_config, replace_existing=True)\")\n",
    "\n",
    "# Quick crypto-specific verification\n",
    "def verify_crypto_data(config):\n",
    "    \"\"\"Quick verification of imported crypto data with crypto-specific checks\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"\\nüîç Quick Crypto Data Verification:\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Basic crypto stats\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(DISTINCT symbol) as unique_symbols,\n",
    "                MIN(timestamp) as first_timestamp,\n",
    "                MAX(timestamp) as last_timestamp,\n",
    "                COUNT(DISTINCT DATE(timestamp)) as trading_days,\n",
    "                ROUND(AVG(close), 4) as avg_price_all_cryptos\n",
    "            FROM crypto_prices\n",
    "        \"\"\")\n",
    "        \n",
    "        result = cur.fetchone()\n",
    "        print(f\"Total Records: {result[0]:,}\")\n",
    "        print(f\"Unique Symbols: {result[1]}\")\n",
    "        print(f\"Time Range: {result[2]} to {result[3]}\")\n",
    "        print(f\"Trading Days: {result[4]:,}\")\n",
    "        print(f\"Average Price (All Cryptos): ${result[5]}\")\n",
    "        \n",
    "        # Check 24/7 trading pattern\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT EXTRACT(hour FROM timestamp) as hour, COUNT(*) as records\n",
    "            FROM crypto_prices\n",
    "            GROUP BY EXTRACT(hour FROM timestamp)\n",
    "            ORDER BY hour\n",
    "        \"\"\")\n",
    "        \n",
    "        hourly_data = cur.fetchall()\n",
    "        print(f\"\\n24/7 Trading Verification:\")\n",
    "        print(f\"Hours with data: {len(hourly_data)}/24\")\n",
    "        if len(hourly_data) == 24:\n",
    "            print(\"‚úÖ Confirmed 24/7 trading data\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Missing some hourly data\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Crypto verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run crypto verification\n",
    "verify_crypto_data(db_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ed44a-2202-448e-bd40-1560414a4d86",
   "metadata": {},
   "source": [
    " Importing Economic Indicators\n",
    "=====================================\n",
    "\n",
    "Economic indicators are different from price data:\n",
    "1. Monthly frequency (not daily)\n",
    "2. Different units (%, billions, index points)\n",
    "3. Some can be negative (trade balance)\n",
    "4. Wide value ranges (0.5% to millions)\n",
    "\n",
    "Understanding the indicators:\n",
    "- GDP_GROWTH: Quarterly GDP growth rate (%)\n",
    "- INFLATION_RATE: Monthly CPI change (%)\n",
    "- UNEMPLOYMENT_RATE: Monthly unemployment (%)\n",
    "- INTEREST_RATE: Federal funds rate (%)\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0928592f-b40a-4522-99f8-542f459d2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Importing economic indicators (skip existing)...\n",
      "üèõÔ∏è Importing Economic Indicators\n",
      "============================================================\n",
      "Step 1: Loading economic data...\n",
      "‚úÖ Loaded 600 rows\n",
      "\n",
      "Economic indicators overview:\n",
      "  GDP_GROWTH: 60 observations, range: 1.10 to 2.99\n",
      "  INFLATION_RATE: 60 observations, range: 0.80 to 2.61\n",
      "  UNEMPLOYMENT_RATE: 60 observations, range: 4.49 to 6.15\n",
      "  INTEREST_RATE: 60 observations, range: 0.15 to 1.66\n",
      "  CONSUMER_CONFIDENCE: 60 observations, range: 96.59 to 123.80\n",
      "  RETAIL_SALES: 60 observations, range: -1.39 to 1.17\n",
      "  INDUSTRIAL_PRODUCTION: 60 observations, range: -1.75 to 3.06\n",
      "  HOUSING_STARTS: 60 observations, range: 907856.01 to 1403145.38\n",
      "  TRADE_BALANCE: 60 observations, range: -110027.96 to -32348.12\n",
      "  MONEY_SUPPLY: 60 observations, range: 15369.05 to 20427.64\n",
      "\n",
      "Step 2: Preprocessing...\n",
      "\n",
      "Step 3: Checking for existing economic data...\n",
      "üìä Found 600 existing economic indicator records\n",
      "üîç Checking for overlapping economic data...\n",
      "‚è≠Ô∏è Skipping 600 economic records that already exist\n",
      "üì• Will import 0 new economic records\n",
      "‚ÑπÔ∏è No new economic data to import\n",
      "\n",
      "üéâ Economic indicators import complete!\n",
      "\n",
      "üîÑ Alternative: Replace existing economic data...\n",
      "Uncomment the line below if you want to replace all existing economic data:\n",
      "# import_economic_indicators(db_config, replace_existing=True)\n",
      "üîß Creating economic analysis views...\n",
      "üìä Creating/updating economic analysis views...\n",
      "‚úÖ Pivot view: 60 records\n",
      "‚úÖ Cycle view: 60 records\n",
      "‚úÖ All economic views created successfully\n",
      "\n",
      "üîÑ Running corrected economic verification...\n",
      "\n",
      "üîç Economic Data Verification:\n",
      "----------------------------------------\n",
      "Total Records: 600\n",
      "Unique Indicators: 10\n",
      "Months of Data: 60\n",
      "Date Range: 2020-01-31 to 2024-12-31\n",
      "\n",
      "Pivot View Status: ‚úÖ 60 complete records\n",
      "\n",
      "Economic Cycle Distribution:\n",
      "  Normal: 52 months\n",
      "  Expansion: 8 months\n",
      "\n",
      "Data Quality Checks:\n",
      "‚úÖ No missing values detected\n",
      "\n",
      "Data Frequency Check:\n",
      "  2020: 12/12 months ‚úÖ\n",
      "  2021: 12/12 months ‚úÖ\n",
      "  2022: 12/12 months ‚úÖ\n",
      "  2023: 12/12 months ‚úÖ\n",
      "  2024: 12/12 months ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "def import_economic_indicators(config, data_dir='mock_financial_data', replace_existing=False):\n",
    "    \"\"\"\n",
    "    Import economic indicators with proper unit handling and duplicate management.\n",
    "    \n",
    "    Args:\n",
    "        config: Database configuration object\n",
    "        data_dir: Directory containing CSV files\n",
    "        replace_existing: If True, clear existing data before import\n",
    "    \"\"\"\n",
    "    csv_file = 'economic_indicators.csv'\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"üèõÔ∏è Importing Economic Indicators\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        print(\"Step 1: Loading economic data...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "        \n",
    "        # Understand the structure\n",
    "        print(\"\\nEconomic indicators overview:\")\n",
    "        indicators = df['Indicator'].unique()\n",
    "        for ind in indicators:\n",
    "            ind_data = df[df['Indicator'] == ind]\n",
    "            print(f\"  {ind}: {len(ind_data)} observations, \"\n",
    "                  f\"range: {ind_data['Value'].min():.2f} to {ind_data['Value'].max():.2f}\")\n",
    "        \n",
    "        # Step 2: Preprocessing\n",
    "        print(\"\\nStep 2: Preprocessing...\")\n",
    "        df['date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.drop('Date', axis=1)\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Check for CSV duplicates\n",
    "        csv_duplicates = df.duplicated(subset=['date', 'indicator'])\n",
    "        if csv_duplicates.any():\n",
    "            print(f\"‚ö†Ô∏è Found {csv_duplicates.sum()} duplicate rows in CSV\")\n",
    "            df = df[~csv_duplicates]\n",
    "            print(f\"‚úÖ Removed CSV duplicates, {len(df):,} rows remaining\")\n",
    "        \n",
    "        # Add unit information based on indicator type\n",
    "        unit_mapping = {\n",
    "            'GDP_GROWTH': '%',\n",
    "            'INFLATION_RATE': '%', \n",
    "            'UNEMPLOYMENT_RATE': '%',\n",
    "            'INTEREST_RATE': '%',\n",
    "            'CONSUMER_CONFIDENCE': 'index',\n",
    "            'RETAIL_SALES': '%',\n",
    "            'INDUSTRIAL_PRODUCTION': '%',\n",
    "            'HOUSING_STARTS': 'thousands',\n",
    "            'TRADE_BALANCE': 'millions USD',\n",
    "            'MONEY_SUPPLY': 'billions USD'\n",
    "        }\n",
    "        \n",
    "        df['unit'] = df['indicator'].map(unit_mapping)\n",
    "        \n",
    "        # Step 3: Handle existing data\n",
    "        print(f\"\\nStep 3: Checking for existing economic data...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Check if table exists and has data\n",
    "        try:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM economic_indicators\")\n",
    "            existing_count = cur.fetchone()[0]\n",
    "        except psycopg2.Error:\n",
    "            existing_count = 0\n",
    "            print(\"‚ÑπÔ∏è economic_indicators table doesn't exist yet\")\n",
    "        \n",
    "        if existing_count > 0:\n",
    "            print(f\"üìä Found {existing_count:,} existing economic indicator records\")\n",
    "            \n",
    "            if replace_existing:\n",
    "                print(\"üóëÔ∏è Clearing existing economic data (replace_existing=True)...\")\n",
    "                cur.execute(\"DELETE FROM economic_indicators\")\n",
    "                conn.commit()\n",
    "                print(\"‚úÖ Existing economic data cleared\")\n",
    "            else:\n",
    "                print(\"üîç Checking for overlapping economic data...\")\n",
    "                \n",
    "                # Get existing date/indicator combinations\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT DISTINCT date, indicator \n",
    "                    FROM economic_indicators\n",
    "                \"\"\")\n",
    "                existing_combinations = set(cur.fetchall())\n",
    "                \n",
    "                # Filter out records that already exist\n",
    "                df['temp_tuple'] = list(zip(df['date'].dt.date, df['indicator']))\n",
    "                df_filtered = df[~df['temp_tuple'].isin(existing_combinations)]\n",
    "                df_filtered = df_filtered.drop('temp_tuple', axis=1)\n",
    "                \n",
    "                if len(df_filtered) < len(df):\n",
    "                    skipped = len(df) - len(df_filtered)\n",
    "                    print(f\"‚è≠Ô∏è Skipping {skipped:,} economic records that already exist\")\n",
    "                    print(f\"üì• Will import {len(df_filtered):,} new economic records\")\n",
    "                    df = df_filtered\n",
    "                else:\n",
    "                    print(\"‚úÖ No overlapping economic data found\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        # Skip import if no new data\n",
    "        if len(df) == 0:\n",
    "            print(\"‚ÑπÔ∏è No new economic data to import\")\n",
    "            return True\n",
    "        \n",
    "        # Step 4: Economic validation\n",
    "        print(f\"\\nStep 4: Economic validation on {len(df):,} records...\")\n",
    "        \n",
    "        # Check for unrealistic values with economic reasoning\n",
    "        validations = {\n",
    "            'GDP_GROWTH': (-10, 20, \"GDP growth outside normal recession to boom range\"),\n",
    "            'INFLATION_RATE': (-5, 30, \"Inflation outside deflation to hyperinflation range\"), \n",
    "            'UNEMPLOYMENT_RATE': (0, 30, \"Unemployment outside 0% to Great Depression levels\"),\n",
    "            'INTEREST_RATE': (-2, 20, \"Interest rates outside negative to very high range\"),\n",
    "            'CONSUMER_CONFIDENCE': (0, 200, \"Consumer confidence outside typical index range\"),\n",
    "            'RETAIL_SALES': (-20, 30, \"Retail sales growth outside realistic range\"),\n",
    "            'INDUSTRIAL_PRODUCTION': (-30, 30, \"Industrial production outside recession/boom range\")\n",
    "        }\n",
    "        \n",
    "        validation_issues = 0\n",
    "        for indicator, (min_val, max_val, description) in validations.items():\n",
    "            if indicator in df['indicator'].values:\n",
    "                ind_data = df[df['indicator'] == indicator]\n",
    "                out_of_range = ind_data[(ind_data['value'] < min_val) | (ind_data['value'] > max_val)]\n",
    "                if len(out_of_range) > 0:\n",
    "                    print(f\"‚ö†Ô∏è {indicator}: {len(out_of_range)} values outside expected range [{min_val}, {max_val}]\")\n",
    "                    print(f\"   Context: {description}\")\n",
    "                    validation_issues += 1\n",
    "        \n",
    "        if validation_issues == 0:\n",
    "            print(\"‚úÖ All economic indicators within expected ranges\")\n",
    "        \n",
    "        # Economic correlation checks\n",
    "        print(\"\\nEconomic relationship validation:\")\n",
    "        if 'UNEMPLOYMENT_RATE' in df['indicator'].values and 'INFLATION_RATE' in df['indicator'].values:\n",
    "            # Phillips Curve relationship check\n",
    "            unemployment = df[df['indicator'] == 'UNEMPLOYMENT_RATE'].set_index('date')['value']\n",
    "            inflation = df[df['indicator'] == 'INFLATION_RATE'].set_index('date')['value']\n",
    "            \n",
    "            # Align dates and calculate correlation\n",
    "            common_dates = unemployment.index.intersection(inflation.index)\n",
    "            if len(common_dates) > 10:\n",
    "                corr = unemployment[common_dates].corr(inflation[common_dates])\n",
    "                print(f\"  Phillips Curve check (Unemployment vs Inflation): correlation = {corr:.3f}\")\n",
    "                if corr < -0.1:\n",
    "                    print(\"  ‚úÖ Negative correlation detected (consistent with Phillips Curve)\")\n",
    "                else:\n",
    "                    print(\"  ‚ÑπÔ∏è Correlation not strongly negative (may indicate supply shocks or stagflation)\")\n",
    "        \n",
    "        # Step 5: Import to database\n",
    "        print(f\"\\nStep 5: Importing {len(df):,} records to PostgreSQL...\")\n",
    "        \n",
    "        engine = create_engine(config.get_connection_string())\n",
    "        \n",
    "        try:\n",
    "            df.to_sql(\n",
    "                'economic_indicators',\n",
    "                engine,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi'\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Successfully imported {len(df):,} economic indicator records\")\n",
    "            \n",
    "        except Exception as import_error:\n",
    "            print(f\"‚ùå Import error: {import_error}\")\n",
    "            return False\n",
    "        \n",
    "        # Step 6: Verify and create analytical views\n",
    "        print(\"\\nStep 6: Creating economic summary views...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Create a pivot view for easier analysis\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW economic_indicators_pivot AS\n",
    "            SELECT \n",
    "                date,\n",
    "                MAX(CASE WHEN indicator = 'GDP_GROWTH' THEN value END) as gdp_growth,\n",
    "                MAX(CASE WHEN indicator = 'INFLATION_RATE' THEN value END) as inflation_rate,\n",
    "                MAX(CASE WHEN indicator = 'UNEMPLOYMENT_RATE' THEN value END) as unemployment_rate,\n",
    "                MAX(CASE WHEN indicator = 'INTEREST_RATE' THEN value END) as interest_rate,\n",
    "                MAX(CASE WHEN indicator = 'CONSUMER_CONFIDENCE' THEN value END) as consumer_confidence,\n",
    "                MAX(CASE WHEN indicator = 'RETAIL_SALES' THEN value END) as retail_sales,\n",
    "                MAX(CASE WHEN indicator = 'INDUSTRIAL_PRODUCTION' THEN value END) as industrial_production\n",
    "            FROM economic_indicators\n",
    "            GROUP BY date\n",
    "            ORDER BY date DESC;\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"‚úÖ Created pivot view for easier economic analysis\")\n",
    "        \n",
    "        # Create economic cycle indicator view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW economic_cycle_indicators AS\n",
    "            SELECT \n",
    "                date,\n",
    "                gdp_growth,\n",
    "                unemployment_rate,\n",
    "                inflation_rate,\n",
    "                CASE \n",
    "                    WHEN gdp_growth > 3 AND unemployment_rate < 5 THEN 'Expansion'\n",
    "                    WHEN gdp_growth < 0 THEN 'Recession' \n",
    "                    WHEN unemployment_rate > 8 THEN 'Recovery'\n",
    "                    ELSE 'Normal'\n",
    "                END as economic_cycle_phase\n",
    "            FROM economic_indicators_pivot\n",
    "            WHERE gdp_growth IS NOT NULL AND unemployment_rate IS NOT NULL;\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"‚úÖ Created economic cycle phase indicator view\")\n",
    "        \n",
    "        # Show recent economic snapshot\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                date,\n",
    "                gdp_growth,\n",
    "                inflation_rate, \n",
    "                unemployment_rate,\n",
    "                interest_rate,\n",
    "                consumer_confidence\n",
    "            FROM economic_indicators_pivot\n",
    "            ORDER BY date DESC\n",
    "            LIMIT 6\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nRecent Economic Snapshot (Last 6 Months):\")\n",
    "        print(f\"{'Date':<12} {'GDP%':<6} {'Infl%':<6} {'Unemp%':<6} {'IntR%':<6} {'ConConf':<7}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            date_str = str(row[0])[:10] if row[0] else \"N/A\"\n",
    "            gdp = f\"{row[1]:.1f}\" if row[1] is not None else \"N/A\"\n",
    "            inf = f\"{row[2]:.1f}\" if row[2] is not None else \"N/A\" \n",
    "            une = f\"{row[3]:.1f}\" if row[3] is not None else \"N/A\"\n",
    "            int_r = f\"{row[4]:.1f}\" if row[4] is not None else \"N/A\"\n",
    "            conf = f\"{row[5]:.0f}\" if row[5] is not None else \"N/A\"\n",
    "            \n",
    "            print(f\"{date_str:<12} {gdp:<6} {inf:<6} {une:<6} {int_r:<6} {conf:<7}\")\n",
    "        \n",
    "        # Economic summary statistics\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(DISTINCT date) as months_of_data,\n",
    "                COUNT(DISTINCT indicator) as unique_indicators,\n",
    "                MIN(date) as first_month,\n",
    "                MAX(date) as last_month\n",
    "            FROM economic_indicators\n",
    "        \"\"\")\n",
    "        \n",
    "        months_data, unique_indicators, first_month, last_month = cur.fetchone()\n",
    "        \n",
    "        print(f\"\\nEconomic Data Summary:\")\n",
    "        print(f\"  Months of data: {months_data}\")\n",
    "        print(f\"  Unique indicators: {unique_indicators}\")\n",
    "        print(f\"  Date range: {first_month} to {last_month}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Economic indicators import failed: {e}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Check if economic_indicators table exists with correct schema\")\n",
    "        print(\"2. Verify date format compatibility\")\n",
    "        print(\"3. Check indicator name spelling and case sensitivity\")\n",
    "        print(\"4. Try with replace_existing=True to clear existing data\")\n",
    "        print(\"5. Verify numeric fields can handle economic indicator ranges\")\n",
    "        return False\n",
    "\n",
    "# Usage examples:\n",
    "\n",
    "# Option 1: Skip existing records (default behavior)\n",
    "print(\"üîÑ Importing economic indicators (skip existing)...\")\n",
    "if import_economic_indicators(db_config):\n",
    "    print(\"\\nüéâ Economic indicators import complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please check economic import errors\")\n",
    "\n",
    "# Option 2: Replace all existing data\n",
    "print(\"\\nüîÑ Alternative: Replace existing economic data...\")\n",
    "print(\"Uncomment the line below if you want to replace all existing economic data:\")\n",
    "print(\"# import_economic_indicators(db_config, replace_existing=True)\")\n",
    "\n",
    "# Economic-specific verification function\n",
    "def verify_economic_data(config):\n",
    "    \"\"\"Verify economic data with safe view handling and macroeconomic relationship checks\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"\\nüîç Economic Data Verification:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Basic economic stats\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(DISTINCT indicator) as unique_indicators,\n",
    "                COUNT(DISTINCT date) as months_of_data,\n",
    "                MIN(date) as first_date,\n",
    "                MAX(date) as last_date\n",
    "            FROM economic_indicators\n",
    "        \"\"\")\n",
    "        \n",
    "        result = cur.fetchone()\n",
    "        print(f\"Total Records: {result[0]:,}\")\n",
    "        print(f\"Unique Indicators: {result[1]}\")\n",
    "        print(f\"Months of Data: {result[2]}\")\n",
    "        print(f\"Date Range: {result[3]} to {result[4]}\")\n",
    "        \n",
    "        # Check if pivot view exists and has data\n",
    "        try:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT COUNT(*) FROM economic_indicators_pivot \n",
    "                WHERE gdp_growth IS NOT NULL AND unemployment_rate IS NOT NULL\n",
    "            \"\"\")\n",
    "            pivot_count = cur.fetchone()[0]\n",
    "            \n",
    "            if pivot_count > 0:\n",
    "                print(f\"\\nPivot View Status: ‚úÖ {pivot_count} complete records\")\n",
    "                \n",
    "                # Try to check if cycle indicators view exists\n",
    "                try:\n",
    "                    cur.execute(\"\"\"\n",
    "                        SELECT \n",
    "                            economic_cycle_phase,\n",
    "                            COUNT(*) as months\n",
    "                        FROM economic_cycle_indicators\n",
    "                        GROUP BY economic_cycle_phase\n",
    "                        ORDER BY months DESC\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    print(f\"\\nEconomic Cycle Distribution:\")\n",
    "                    cycle_data = cur.fetchall()\n",
    "                    for phase, months in cycle_data:\n",
    "                        print(f\"  {phase}: {months} months\")\n",
    "                        \n",
    "                except psycopg2.Error:\n",
    "                    # Create the view manually if it doesn't exist\n",
    "                    print(f\"\\nüìä Creating economic cycle indicators view...\")\n",
    "                    \n",
    "                    cur.execute(\"\"\"\n",
    "                        CREATE OR REPLACE VIEW economic_cycle_indicators AS\n",
    "                        SELECT \n",
    "                            date,\n",
    "                            gdp_growth,\n",
    "                            unemployment_rate,\n",
    "                            inflation_rate,\n",
    "                            CASE \n",
    "                                WHEN gdp_growth > 3 AND unemployment_rate < 5 THEN 'Expansion'\n",
    "                                WHEN gdp_growth < 0 THEN 'Recession' \n",
    "                                WHEN unemployment_rate > 8 THEN 'Recovery'\n",
    "                                ELSE 'Normal'\n",
    "                            END as economic_cycle_phase\n",
    "                        FROM economic_indicators_pivot\n",
    "                        WHERE gdp_growth IS NOT NULL AND unemployment_rate IS NOT NULL;\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    conn.commit()\n",
    "                    print(\"‚úÖ Economic cycle view created\")\n",
    "                    \n",
    "                    # Now get the cycle distribution\n",
    "                    cur.execute(\"\"\"\n",
    "                        SELECT \n",
    "                            economic_cycle_phase,\n",
    "                            COUNT(*) as months\n",
    "                        FROM economic_cycle_indicators\n",
    "                        GROUP BY economic_cycle_phase\n",
    "                        ORDER BY months DESC\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    print(f\"\\nEconomic Cycle Distribution:\")\n",
    "                    cycle_data = cur.fetchall()\n",
    "                    for phase, months in cycle_data:\n",
    "                        print(f\"  {phase}: {months} months\")\n",
    "                        \n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è Pivot view exists but lacks complete GDP/unemployment data\")\n",
    "                \n",
    "        except psycopg2.Error as view_error:\n",
    "            print(f\"\\n‚ö†Ô∏è Pivot view issue: {view_error}\")\n",
    "            print(\"Creating basic economic analysis instead...\")\n",
    "            \n",
    "            # Fallback: Direct analysis from raw table\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT \n",
    "                    indicator,\n",
    "                    COUNT(*) as observations,\n",
    "                    ROUND(AVG(value), 2) as avg_value,\n",
    "                    ROUND(MIN(value), 2) as min_value,\n",
    "                    ROUND(MAX(value), 2) as max_value\n",
    "                FROM economic_indicators\n",
    "                GROUP BY indicator\n",
    "                ORDER BY indicator\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"\\nIndicator Summary:\")\n",
    "            print(f\"{'Indicator':<20} {'Obs':<5} {'Avg':<8} {'Min':<8} {'Max':<8}\")\n",
    "            print(\"-\" * 55)\n",
    "            \n",
    "            for row in cur.fetchall():\n",
    "                indicator, obs, avg_val, min_val, max_val = row\n",
    "                print(f\"{indicator:<20} {obs:<5} {avg_val:<8} {min_val:<8} {max_val:<8}\")\n",
    "        \n",
    "        # Additional validation checks\n",
    "        print(f\"\\nData Quality Checks:\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                indicator,\n",
    "                COUNT(*) as total,\n",
    "                COUNT(*) - COUNT(value) as missing_values\n",
    "            FROM economic_indicators\n",
    "            GROUP BY indicator\n",
    "            HAVING COUNT(*) - COUNT(value) > 0\n",
    "        \"\"\")\n",
    "        \n",
    "        missing_data = cur.fetchall()\n",
    "        if missing_data:\n",
    "            print(\"‚ö†Ô∏è Missing values detected:\")\n",
    "            for indicator, total, missing in missing_data:\n",
    "                print(f\"  {indicator}: {missing}/{total} missing\")\n",
    "        else:\n",
    "            print(\"‚úÖ No missing values detected\")\n",
    "        \n",
    "        # Check data frequency\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                DATE_PART('year', date) as year,\n",
    "                COUNT(DISTINCT date) as months_per_year\n",
    "            FROM economic_indicators\n",
    "            GROUP BY DATE_PART('year', date)\n",
    "            ORDER BY year\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"\\nData Frequency Check:\")\n",
    "        frequency_data = cur.fetchall()\n",
    "        for year, months in frequency_data:\n",
    "            expected = 12 if year < 2024 else 12  # Adjust based on current data\n",
    "            status = \"‚úÖ\" if months >= 10 else \"‚ö†Ô∏è\"  # Allow for some missing months\n",
    "            print(f\"  {int(year)}: {months}/12 months {status}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Economic verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Also create a function to ensure all views are properly created\n",
    "def create_economic_views(config):\n",
    "    \"\"\"Ensure all economic analysis views are properly created\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"üìä Creating/updating economic analysis views...\")\n",
    "        \n",
    "        # Create pivot view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW economic_indicators_pivot AS\n",
    "            SELECT \n",
    "                date,\n",
    "                MAX(CASE WHEN indicator = 'GDP_GROWTH' THEN value END) as gdp_growth,\n",
    "                MAX(CASE WHEN indicator = 'INFLATION_RATE' THEN value END) as inflation_rate,\n",
    "                MAX(CASE WHEN indicator = 'UNEMPLOYMENT_RATE' THEN value END) as unemployment_rate,\n",
    "                MAX(CASE WHEN indicator = 'INTEREST_RATE' THEN value END) as interest_rate,\n",
    "                MAX(CASE WHEN indicator = 'CONSUMER_CONFIDENCE' THEN value END) as consumer_confidence,\n",
    "                MAX(CASE WHEN indicator = 'RETAIL_SALES' THEN value END) as retail_sales,\n",
    "                MAX(CASE WHEN indicator = 'INDUSTRIAL_PRODUCTION' THEN value END) as industrial_production,\n",
    "                MAX(CASE WHEN indicator = 'HOUSING_STARTS' THEN value END) as housing_starts,\n",
    "                MAX(CASE WHEN indicator = 'TRADE_BALANCE' THEN value END) as trade_balance,\n",
    "                MAX(CASE WHEN indicator = 'MONEY_SUPPLY' THEN value END) as money_supply\n",
    "            FROM economic_indicators\n",
    "            GROUP BY date\n",
    "            ORDER BY date DESC;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create economic cycle view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW economic_cycle_indicators AS\n",
    "            SELECT \n",
    "                date,\n",
    "                gdp_growth,\n",
    "                unemployment_rate,\n",
    "                inflation_rate,\n",
    "                interest_rate,\n",
    "                CASE \n",
    "                    WHEN gdp_growth > 3 AND unemployment_rate < 5 THEN 'Expansion'\n",
    "                    WHEN gdp_growth < 0 THEN 'Recession' \n",
    "                    WHEN unemployment_rate > 8 THEN 'Recovery'\n",
    "                    ELSE 'Normal'\n",
    "                END as economic_cycle_phase\n",
    "            FROM economic_indicators_pivot\n",
    "            WHERE gdp_growth IS NOT NULL AND unemployment_rate IS NOT NULL;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create monthly summary view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW economic_monthly_summary AS\n",
    "            SELECT \n",
    "                DATE_TRUNC('month', date) as month,\n",
    "                COUNT(DISTINCT indicator) as indicators_reported,\n",
    "                COUNT(*) as total_data_points,\n",
    "                STRING_AGG(DISTINCT indicator, ', ' ORDER BY indicator) as available_indicators\n",
    "            FROM economic_indicators\n",
    "            GROUP BY DATE_TRUNC('month', date)\n",
    "            ORDER BY month DESC;\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "        # Test the views\n",
    "        cur.execute(\"SELECT COUNT(*) FROM economic_indicators_pivot\")\n",
    "        pivot_count = cur.fetchone()[0]\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) FROM economic_cycle_indicators\")\n",
    "        cycle_count = cur.fetchone()[0]\n",
    "        \n",
    "        print(f\"‚úÖ Pivot view: {pivot_count} records\")\n",
    "        print(f\"‚úÖ Cycle view: {cycle_count} records\") \n",
    "        print(\"‚úÖ All economic views created successfully\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create economic views: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the corrected verification\n",
    "print(\"üîß Creating economic analysis views...\")\n",
    "if create_economic_views(db_config):\n",
    "    print(\"\\nüîÑ Running corrected economic verification...\")\n",
    "    verify_economic_data(db_config)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not create economic views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061baf1f-5cfe-4df4-9e65-c2a095cb1204",
   "metadata": {},
   "source": [
    "Importing Portfolio Holdings Data\n",
    "=========================================\n",
    "\n",
    "Portfolio data represents:\n",
    "- Investment accounts with different risk profiles\n",
    "- Monthly snapshots of holdings\n",
    "- Asset allocation (stocks, bonds, cash)\n",
    "- Performance metrics\n",
    "\n",
    "Key validations:\n",
    "- Weights must sum to 1.0 (100%)\n",
    "- Returns should be realistic\n",
    "- Total value should be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99aa0cea-f25b-408d-8908-77ba2c7e6621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Importing portfolio data (skip existing)...\n",
      "üíº Importing Portfolio Holdings Data\n",
      "============================================================\n",
      "Step 1: Loading portfolio data...\n",
      "‚úÖ Loaded 6,000 rows\n",
      "\n",
      "Step 2: Preprocessing...\n",
      "\n",
      "Step 3: Checking for existing portfolio data...\n",
      "üìä Found 6,000 existing portfolio records\n",
      "üîç Checking for overlapping portfolio data...\n",
      "‚è≠Ô∏è Skipping 6,000 portfolio records that already exist\n",
      "üì• Will import 0 new portfolio records\n",
      "‚ÑπÔ∏è No new portfolio data to import\n",
      "\n",
      "üéâ Portfolio data import complete!\n",
      "\n",
      "üîÑ Alternative: Replace existing portfolio data...\n",
      "Uncomment the line below if you want to replace all existing portfolio data:\n",
      "# import_portfolio_data(db_config, replace_existing=True)\n",
      "üîÑ Running portfolio data verification...\n",
      "\n",
      "üîç Portfolio Data Verification:\n",
      "----------------------------------------\n",
      "Total Records: 6,000\n",
      "Unique Portfolios: 100\n",
      "Months of Data: 60\n",
      "Date Range: 2020-01-31 to 2024-12-31\n",
      "\n",
      "Risk-Return Verification:\n",
      "  Conservative: 5.57% return, 9.13% volatility (2,100 records)\n",
      "  Moderate: 7.26% return, 13.46% volatility (2,640 records)\n",
      "  Aggressive: 11.04% return, 20.00% volatility (1,260 records)\n",
      "\n",
      "Portfolio Allocation Verification:\n",
      "Risk Level   Stock %  Bond %   Cash %  \n",
      "----------------------------------------\n",
      "Conservative 38.3     48.0     13.7    \n",
      "Moderate     57.0     30.8     12.1    \n",
      "Aggressive   76.1     14.1     9.8     \n",
      "\n",
      "Weight Allocation Validation:\n",
      "‚úÖ All 6,000 records have weights summing to 1.0\n",
      "\n",
      "Performance Consistency Check:\n",
      "‚úÖ Risk-return relationship follows expected pattern (Conservative < Moderate < Aggressive)\n",
      "\n",
      "‚úÖ All portfolios have sufficient historical data\n"
     ]
    }
   ],
   "source": [
    "def import_portfolio_data(config, data_dir='mock_financial_data', replace_existing=False):\n",
    "    \"\"\"\n",
    "    Import portfolio holdings with allocation validation and duplicate handling.\n",
    "    \n",
    "    Args:\n",
    "        config: Database configuration object\n",
    "        data_dir: Directory containing CSV files\n",
    "        replace_existing: If True, clear existing data before import\n",
    "    \"\"\"\n",
    "    csv_file = 'portfolio_data.csv'\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"üíº Importing Portfolio Holdings Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        print(\"Step 1: Loading portfolio data...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "        \n",
    "        # Step 2: Basic preprocessing\n",
    "        print(\"\\nStep 2: Preprocessing...\")\n",
    "        df['date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.drop('Date', axis=1)\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Rename columns to match database schema\n",
    "        column_mapping = {\n",
    "            'portfolioid': 'portfolio_id',\n",
    "            'risklevel': 'risk_level',\n",
    "            'totalvalue': 'total_value',\n",
    "            'stockweight': 'stock_weight',\n",
    "            'bondweight': 'bond_weight',\n",
    "            'cashweight': 'cash_weight',\n",
    "            'monthlyreturn': 'monthly_return'\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Check for CSV duplicates\n",
    "        csv_duplicates = df.duplicated(subset=['date', 'portfolio_id'])\n",
    "        if csv_duplicates.any():\n",
    "            print(f\"‚ö†Ô∏è Found {csv_duplicates.sum()} duplicate rows in CSV\")\n",
    "            df = df[~csv_duplicates]\n",
    "            print(f\"‚úÖ Removed CSV duplicates, {len(df):,} rows remaining\")\n",
    "        \n",
    "        # Step 3: Handle existing data\n",
    "        print(f\"\\nStep 3: Checking for existing portfolio data...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Check if table exists and has data\n",
    "        try:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM portfolio_holdings\")\n",
    "            existing_count = cur.fetchone()[0]\n",
    "        except psycopg2.Error:\n",
    "            existing_count = 0\n",
    "            print(\"‚ÑπÔ∏è portfolio_holdings table doesn't exist yet\")\n",
    "        \n",
    "        if existing_count > 0:\n",
    "            print(f\"üìä Found {existing_count:,} existing portfolio records\")\n",
    "            \n",
    "            if replace_existing:\n",
    "                print(\"üóëÔ∏è Clearing existing portfolio data (replace_existing=True)...\")\n",
    "                cur.execute(\"DELETE FROM portfolio_holdings\")\n",
    "                conn.commit()\n",
    "                print(\"‚úÖ Existing portfolio data cleared\")\n",
    "            else:\n",
    "                print(\"üîç Checking for overlapping portfolio data...\")\n",
    "                \n",
    "                # Get existing date/portfolio_id combinations\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT DISTINCT date, portfolio_id \n",
    "                    FROM portfolio_holdings\n",
    "                \"\"\")\n",
    "                existing_combinations = set(cur.fetchall())\n",
    "                \n",
    "                # Filter out records that already exist\n",
    "                df['temp_tuple'] = list(zip(df['date'].dt.date, df['portfolio_id']))\n",
    "                df_filtered = df[~df['temp_tuple'].isin(existing_combinations)]\n",
    "                df_filtered = df_filtered.drop('temp_tuple', axis=1)\n",
    "                \n",
    "                if len(df_filtered) < len(df):\n",
    "                    skipped = len(df) - len(df_filtered)\n",
    "                    print(f\"‚è≠Ô∏è Skipping {skipped:,} portfolio records that already exist\")\n",
    "                    print(f\"üì• Will import {len(df_filtered):,} new portfolio records\")\n",
    "                    df = df_filtered\n",
    "                else:\n",
    "                    print(\"‚úÖ No overlapping portfolio data found\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        # Skip import if no new data\n",
    "        if len(df) == 0:\n",
    "            print(\"‚ÑπÔ∏è No new portfolio data to import\")\n",
    "            return True\n",
    "        \n",
    "        # Step 4: Portfolio-specific validations\n",
    "        print(f\"\\nStep 4: Portfolio validation on {len(df):,} records...\")\n",
    "        \n",
    "        # Check if weights sum to 1.0 (portfolio allocation constraint)\n",
    "        df['weight_sum'] = df['stock_weight'] + df['bond_weight'] + df['cash_weight']\n",
    "        weight_errors = df[abs(df['weight_sum'] - 1.0) > 0.001]  # Allow small rounding errors\n",
    "        \n",
    "        if len(weight_errors) > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {len(weight_errors)} rows where weights don't sum to 1.0\")\n",
    "            print(\"Sample weight errors:\")\n",
    "            sample_errors = weight_errors[['portfolio_id', 'date', 'weight_sum']].head()\n",
    "            for _, row in sample_errors.iterrows():\n",
    "                print(f\"  {row['portfolio_id']} on {row['date'].date()}: sum = {row['weight_sum']:.4f}\")\n",
    "            \n",
    "            # Fix by normalizing weights\n",
    "            print(\"Normalizing weight allocations...\")\n",
    "            for idx in weight_errors.index:\n",
    "                total = df.loc[idx, 'weight_sum']\n",
    "                if total > 0:\n",
    "                    df.loc[idx, 'stock_weight'] /= total\n",
    "                    df.loc[idx, 'bond_weight'] /= total\n",
    "                    df.loc[idx, 'cash_weight'] /= total\n",
    "            print(\"‚úÖ Weight allocations normalized\")\n",
    "        else:\n",
    "            print(\"‚úÖ All portfolio weights sum to 1.0\")\n",
    "        \n",
    "        df = df.drop('weight_sum', axis=1)\n",
    "        \n",
    "        # Check for negative weights (shouldn't happen in practice)\n",
    "        negative_weights = df[\n",
    "            (df['stock_weight'] < 0) | \n",
    "            (df['bond_weight'] < 0) | \n",
    "            (df['cash_weight'] < 0)\n",
    "        ]\n",
    "        if len(negative_weights) > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {len(negative_weights)} rows with negative weights\")\n",
    "        else:\n",
    "            print(\"‚úÖ No negative weights detected\")\n",
    "        \n",
    "        # Check for unrealistic returns (>100% or <-50% monthly)\n",
    "        extreme_returns = df[(df['monthly_return'] > 1.0) | (df['monthly_return'] < -0.5)]\n",
    "        if len(extreme_returns) > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {len(extreme_returns)} extreme monthly returns (>100% or <-50%)\")\n",
    "            extreme_sample = extreme_returns[['portfolio_id', 'date', 'monthly_return']].head()\n",
    "            for _, row in extreme_sample.iterrows():\n",
    "                print(f\"  {row['portfolio_id']} on {row['date'].date()}: {row['monthly_return']:.1%}\")\n",
    "        else:\n",
    "            print(\"‚úÖ No extreme monthly returns detected\")\n",
    "        \n",
    "        # Analyze performance by risk level\n",
    "        print(\"\\nPortfolio Performance Analysis by Risk Level:\")\n",
    "        risk_stats = df.groupby('risk_level').agg({\n",
    "            'monthly_return': ['mean', 'std', 'min', 'max'],\n",
    "            'portfolio_id': 'nunique',\n",
    "            'total_value': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        print(\"\\nRisk Level Performance Summary:\")\n",
    "        print(f\"{'Risk Level':<12} {'Count':<6} {'Avg Return':<11} {'Volatility':<10} {'Min Return':<11} {'Max Return':<11}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for risk_level in risk_stats.index:\n",
    "            count = risk_stats.loc[risk_level, ('portfolio_id', 'nunique')]\n",
    "            avg_ret = risk_stats.loc[risk_level, ('monthly_return', 'mean')]\n",
    "            volatility = risk_stats.loc[risk_level, ('monthly_return', 'std')]\n",
    "            min_ret = risk_stats.loc[risk_level, ('monthly_return', 'min')]\n",
    "            max_ret = risk_stats.loc[risk_level, ('monthly_return', 'max')]\n",
    "            \n",
    "            print(f\"{risk_level:<12} {count:<6} {avg_ret:<11.4f} {volatility:<10.4f} {min_ret:<11.4f} {max_ret:<11.4f}\")\n",
    "        \n",
    "        # Risk-return validation\n",
    "        risk_levels = ['Conservative', 'Moderate', 'Aggressive']\n",
    "        if all(level in risk_stats.index for level in risk_levels):\n",
    "            conservative_ret = risk_stats.loc['Conservative', ('monthly_return', 'mean')]\n",
    "            moderate_ret = risk_stats.loc['Moderate', ('monthly_return', 'mean')]\n",
    "            aggressive_ret = risk_stats.loc['Aggressive', ('monthly_return', 'mean')]\n",
    "            \n",
    "            if conservative_ret < moderate_ret < aggressive_ret:\n",
    "                print(\"‚úÖ Risk-return relationship is correct (Conservative < Moderate < Aggressive)\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Risk-return relationship may be inverted\")\n",
    "        \n",
    "        # Step 5: Import to database\n",
    "        print(f\"\\nStep 5: Importing {len(df):,} records to PostgreSQL...\")\n",
    "        \n",
    "        engine = create_engine(config.get_connection_string())\n",
    "        \n",
    "        try:\n",
    "            df.to_sql(\n",
    "                'portfolio_holdings',\n",
    "                engine,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi',\n",
    "                chunksize=5000\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Successfully imported {len(df):,} portfolio records\")\n",
    "            \n",
    "        except Exception as import_error:\n",
    "            print(f\"‚ùå Import error: {import_error}\")\n",
    "            return False\n",
    "        \n",
    "        # Step 6: Create portfolio analysis views\n",
    "        print(\"\\nStep 6: Creating portfolio analysis views...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Portfolio performance summary view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW portfolio_performance_summary AS\n",
    "            WITH returns_calc AS (\n",
    "                SELECT \n",
    "                    portfolio_id,\n",
    "                    risk_level,\n",
    "                    AVG(monthly_return) * 12 as annual_return,\n",
    "                    STDDEV(monthly_return) * SQRT(12) as annual_volatility,\n",
    "                    MIN(total_value) as min_value,\n",
    "                    MAX(total_value) as max_value,\n",
    "                    (MAX(total_value) / NULLIF(MIN(total_value), 0) - 1) as total_growth,\n",
    "                    COUNT(*) as months_of_data\n",
    "                FROM portfolio_holdings\n",
    "                GROUP BY portfolio_id, risk_level\n",
    "            )\n",
    "            SELECT \n",
    "                risk_level,\n",
    "                COUNT(*) as portfolio_count,\n",
    "                ROUND(AVG(annual_return), 4) as avg_annual_return,\n",
    "                ROUND(AVG(annual_volatility), 4) as avg_annual_volatility,\n",
    "                ROUND(AVG(annual_return) / NULLIF(AVG(annual_volatility), 0), 4) as avg_sharpe_ratio,\n",
    "                ROUND(AVG(total_growth), 4) as avg_total_growth,\n",
    "                ROUND(AVG(months_of_data), 1) as avg_months_data\n",
    "            FROM returns_calc\n",
    "            WHERE annual_volatility > 0\n",
    "            GROUP BY risk_level\n",
    "            ORDER BY \n",
    "                CASE risk_level \n",
    "                    WHEN 'Conservative' THEN 1 \n",
    "                    WHEN 'Moderate' THEN 2 \n",
    "                    WHEN 'Aggressive' THEN 3 \n",
    "                END;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Portfolio allocation drift view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW portfolio_allocation_drift AS\n",
    "            SELECT \n",
    "                portfolio_id,\n",
    "                risk_level,\n",
    "                ROUND(AVG(stock_weight), 3) as avg_stock_weight,\n",
    "                ROUND(STDDEV(stock_weight), 3) as stock_weight_volatility,\n",
    "                ROUND(AVG(bond_weight), 3) as avg_bond_weight,\n",
    "                ROUND(STDDEV(bond_weight), 3) as bond_weight_volatility,\n",
    "                ROUND(AVG(cash_weight), 3) as avg_cash_weight,\n",
    "                ROUND(STDDEV(cash_weight), 3) as cash_weight_volatility\n",
    "            FROM portfolio_holdings\n",
    "            GROUP BY portfolio_id, risk_level;\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"‚úÖ Created portfolio performance summary view\")\n",
    "        print(\"‚úÖ Created portfolio allocation drift view\")\n",
    "        \n",
    "        # Show the performance summary\n",
    "        cur.execute(\"SELECT * FROM portfolio_performance_summary\")\n",
    "        \n",
    "        print(\"\\nPortfolio Performance Summary:\")\n",
    "        print(f\"{'Risk Level':<12} {'Count':<6} {'Ann.Return':<10} {'Ann.Vol':<8} {'Sharpe':<7} {'Growth':<8} {'Months':<7}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            risk_level, count, ann_ret, ann_vol, sharpe, growth, months = row\n",
    "            print(f\"{risk_level:<12} {count:<6} {ann_ret:<10.4f} {ann_vol:<8.4f} {sharpe or 0:<7.2f} {growth or 0:<8.2f} {months:<7.1f}\")\n",
    "        \n",
    "        # Verification queries\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(DISTINCT portfolio_id) as unique_portfolios,\n",
    "                COUNT(DISTINCT date) as months_of_data,\n",
    "                MIN(date) as first_date,\n",
    "                MAX(date) as last_date\n",
    "            FROM portfolio_holdings\n",
    "        \"\"\")\n",
    "        \n",
    "        total_records, unique_portfolios, months_data, first_date, last_date = cur.fetchone()\n",
    "        \n",
    "        print(f\"\\nPortfolio Data Summary:\")\n",
    "        print(f\"  Total records: {total_records:,}\")\n",
    "        print(f\"  Unique portfolios: {unique_portfolios}\")\n",
    "        print(f\"  Months of data: {months_data}\")\n",
    "        print(f\"  Date range: {first_date} to {last_date}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Portfolio import failed: {e}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Check if portfolio_holdings table exists with correct schema\")\n",
    "        print(\"2. Verify date format compatibility\")\n",
    "        print(\"3. Check portfolio_id format and case sensitivity\")\n",
    "        print(\"4. Try with replace_existing=True to clear existing data\")\n",
    "        print(\"5. Verify numeric fields can handle portfolio values and returns\")\n",
    "        return False\n",
    "\n",
    "# Usage examples:\n",
    "\n",
    "# Option 1: Skip existing records (default behavior)\n",
    "print(\"üîÑ Importing portfolio data (skip existing)...\")\n",
    "if import_portfolio_data(db_config):\n",
    "    print(\"\\nüéâ Portfolio data import complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please check portfolio import errors\")\n",
    "\n",
    "# Option 2: Replace all existing data\n",
    "print(\"\\nüîÑ Alternative: Replace existing portfolio data...\")\n",
    "print(\"Uncomment the line below if you want to replace all existing portfolio data:\")\n",
    "print(\"# import_portfolio_data(db_config, replace_existing=True)\")\n",
    "\n",
    "# Portfolio-specific verification function\n",
    "def verify_portfolio_data(config):\n",
    "    \"\"\"Verify portfolio data with Modern Portfolio Theory validation and proper PostgreSQL type casting\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"\\nüîç Portfolio Data Verification:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Basic portfolio stats\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(DISTINCT portfolio_id) as unique_portfolios,\n",
    "                COUNT(DISTINCT date) as months_of_data,\n",
    "                MIN(date) as first_date,\n",
    "                MAX(date) as last_date\n",
    "            FROM portfolio_holdings\n",
    "        \"\"\")\n",
    "        \n",
    "        result = cur.fetchone()\n",
    "        print(f\"Total Records: {result[0]:,}\")\n",
    "        print(f\"Unique Portfolios: {result[1]}\")\n",
    "        print(f\"Months of Data: {result[2]}\")\n",
    "        print(f\"Date Range: {result[3]} to {result[4]}\")\n",
    "        \n",
    "        # Risk-return verification with proper type casting\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                risk_level,\n",
    "                COUNT(*) as records,\n",
    "                ROUND(CAST(AVG(monthly_return * 12) AS numeric), 4) as annualized_return,\n",
    "                ROUND(CAST(STDDEV(monthly_return) * SQRT(12) AS numeric), 4) as annualized_volatility\n",
    "            FROM portfolio_holdings\n",
    "            GROUP BY risk_level\n",
    "            ORDER BY \n",
    "                CASE risk_level \n",
    "                    WHEN 'Conservative' THEN 1 \n",
    "                    WHEN 'Moderate' THEN 2 \n",
    "                    WHEN 'Aggressive' THEN 3 \n",
    "                END\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"\\nRisk-Return Verification:\")\n",
    "        risk_data = cur.fetchall()\n",
    "        for row in risk_data:\n",
    "            risk_level, records, ann_return, ann_vol = row\n",
    "            if ann_return is not None and ann_vol is not None:\n",
    "                print(f\"  {risk_level}: {float(ann_return):.2%} return, {float(ann_vol):.2%} volatility ({records:,} records)\")\n",
    "            else:\n",
    "                print(f\"  {risk_level}: Insufficient data for calculation ({records:,} records)\")\n",
    "        \n",
    "        # Portfolio allocation verification\n",
    "        print(f\"\\nPortfolio Allocation Verification:\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                risk_level,\n",
    "                ROUND(CAST(AVG(stock_weight) AS numeric), 3) as avg_stock_weight,\n",
    "                ROUND(CAST(AVG(bond_weight) AS numeric), 3) as avg_bond_weight,\n",
    "                ROUND(CAST(AVG(cash_weight) AS numeric), 3) as avg_cash_weight\n",
    "            FROM portfolio_holdings\n",
    "            GROUP BY risk_level\n",
    "            ORDER BY \n",
    "                CASE risk_level \n",
    "                    WHEN 'Conservative' THEN 1 \n",
    "                    WHEN 'Moderate' THEN 2 \n",
    "                    WHEN 'Aggressive' THEN 3 \n",
    "                END\n",
    "        \"\"\")\n",
    "        \n",
    "        allocation_data = cur.fetchall()\n",
    "        print(f\"{'Risk Level':<12} {'Stock %':<8} {'Bond %':<8} {'Cash %':<8}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for row in allocation_data:\n",
    "            risk_level, stock_w, bond_w, cash_w = row\n",
    "            print(f\"{risk_level:<12} {float(stock_w)*100:<8.1f} {float(bond_w)*100:<8.1f} {float(cash_w)*100:<8.1f}\")\n",
    "        \n",
    "        # Weight sum validation\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(*) - COUNT(CASE WHEN ABS((stock_weight + bond_weight + cash_weight) - 1.0) <= 0.001 THEN 1 END) as weight_errors\n",
    "            FROM portfolio_holdings\n",
    "        \"\"\")\n",
    "        \n",
    "        total_records, weight_errors = cur.fetchone()\n",
    "        print(f\"\\nWeight Allocation Validation:\")\n",
    "        if weight_errors == 0:\n",
    "            print(f\"‚úÖ All {total_records:,} records have weights summing to 1.0\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {weight_errors:,} out of {total_records:,} records have weight sum errors\")\n",
    "        \n",
    "        # Performance consistency check\n",
    "        print(f\"\\nPerformance Consistency Check:\")\n",
    "        \n",
    "        # Check if Conservative < Moderate < Aggressive returns (expected relationship)\n",
    "        conservative_return = None\n",
    "        moderate_return = None\n",
    "        aggressive_return = None\n",
    "        \n",
    "        for row in risk_data:\n",
    "            risk_level, _, ann_return, _ = row\n",
    "            if ann_return is not None:\n",
    "                if risk_level == 'Conservative':\n",
    "                    conservative_return = float(ann_return)\n",
    "                elif risk_level == 'Moderate':\n",
    "                    moderate_return = float(ann_return)\n",
    "                elif risk_level == 'Aggressive':\n",
    "                    aggressive_return = float(ann_return)\n",
    "        \n",
    "        if all(x is not None for x in [conservative_return, moderate_return, aggressive_return]):\n",
    "            if conservative_return < moderate_return < aggressive_return:\n",
    "                print(\"‚úÖ Risk-return relationship follows expected pattern (Conservative < Moderate < Aggressive)\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Risk-return relationship doesn't follow expected pattern\")\n",
    "                print(f\"   Conservative: {conservative_return:.2%}\")\n",
    "                print(f\"   Moderate: {moderate_return:.2%}\")\n",
    "                print(f\"   Aggressive: {aggressive_return:.2%}\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Insufficient data to verify risk-return relationship\")\n",
    "        \n",
    "        # Data completeness check\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                portfolio_id,\n",
    "                COUNT(*) as months_of_data,\n",
    "                MIN(date) as first_date,\n",
    "                MAX(date) as last_date\n",
    "            FROM portfolio_holdings\n",
    "            GROUP BY portfolio_id\n",
    "            HAVING COUNT(*) < 50  -- Flag portfolios with less than 50 months of data\n",
    "            ORDER BY COUNT(*)\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        \n",
    "        incomplete_portfolios = cur.fetchall()\n",
    "        if incomplete_portfolios:\n",
    "            print(f\"\\nData Completeness Warning:\")\n",
    "            print(f\"Portfolios with incomplete data (< 50 months):\")\n",
    "            for portfolio_id, months, first_date, last_date in incomplete_portfolios:\n",
    "                print(f\"  {portfolio_id}: {months} months ({first_date} to {last_date})\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ All portfolios have sufficient historical data\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Portfolio verification failed: {e}\")\n",
    "        print(\"Note: This may be due to PostgreSQL type casting requirements\")\n",
    "        return False\n",
    "\n",
    "# Also create a simplified verification function that avoids complex calculations\n",
    "def verify_portfolio_data_simple(config):\n",
    "    \"\"\"Simplified portfolio verification without complex PostgreSQL functions\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"\\nüîç Simple Portfolio Data Verification:\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Basic counts and structure\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                risk_level,\n",
    "                COUNT(*) as records,\n",
    "                COUNT(DISTINCT portfolio_id) as portfolios,\n",
    "                AVG(monthly_return) as avg_monthly_return,\n",
    "                AVG(total_value) as avg_portfolio_value\n",
    "            FROM portfolio_holdings\n",
    "            GROUP BY risk_level\n",
    "            ORDER BY \n",
    "                CASE risk_level \n",
    "                    WHEN 'Conservative' THEN 1 \n",
    "                    WHEN 'Moderate' THEN 2 \n",
    "                    WHEN 'Aggressive' THEN 3 \n",
    "                END\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"{'Risk Level':<12} {'Records':<8} {'Portfolios':<10} {'Avg Monthly Return':<16} {'Avg Value':<12}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            risk_level, records, portfolios, avg_return, avg_value = row\n",
    "            avg_return_pct = (avg_return or 0) * 100\n",
    "            avg_value_formatted = f\"${(avg_value or 0):,.0f}\"\n",
    "            print(f\"{risk_level:<12} {records:<8} {portfolios:<10} {avg_return_pct:<16.3f}% {avg_value_formatted:<12}\")\n",
    "        \n",
    "        # Simple allocation check\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                risk_level,\n",
    "                AVG(stock_weight) as avg_stock,\n",
    "                AVG(bond_weight) as avg_bond,\n",
    "                AVG(cash_weight) as avg_cash\n",
    "            FROM portfolio_holdings\n",
    "            GROUP BY risk_level\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"\\nAverage Asset Allocation by Risk Level:\")\n",
    "        print(f\"{'Risk Level':<12} {'Stock %':<8} {'Bond %':<8} {'Cash %':<8}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            risk_level, stock_w, bond_w, cash_w = row\n",
    "            print(f\"{risk_level:<12} {(stock_w or 0)*100:<8.1f} {(bond_w or 0)*100:<8.1f} {(cash_w or 0)*100:<8.1f}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Simple portfolio verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Try the fixed verification first, fall back to simple if needed\n",
    "print(\"üîÑ Running portfolio data verification...\")\n",
    "if not verify_portfolio_data(db_config):\n",
    "    print(\"\\nüîÑ Falling back to simplified verification...\")\n",
    "    verify_portfolio_data_simple(db_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834be39-a539-482d-a0de-dd2cd3642e92",
   "metadata": {},
   "source": [
    "Importing Customer Demographics Data\n",
    "============================================\n",
    "\n",
    "Customer data is sensitive and requires:\n",
    "1. Privacy considerations (even for mock data)\n",
    "2. Data type validation (age, credit scores)\n",
    "3. Business rule validation\n",
    "4. Risk segmentation accuracy\n",
    "\n",
    "This data supports:\n",
    "- Credit scoring models\n",
    "- Customer segmentation\n",
    "- Risk assessment\n",
    "- Marketing analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b923a4e-243e-4ae8-b5d3-195161651dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running FIXED customer import (skip existing)...\n",
      "üë• Importing Customer Demographics Data (Schema Fixed)\n",
      "============================================================\n",
      "Step 1: Loading customer data...\n",
      "‚úÖ Loaded 10,000 customer records\n",
      "\n",
      "Step 2: Preprocessing with schema mapping...\n",
      "‚úÖ Column names mapped to database schema\n",
      "\n",
      "Step 3: Checking for existing customer data...\n",
      "üìä Found 10,000 existing customer records\n",
      "üîç Checking for overlapping customer data...\n",
      "‚è≠Ô∏è Skipping 10,000 customers that already exist\n",
      "üì• Will import 0 new customers\n",
      "‚ÑπÔ∏è No new customer data to import\n",
      "\n",
      "üéâ Customer data import complete!\n",
      "\n",
      "üîç Customer Data Verification (Schema Fixed):\n",
      "--------------------------------------------------\n",
      "Total Customers: 10,000\n",
      "Average Age: 39.8 years\n",
      "Average Income: $41,638\n",
      "Average Credit Score: 690\n",
      "Average Account Balance: $7,754\n",
      "\n",
      "Risk Segment Distribution:\n",
      "  Medium: 5,391 customers (53.9%)\n",
      "  Low: 2,855 customers (28.6%)\n",
      "  High: 1,754 customers (17.5%)\n",
      "\n",
      "üîÑ Alternative: Replace existing customer data...\n",
      "Uncomment the line below if you want to replace all existing customer data:\n",
      "# import_customer_data_fixed(db_config, replace_existing=True)\n"
     ]
    }
   ],
   "source": [
    "def import_customer_data_fixed(config, data_dir='mock_financial_data', replace_existing=False):\n",
    "    \"\"\"\n",
    "    Import customer demographics data with correct schema mapping and duplicate handling.\n",
    "    \n",
    "    Args:\n",
    "        config: Database configuration object\n",
    "        data_dir: Directory containing CSV files\n",
    "        replace_existing: If True, clear existing data before import\n",
    "    \"\"\"\n",
    "    csv_file = 'customer_data.csv'\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"üë• Importing Customer Demographics Data (Schema Fixed)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load customer data\n",
    "        print(\"Step 1: Loading customer data...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"‚úÖ Loaded {len(df):,} customer records\")\n",
    "        \n",
    "        # Step 2: Preprocessing with correct column mapping\n",
    "        print(\"\\nStep 2: Preprocessing with schema mapping...\")\n",
    "        \n",
    "        # Ensure column names match database schema\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Map CSV column names to database column names\n",
    "        column_mapping = {\n",
    "            'customerid': 'customer_id',\n",
    "            'creditscore': 'credit_score',\n",
    "            'accountagedays': 'account_age_days',\n",
    "            'accountbalance': 'account_balance',\n",
    "            'monthlytransactions': 'monthly_transactions',\n",
    "            'avgtransactionamount': 'avg_transaction_amount',\n",
    "            'numproducts': 'num_products',\n",
    "            'hasloan': 'has_loan',\n",
    "            'risksegment': 'risk_segment'\n",
    "            # age and income stay the same\n",
    "        }\n",
    "        \n",
    "        df = df.rename(columns=column_mapping)\n",
    "        print(\"‚úÖ Column names mapped to database schema\")\n",
    "        \n",
    "        # Check for CSV duplicates on customer_id\n",
    "        csv_duplicates = df.duplicated(subset=['customer_id'])\n",
    "        if csv_duplicates.any():\n",
    "            print(f\"‚ö†Ô∏è Found {csv_duplicates.sum()} duplicate customer IDs in CSV\")\n",
    "            df = df[~csv_duplicates]\n",
    "            print(f\"‚úÖ Removed CSV duplicates, {len(df):,} customers remaining\")\n",
    "        \n",
    "        # Step 3: Handle existing data\n",
    "        print(f\"\\nStep 3: Checking for existing customer data...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Check if table exists and has data\n",
    "        try:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM customers\")\n",
    "            existing_count = cur.fetchone()[0]\n",
    "        except psycopg2.Error:\n",
    "            existing_count = 0\n",
    "            print(\"‚ÑπÔ∏è customers table doesn't exist yet\")\n",
    "        \n",
    "        if existing_count > 0:\n",
    "            print(f\"üìä Found {existing_count:,} existing customer records\")\n",
    "            \n",
    "            if replace_existing:\n",
    "                print(\"üóëÔ∏è Clearing existing customer data (replace_existing=True)...\")\n",
    "                cur.execute(\"DELETE FROM customers\")\n",
    "                conn.commit()\n",
    "                print(\"‚úÖ Existing customer data cleared\")\n",
    "            else:\n",
    "                print(\"üîç Checking for overlapping customer data...\")\n",
    "                \n",
    "                # Get existing customer IDs (using correct column name)\n",
    "                cur.execute(\"SELECT DISTINCT customer_id FROM customers\")\n",
    "                existing_customer_ids = set(row[0] for row in cur.fetchall())\n",
    "                \n",
    "                # Filter out customers that already exist\n",
    "                df_filtered = df[~df['customer_id'].isin(existing_customer_ids)]\n",
    "                \n",
    "                if len(df_filtered) < len(df):\n",
    "                    skipped = len(df) - len(df_filtered)\n",
    "                    print(f\"‚è≠Ô∏è Skipping {skipped:,} customers that already exist\")\n",
    "                    print(f\"üì• Will import {len(df_filtered):,} new customers\")\n",
    "                    df = df_filtered\n",
    "                else:\n",
    "                    print(\"‚úÖ No overlapping customer data found\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        # Skip import if no new data\n",
    "        if len(df) == 0:\n",
    "            print(\"‚ÑπÔ∏è No new customer data to import\")\n",
    "            return True\n",
    "        \n",
    "        # Step 4: Customer data validation (using correct column names)\n",
    "        print(f\"\\nStep 4: Customer data validation on {len(df):,} customers...\")\n",
    "        \n",
    "        # Age validation\n",
    "        age_issues = df[(df['age'] < 18) | (df['age'] > 100)]\n",
    "        if len(age_issues) > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {len(age_issues)} customers with unusual ages\")\n",
    "        else:\n",
    "            print(\"‚úÖ All customer ages are within reasonable range (18-100)\")\n",
    "        \n",
    "        # Income validation\n",
    "        income_issues = df[(df['income'] < 0) | (df['income'] > 1000000)]\n",
    "        if len(income_issues) > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {len(income_issues)} customers with unusual income levels\")\n",
    "        else:\n",
    "            print(\"‚úÖ All customer incomes are within expected range\")\n",
    "        \n",
    "        # Credit score validation (using correct column name)\n",
    "        credit_issues = df[(df['credit_score'] < 300) | (df['credit_score'] > 850)]\n",
    "        if len(credit_issues) > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {len(credit_issues)} customers with invalid credit scores\")\n",
    "        else:\n",
    "            print(\"‚úÖ All credit scores are within FICO range (300-850)\")\n",
    "        \n",
    "        # Customer segmentation analysis (using correct column names)\n",
    "        print(\"\\nCustomer Segmentation Analysis:\")\n",
    "        segmentation = df.groupby('risk_segment').agg({\n",
    "            'customer_id': 'count',\n",
    "            'age': 'mean',\n",
    "            'income': 'mean', \n",
    "            'credit_score': 'mean',\n",
    "            'account_balance': 'mean',\n",
    "            'has_loan': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        segmentation.columns = ['count', 'avg_age', 'avg_income', 'avg_credit', 'avg_balance', 'loan_rate']\n",
    "        print(segmentation)\n",
    "        \n",
    "        # Behavioral analysis (using correct column names)\n",
    "        print(\"\\nCustomer Behavioral Analysis:\")\n",
    "        \n",
    "        transaction_analysis = df.groupby('risk_segment').agg({\n",
    "            'monthly_transactions': ['mean', 'std'],\n",
    "            'avg_transaction_amount': ['mean', 'std'],\n",
    "            'num_products': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        print(\"Transaction Patterns by Risk Segment:\")\n",
    "        for risk_segment in ['Low', 'Medium', 'High']:\n",
    "            if risk_segment in transaction_analysis.index:\n",
    "                avg_txns = transaction_analysis.loc[risk_segment, ('monthly_transactions', 'mean')]\n",
    "                avg_amount = transaction_analysis.loc[risk_segment, ('avg_transaction_amount', 'mean')]\n",
    "                avg_products = transaction_analysis.loc[risk_segment, ('num_products', 'mean')]\n",
    "                print(f\"  {risk_segment} Risk: {avg_txns:.1f} monthly txns, ${avg_amount:.2f} avg amount, {avg_products:.1f} products\")\n",
    "        \n",
    "        # Data quality checks\n",
    "        print(\"\\nData Quality Checks:\")\n",
    "        \n",
    "        # Check for duplicate customer IDs (should be none after preprocessing)\n",
    "        if df['customer_id'].duplicated().any():\n",
    "            print(\"‚ö†Ô∏è Duplicate customer IDs detected\")\n",
    "        else:\n",
    "            print(\"‚úÖ No duplicate customer IDs\")\n",
    "        \n",
    "        # Income-credit score correlation (should be positive)\n",
    "        income_credit_corr = df['income'].corr(df['credit_score'])\n",
    "        print(f\"Income-Credit Score correlation: {income_credit_corr:.3f}\")\n",
    "        if income_credit_corr > 0.05:\n",
    "            print(\"‚úÖ Positive correlation between income and credit score (expected)\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Weak correlation between income and credit score\")\n",
    "        \n",
    "        # Step 5: Import to database\n",
    "        print(f\"\\nStep 5: Importing {len(df):,} records to PostgreSQL...\")\n",
    "        \n",
    "        engine = create_engine(config.get_connection_string())\n",
    "        \n",
    "        try:\n",
    "            df.to_sql(\n",
    "                'customers',\n",
    "                engine,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi',\n",
    "                chunksize=5000\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Successfully imported {len(df):,} customer records\")\n",
    "            \n",
    "        except Exception as import_error:\n",
    "            print(f\"‚ùå Import error: {import_error}\")\n",
    "            return False\n",
    "        \n",
    "        # Step 6: Create customer analysis views (using correct column names)\n",
    "        print(\"\\nStep 6: Creating customer analysis views...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Customer segmentation summary view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW customer_segmentation_summary AS\n",
    "            SELECT \n",
    "                risk_segment,\n",
    "                COUNT(*) as customer_count,\n",
    "                ROUND(AVG(age), 1) as avg_age,\n",
    "                ROUND(AVG(income), 0) as avg_income,\n",
    "                ROUND(AVG(credit_score), 0) as avg_credit_score,\n",
    "                ROUND(AVG(account_balance), 0) as avg_account_balance,\n",
    "                ROUND(AVG(monthly_transactions), 1) as avg_monthly_transactions,\n",
    "                ROUND(AVG(avg_transaction_amount), 2) as avg_transaction_amount,\n",
    "                ROUND(AVG(num_products), 1) as avg_num_products,\n",
    "                ROUND(AVG(CASE WHEN has_loan THEN 1 ELSE 0 END), 3) as loan_penetration_rate\n",
    "            FROM customers\n",
    "            GROUP BY risk_segment\n",
    "            ORDER BY \n",
    "                CASE risk_segment \n",
    "                    WHEN 'Low' THEN 1 \n",
    "                    WHEN 'Medium' THEN 2 \n",
    "                    WHEN 'High' THEN 3 \n",
    "                END;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Customer lifetime value estimation view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW customer_value_analysis AS\n",
    "            SELECT \n",
    "                customer_id,\n",
    "                risk_segment,\n",
    "                age,\n",
    "                income,\n",
    "                credit_score,\n",
    "                account_balance,\n",
    "                monthly_transactions * avg_transaction_amount as estimated_monthly_volume,\n",
    "                CASE \n",
    "                    WHEN risk_segment = 'Low' AND account_balance > 50000 THEN 'High Value'\n",
    "                    WHEN risk_segment = 'Low' OR account_balance > 25000 THEN 'Medium Value'\n",
    "                    ELSE 'Standard Value'\n",
    "                END as value_segment\n",
    "            FROM customers;\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"‚úÖ Created customer segmentation summary view\")\n",
    "        print(\"‚úÖ Created customer value analysis view\")\n",
    "        \n",
    "        # Show segmentation summary\n",
    "        cur.execute(\"SELECT * FROM customer_segmentation_summary\")\n",
    "        \n",
    "        print(\"\\nCustomer Segmentation Summary:\")\n",
    "        print(f\"{'Risk':<6} {'Count':<6} {'Age':<5} {'Income':<8} {'Credit':<6} {'Balance':<8} {'Txns':<5} {'TxnAmt':<7} {'Prods':<5} {'Loans':<6}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            risk, count, age, income, credit, balance, txns, txn_amt, prods, loans = row\n",
    "            print(f\"{risk:<6} {count:<6} {age:<5.1f} ${income:<7,.0f} {credit:<6.0f} ${balance:<7,.0f} {txns:<5.1f} ${txn_amt:<6.2f} {prods:<5.1f} {loans:<6.1%}\")\n",
    "        \n",
    "        # Customer value distribution\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                value_segment,\n",
    "                COUNT(*) as customers,\n",
    "                AVG(account_balance) as avg_balance\n",
    "            FROM customer_value_analysis\n",
    "            GROUP BY value_segment\n",
    "            ORDER BY AVG(account_balance) DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"\\nCustomer Value Distribution:\")\n",
    "        for row in cur.fetchall():\n",
    "            value_segment, customers, avg_balance = row\n",
    "            print(f\"  {value_segment}: {customers:,} customers (avg balance: ${avg_balance:,.0f})\")\n",
    "        \n",
    "        # Final verification\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_customers,\n",
    "                COUNT(DISTINCT customer_id) as unique_customer_ids,\n",
    "                MIN(age) as min_age,\n",
    "                MAX(age) as max_age,\n",
    "                MIN(credit_score) as min_credit,\n",
    "                MAX(credit_score) as max_credit\n",
    "            FROM customers\n",
    "        \"\"\")\n",
    "        \n",
    "        total, unique_ids, min_age, max_age, min_credit, max_credit = cur.fetchone()\n",
    "        \n",
    "        print(f\"\\nCustomer Data Summary:\")\n",
    "        print(f\"  Total customers: {total:,}\")\n",
    "        print(f\"  Unique customer IDs: {unique_ids:,}\")\n",
    "        print(f\"  Age range: {min_age} to {max_age} years\")\n",
    "        print(f\"  Credit score range: {min_credit} to {max_credit}\")\n",
    "        \n",
    "        if total == unique_ids:\n",
    "            print(\"‚úÖ Customer ID uniqueness verified\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Customer ID uniqueness issue detected\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Customer import failed: {e}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Check if customers table exists with correct schema\")\n",
    "        print(\"2. Verify customer_id format matches database expectations\")\n",
    "        print(\"3. Check for data type mismatches\")\n",
    "        print(\"4. Try with replace_existing=True to clear existing data\")\n",
    "        print(\"5. Verify boolean fields (has_loan) are properly formatted\")\n",
    "        return False\n",
    "\n",
    "def verify_customer_data_fixed(config):\n",
    "    \"\"\"Verify customer data with correct column names\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"\\nüîç Customer Data Verification (Schema Fixed):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Basic customer stats (using correct column names)\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_customers,\n",
    "                AVG(age) as avg_age,\n",
    "                AVG(income) as avg_income,\n",
    "                AVG(credit_score) as avg_credit_score,\n",
    "                AVG(account_balance) as avg_balance\n",
    "            FROM customers\n",
    "        \"\"\")\n",
    "        \n",
    "        result = cur.fetchone()\n",
    "        print(f\"Total Customers: {result[0]:,}\")\n",
    "        print(f\"Average Age: {result[1]:.1f} years\")\n",
    "        print(f\"Average Income: ${result[2]:,.0f}\")\n",
    "        print(f\"Average Credit Score: {result[3]:.0f}\")\n",
    "        print(f\"Average Account Balance: ${result[4]:,.0f}\")\n",
    "        \n",
    "        # Risk distribution (using correct column name)\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                risk_segment,\n",
    "                COUNT(*) as customers,\n",
    "                ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 1) as percentage\n",
    "            FROM customers\n",
    "            GROUP BY risk_segment\n",
    "            ORDER BY COUNT(*) DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"\\nRisk Segment Distribution:\")\n",
    "        for row in cur.fetchall():\n",
    "            risk_segment, count, percentage = row\n",
    "            print(f\"  {risk_segment}: {count:,} customers ({percentage}%)\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Customer verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the fixed customer import\n",
    "print(\"üîÑ Running FIXED customer import (skip existing)...\")\n",
    "if import_customer_data_fixed(db_config):\n",
    "    print(\"\\nüéâ Customer data import complete!\")\n",
    "    # Run verification\n",
    "    verify_customer_data_fixed(db_config)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please check customer import errors\")\n",
    "\n",
    "# Alternative option\n",
    "print(\"\\nüîÑ Alternative: Replace existing customer data...\")\n",
    "print(\"Uncomment the line below if you want to replace all existing customer data:\")\n",
    "print(\"# import_customer_data_fixed(db_config, replace_existing=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0efb1e-6120-4684-b453-3ed7a2006bdd",
   "metadata": {},
   "source": [
    "Comprehensive Data Validation\n",
    "=====================================\n",
    "\n",
    "Now that all data is imported, let's run comprehensive\n",
    "validation queries to ensure data integrity across tables.\n",
    "\n",
    "This includes:\n",
    "1. Referential integrity checks\n",
    "2. Data completeness analysis\n",
    "3. Anomaly detection\n",
    "4. Cross-table consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "722058a6-62d2-49ba-a13b-915b1c5cee55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running Comprehensive Data Validation\n",
      "============================================================\n",
      "\n",
      "1. Table Row Counts:\n",
      "----------------------------------------\n",
      "  stock_prices        :     26,100 rows\n",
      "  crypto_prices       :     73,050 rows\n",
      "  economic_indicators :        600 rows\n",
      "  portfolio_holdings  :      6,000 rows\n",
      "  customers           :     10,000 rows\n",
      "\n",
      "2. Date Range Analysis:\n",
      "----------------------------------------\n",
      "  Stock prices: 2020-01-01 to 2024-12-31 (1305 days)\n",
      "  Economic data: 2020-01-31 to 2024-12-31 (60 months)\n",
      "\n",
      "3. Missing Data Check:\n",
      "----------------------------------------\n",
      "  Missing trading days: 0\n",
      "\n",
      "4. Data Integrity Checks:\n",
      "----------------------------------------\n",
      "  Invalid OHLC relationships: 0\n",
      "  Invalid portfolio weights: 0\n",
      "\n",
      "5. Statistical Anomaly Detection:\n",
      "----------------------------------------\n",
      "  Extreme daily returns (>20%): 0\n",
      "  Maximum daily return: 0.0%\n",
      "\n",
      "6. Cross-Table Consistency:\n",
      "----------------------------------------\n",
      "  Stock/Economic date overlap: Full overlap\n",
      "\n",
      "7. Data Quality Score:\n",
      "----------------------------------------\n",
      "  Total checks: 8\n",
      "  Passed: 8\n",
      "  Failed: 0\n",
      "  Quality Score: 100.0%\n",
      "\n",
      "üìÑ Detailed report saved to: data_quality_report.md\n"
     ]
    }
   ],
   "source": [
    "def run_data_validation(config):\n",
    "    \"\"\"\n",
    "    Run comprehensive validation queries across all tables.\n",
    "    Generate a data quality report.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Running Comprehensive Data Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        validation_results = []\n",
    "        \n",
    "        # 1. Table Row Counts\n",
    "        print(\"\\n1. Table Row Counts:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        tables = ['stock_prices', 'crypto_prices', 'economic_indicators', \n",
    "                  'portfolio_holdings', 'customers']\n",
    "        \n",
    "        for table in tables:\n",
    "            cur.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "            count = cur.fetchone()[0]\n",
    "            print(f\"  {table:20}: {count:>10,} rows\")\n",
    "            validation_results.append({\n",
    "                'check': f'{table} row count',\n",
    "                'result': count,\n",
    "                'status': 'PASS' if count > 0 else 'FAIL'\n",
    "            })\n",
    "        \n",
    "        # 2. Date Range Consistency\n",
    "        print(\"\\n2. Date Range Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Stock prices date range\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT MIN(date) as min_date, MAX(date) as max_date,\n",
    "                   COUNT(DISTINCT date) as trading_days\n",
    "            FROM stock_prices\n",
    "        \"\"\")\n",
    "        stock_dates = cur.fetchone()\n",
    "        print(f\"  Stock prices: {stock_dates[0]} to {stock_dates[1]} ({stock_dates[2]} days)\")\n",
    "        \n",
    "        # Economic indicators date range\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT MIN(date) as min_date, MAX(date) as max_date,\n",
    "                   COUNT(DISTINCT date) as months\n",
    "            FROM economic_indicators\n",
    "        \"\"\")\n",
    "        econ_dates = cur.fetchone()\n",
    "        print(f\"  Economic data: {econ_dates[0]} to {econ_dates[1]} ({econ_dates[2]} months)\")\n",
    "        \n",
    "        # 3. Missing Data Analysis\n",
    "        print(\"\\n3. Missing Data Check:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check for missing trading days (excluding weekends)\n",
    "        cur.execute(\"\"\"\n",
    "            WITH expected_days AS (\n",
    "                SELECT generate_series(\n",
    "                    (SELECT MIN(date) FROM stock_prices),\n",
    "                    (SELECT MAX(date) FROM stock_prices),\n",
    "                    '1 day'::interval\n",
    "                )::date AS trading_date\n",
    "            ),\n",
    "            actual_days AS (\n",
    "                SELECT DISTINCT date FROM stock_prices\n",
    "            )\n",
    "            SELECT COUNT(*) as missing_days\n",
    "            FROM expected_days e\n",
    "            LEFT JOIN actual_days a ON e.trading_date = a.date\n",
    "            WHERE a.date IS NULL\n",
    "              AND EXTRACT(DOW FROM e.trading_date) NOT IN (0, 6)\n",
    "        \"\"\")\n",
    "        \n",
    "        missing_days = cur.fetchone()[0]\n",
    "        print(f\"  Missing trading days: {missing_days}\")\n",
    "        validation_results.append({\n",
    "            'check': 'Missing trading days',\n",
    "            'result': missing_days,\n",
    "            'status': 'PASS' if missing_days < 10 else 'WARNING'\n",
    "        })\n",
    "        \n",
    "        # 4. Data Integrity Checks\n",
    "        print(\"\\n4. Data Integrity Checks:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check OHLC consistency\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT COUNT(*) as invalid_ohlc\n",
    "            FROM stock_prices\n",
    "            WHERE high < low \n",
    "               OR high < open \n",
    "               OR high < close\n",
    "               OR low > open \n",
    "               OR low > close\n",
    "        \"\"\")\n",
    "        invalid_ohlc = cur.fetchone()[0]\n",
    "        print(f\"  Invalid OHLC relationships: {invalid_ohlc}\")\n",
    "        validation_results.append({\n",
    "            'check': 'OHLC consistency',\n",
    "            'result': invalid_ohlc,\n",
    "            'status': 'PASS' if invalid_ohlc == 0 else 'FAIL'\n",
    "        })\n",
    "        \n",
    "        # Check portfolio weight sums\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT COUNT(*) as invalid_weights\n",
    "            FROM portfolio_holdings\n",
    "            WHERE ABS((stock_weight + bond_weight + cash_weight) - 1.0) > 0.01\n",
    "        \"\"\")\n",
    "        invalid_weights = cur.fetchone()[0]\n",
    "        print(f\"  Invalid portfolio weights: {invalid_weights}\")\n",
    "        validation_results.append({\n",
    "            'check': 'Portfolio weight sums',\n",
    "            'result': invalid_weights,\n",
    "            'status': 'PASS' if invalid_weights == 0 else 'FAIL'\n",
    "        })\n",
    "        \n",
    "        # 5. Statistical Anomaly Detection\n",
    "        print(\"\\n5. Statistical Anomaly Detection:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check for extreme returns\n",
    "        cur.execute(\"\"\"\n",
    "            WITH daily_returns AS (\n",
    "                SELECT \n",
    "                    symbol,\n",
    "                    date,\n",
    "                    (close / LAG(close) OVER (PARTITION BY symbol ORDER BY date) - 1) as return\n",
    "                FROM stock_prices\n",
    "            )\n",
    "            SELECT \n",
    "                COUNT(*) as extreme_returns,\n",
    "                MAX(ABS(return)) as max_return\n",
    "            FROM daily_returns\n",
    "            WHERE ABS(return) > 0.20  -- 20% daily move\n",
    "        \"\"\")\n",
    "        result = cur.fetchone()\n",
    "        extreme_returns, max_return = result[0], result[1] if result[1] is not None else 0\n",
    "        print(f\"  Extreme daily returns (>20%): {extreme_returns}\")\n",
    "        print(f\"  Maximum daily return: {max_return*100:.1f}%\")\n",
    "        \n",
    "        # 6. Cross-Table Consistency\n",
    "        print(\"\\n6. Cross-Table Consistency:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check date overlap between tables\n",
    "        cur.execute(\"\"\"\n",
    "            WITH stock_dates AS (\n",
    "                SELECT MIN(date) as min_date, MAX(date) as max_date FROM stock_prices\n",
    "            ),\n",
    "            econ_dates AS (\n",
    "                SELECT MIN(date) as min_date, MAX(date) as max_date FROM economic_indicators\n",
    "            )\n",
    "            SELECT \n",
    "                CASE \n",
    "                    WHEN s.min_date <= e.min_date AND s.max_date >= e.max_date THEN 'Full overlap'\n",
    "                    WHEN s.max_date < e.min_date OR s.min_date > e.max_date THEN 'No overlap'\n",
    "                    ELSE 'Partial overlap'\n",
    "                END as date_overlap\n",
    "            FROM stock_dates s, econ_dates e\n",
    "        \"\"\")\n",
    "        date_overlap = cur.fetchone()[0]\n",
    "        print(f\"  Stock/Economic date overlap: {date_overlap}\")\n",
    "        \n",
    "        # 7. Generate Quality Score\n",
    "        print(\"\\n7. Data Quality Score:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        total_checks = len(validation_results)\n",
    "        passed_checks = sum(1 for r in validation_results if r['status'] == 'PASS')\n",
    "        quality_score = (passed_checks / total_checks) * 100 if total_checks > 0 else 0\n",
    "        \n",
    "        print(f\"  Total checks: {total_checks}\")\n",
    "        print(f\"  Passed: {passed_checks}\")\n",
    "        print(f\"  Failed: {total_checks - passed_checks}\")\n",
    "        print(f\"  Quality Score: {quality_score:.1f}%\")\n",
    "        \n",
    "        # Generate detailed report\n",
    "        report_content = f\"\"\"\n",
    "# Data Quality Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Summary\n",
    "- Quality Score: {quality_score:.1f}%\n",
    "- Total Checks: {total_checks}\n",
    "- Passed: {passed_checks}\n",
    "- Failed: {total_checks - passed_checks}\n",
    "\n",
    "## Detailed Results\n",
    "\"\"\"\n",
    "        \n",
    "        for result in validation_results:\n",
    "            status_emoji = \"PASS\" if result['status'] == 'PASS' else \"FAIL\"\n",
    "            report_content += f\"- {result['check']}: {result['result']} [{status_emoji}]\\n\"\n",
    "        \n",
    "        # Save report with UTF-8 encoding\n",
    "        with open('data_quality_report.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(report_content)\n",
    "        \n",
    "        print(f\"\\nüìÑ Detailed report saved to: data_quality_report.md\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return validation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run validation\n",
    "validation_results = run_data_validation(db_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff45bf4-2556-43fc-8d98-17a3d675a41b",
   "metadata": {},
   "source": [
    "Creating Analysis Views and Performance Indexes\n",
    "=======================================================\n",
    "\n",
    "Views are like \"saved queries\" that simplify complex analysis.\n",
    "Indexes make queries faster by creating lookup tables.\n",
    "\n",
    "We'll create views for:\n",
    "1. Daily returns calculation\n",
    "2. Moving averages\n",
    "3. Volatility metrics\n",
    "4. Correlation matrices\n",
    "5. Portfolio performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86df0aa6-2523-4005-ac9a-5eb84f26d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting FIXED infrastructure setup...\n",
      "üèóÔ∏è Creating Fixed Analysis Infrastructure\n",
      "============================================================\n",
      "\n",
      "üßπ PART 0: Comprehensive Cleanup\n",
      "----------------------------------------\n",
      "‚úÖ Executed: IF EXISTS\n",
      "‚úÖ Executed: VIEW IF\n",
      "‚úÖ Executed: IF EXISTS\n",
      "‚úÖ Executed: IF EXISTS\n",
      "‚úÖ Cleanup completed\n",
      "\n",
      "üìä PART 1: Creating Fixed Analysis Views\n",
      "----------------------------------------\n",
      "Creating view: daily_returns... ‚úÖ\n",
      "Creating view: moving_averages_fixed... ‚úÖ\n",
      "Creating view: volatility_metrics_fixed... ‚úÖ\n",
      "Creating view: market_summary_fixed... ‚úÖ\n",
      "\n",
      "üìä PART 2: Setting Up Fixed Correlation Analysis\n",
      "----------------------------------------\n",
      "Creating correlation storage table... ‚úÖ\n",
      "Calculating correlations for 10 symbols...\n",
      "  Progress: 88.9% (40/45)\n",
      "‚úÖ Calculated 45 correlations\n",
      "Creating correlation view... ‚úÖ\n",
      "\n",
      "üîç PART 3: Testing Fixed Infrastructure\n",
      "----------------------------------------\n",
      "\n",
      "Market Summary (Top Movers):\n",
      "     symbol    | current_pric | pct_change_3\n",
      "  ------------------------------------------\n",
      "       KO      |    77.20     |    -5.63    \n",
      "      META     |    233.16    |     4.54    \n",
      "      JNJ      |    319.85    |    -4.38    \n",
      "\n",
      "Recent Volatility Leaders (Fixed):\n",
      "     symbol    |     date     | volatility_p\n",
      "  ------------------------------------------\n",
      "      NKE      |  2024-12-27  |     62.7    \n",
      "      NKE      |  2024-12-30  |     62.6    \n",
      "      WMT      |  2024-12-26  |     61.0    \n",
      "\n",
      "Top Correlations:\n",
      "    symbol1    |   symbol2    |     corr     |     obs     \n",
      "  ---------------------------------------------------------\n",
      "      AAPL     |     AMZN     |    0.0688    |   1304.00   \n",
      "       HD      |     JNJ      |    0.0622    |   1304.00   \n",
      "      DIS      |     JNJ      |    0.0540    |   1304.00   \n",
      "\n",
      "Moving Average Signals (Fixed):\n",
      "     symbol    |    price     |    sma20     |    signal   \n",
      "  ---------------------------------------------------------\n",
      "      AAPL     |    425.73    |    423.14    |    Above    \n",
      "      AMZN     |    360.01    |    354.54    |    Above    \n",
      "      BAC      |    365.12    |    369.04    |    Below    \n",
      "\n",
      "============================================================\n",
      "üìä FIXED ANALYSIS INFRASTRUCTURE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Views:\n",
      "  ‚úÖ Created: 4\n",
      "  ‚ùå Failed: 0\n",
      "  Available views: daily_returns, moving_averages_fixed, volatility_metrics_fixed, market_summary_fixed\n",
      "\n",
      "Correlations:\n",
      "  Status: success\n",
      "  Calculated: 45 pairs\n",
      "\n",
      "üìö Available Fixed Analysis Tools:\n",
      "----------------------------------------\n",
      "Views:\n",
      "  ‚úì daily_returns\n",
      "  ‚úì moving_averages_fixed\n",
      "  ‚úì volatility_metrics_fixed\n",
      "  ‚úì market_summary_fixed\n",
      "  ‚úì stock_correlation_view\n",
      "\n",
      "üìù Example Usage (Fixed):\n",
      "----------------------------------------\n",
      "-- Get top correlations\n",
      "SELECT * FROM stock_correlation_view LIMIT 10;\n",
      "\n",
      "-- Find volatile stocks\n",
      "SELECT symbol, volatility_20d\n",
      "FROM volatility_metrics_fixed \n",
      "WHERE date = (SELECT MAX(date) FROM volatility_metrics_fixed)\n",
      "  AND volatility_20d IS NOT NULL\n",
      "ORDER BY volatility_20d DESC;\n",
      "\n",
      "-- Moving average analysis\n",
      "SELECT symbol, close, sma_twenty, (close/sma_twenty - 1)*100 as pct_above_sma\n",
      "FROM moving_averages_fixed \n",
      "WHERE date = (SELECT MAX(date) FROM moving_averages_fixed)\n",
      "  AND sma_twenty IS NOT NULL;\n",
      "\n",
      "‚úÖ Fixed analysis infrastructure setup complete!\n",
      "\n",
      "üîç Final Validation:\n",
      "  daily_returns: 26,100 records ‚úÖ\n",
      "  moving_averages_fixed: 26,100 records ‚úÖ\n",
      "  volatility_metrics_fixed: 26,080 records ‚úÖ\n",
      "  market_summary_fixed: 20 records ‚úÖ\n",
      "  stock_correlations: 45 pairs ‚úÖ\n",
      "\n",
      "üíæ FIXED Infrastructure setup complete! All PostgreSQL compatibility issues resolved.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analysis Infrastructure Setup\n",
    "==================================\n",
    "\"\"\"\n",
    "\n",
    "def create_fixed_analysis_infrastructure(config):\n",
    "    \"\"\"\n",
    "    Create analysis infrastructure with all PostgreSQL compatibility fixes.\n",
    "    \"\"\"\n",
    "    print(\"üèóÔ∏è Creating Fixed Analysis Infrastructure\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    results = {\n",
    "        'views': {'created': [], 'failed': []},\n",
    "        'indexes': {'created': [], 'failed': []},\n",
    "        'correlations': {'status': 'pending', 'count': 0}\n",
    "    }\n",
    "    \n",
    "    # PART 0: COMPREHENSIVE CLEANUP\n",
    "    print(\"\\nüßπ PART 0: Comprehensive Cleanup\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Drop all problematic objects\n",
    "        cleanup_commands = [\n",
    "            \"DROP VIEW IF EXISTS correlation_matrix CASCADE\",\n",
    "            \"DROP MATERIALIZED VIEW IF EXISTS correlation_matrix CASCADE\", \n",
    "            \"DROP VIEW IF EXISTS moving_averages CASCADE\",\n",
    "            \"DROP VIEW IF EXISTS volatility_metrics CASCADE\"\n",
    "        ]\n",
    "        \n",
    "        for cmd in cleanup_commands:\n",
    "            try:\n",
    "                cur.execute(cmd)\n",
    "                print(f\"‚úÖ Executed: {cmd.split()[2]} {cmd.split()[3]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ÑπÔ∏è {cmd.split()[2]} {cmd.split()[3]} didn't exist\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(\"‚úÖ Cleanup completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Cleanup error: {e}\")\n",
    "    \n",
    "    # PART 1: CREATE FIXED VIEWS\n",
    "    print(\"\\nüìä PART 1: Creating Fixed Analysis Views\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    views = {\n",
    "        'daily_returns': \"\"\"\n",
    "            CREATE OR REPLACE VIEW daily_returns AS\n",
    "            WITH price_data AS (\n",
    "                SELECT \n",
    "                    date,\n",
    "                    symbol,\n",
    "                    close,\n",
    "                    LAG(close) OVER (PARTITION BY symbol ORDER BY date) as prev_close\n",
    "                FROM stock_prices\n",
    "            )\n",
    "            SELECT \n",
    "                date,\n",
    "                symbol,\n",
    "                close,\n",
    "                prev_close,\n",
    "                CASE \n",
    "                    WHEN prev_close IS NOT NULL AND prev_close > 0 \n",
    "                    THEN (close - prev_close) / prev_close \n",
    "                    ELSE NULL \n",
    "                END as simple_return,\n",
    "                CASE \n",
    "                    WHEN prev_close IS NOT NULL AND prev_close > 0 \n",
    "                    THEN LN(close / prev_close) \n",
    "                    ELSE NULL \n",
    "                END as log_return\n",
    "            FROM price_data\n",
    "            ORDER BY symbol, date\n",
    "        \"\"\",\n",
    "        \n",
    "        'moving_averages_fixed': \"\"\"\n",
    "            CREATE OR REPLACE VIEW moving_averages_fixed AS\n",
    "            SELECT \n",
    "                date,\n",
    "                symbol,\n",
    "                close,\n",
    "                volume,\n",
    "                -- Simple moving averages with unique names\n",
    "                AVG(close) OVER (\n",
    "                    PARTITION BY symbol \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 9 PRECEDING AND CURRENT ROW\n",
    "                ) as sma_ten,\n",
    "                AVG(close) OVER (\n",
    "                    PARTITION BY symbol \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "                ) as sma_twenty,\n",
    "                AVG(close) OVER (\n",
    "                    PARTITION BY symbol \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 49 PRECEDING AND CURRENT ROW\n",
    "                ) as sma_fifty,\n",
    "                -- Volume weighted average price (20-day)\n",
    "                CASE \n",
    "                    WHEN SUM(volume) OVER (\n",
    "                        PARTITION BY symbol \n",
    "                        ORDER BY date \n",
    "                        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "                    ) > 0 THEN\n",
    "                    SUM(close * volume) OVER (\n",
    "                        PARTITION BY symbol \n",
    "                        ORDER BY date \n",
    "                        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "                    ) / SUM(volume) OVER (\n",
    "                        PARTITION BY symbol \n",
    "                        ORDER BY date \n",
    "                        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "                    )\n",
    "                    ELSE NULL\n",
    "                END as vwap_twenty\n",
    "            FROM stock_prices\n",
    "            ORDER BY symbol, date\n",
    "        \"\"\",\n",
    "        \n",
    "        'volatility_metrics_fixed': \"\"\"\n",
    "            CREATE OR REPLACE VIEW volatility_metrics_fixed AS\n",
    "            WITH returns AS (\n",
    "                SELECT * FROM daily_returns WHERE log_return IS NOT NULL\n",
    "            )\n",
    "            SELECT \n",
    "                symbol,\n",
    "                date,\n",
    "                log_return,\n",
    "                -- Rolling volatility (20-day) with proper casting\n",
    "                CASE \n",
    "                    WHEN COUNT(log_return) OVER (\n",
    "                        PARTITION BY symbol \n",
    "                        ORDER BY date \n",
    "                        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "                    ) >= 20 THEN\n",
    "                    STDDEV(log_return) OVER (\n",
    "                        PARTITION BY symbol \n",
    "                        ORDER BY date \n",
    "                        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "                    ) * SQRT(252)\n",
    "                    ELSE NULL\n",
    "                END as volatility_20d,\n",
    "                -- Rolling volatility (60-day)\n",
    "                CASE \n",
    "                    WHEN COUNT(log_return) OVER (\n",
    "                        PARTITION BY symbol \n",
    "                        ORDER BY date \n",
    "                        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW\n",
    "                    ) >= 60 THEN\n",
    "                    STDDEV(log_return) OVER (\n",
    "                        PARTITION BY symbol \n",
    "                        ORDER BY date \n",
    "                        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW\n",
    "                    ) * SQRT(252)\n",
    "                    ELSE NULL\n",
    "                END as volatility_60d\n",
    "            FROM returns\n",
    "        \"\"\",\n",
    "        \n",
    "        'market_summary_fixed': \"\"\"\n",
    "            CREATE OR REPLACE VIEW market_summary_fixed AS\n",
    "            WITH latest_data AS (\n",
    "                SELECT DISTINCT ON (symbol) \n",
    "                    symbol,\n",
    "                    date,\n",
    "                    close,\n",
    "                    volume\n",
    "                FROM stock_prices\n",
    "                ORDER BY symbol, date DESC\n",
    "            ),\n",
    "            period_stats AS (\n",
    "                SELECT \n",
    "                    symbol,\n",
    "                    AVG(close) as avg_price_30d,\n",
    "                    MIN(close) as min_price_30d,\n",
    "                    MAX(close) as max_price_30d,\n",
    "                    SUM(volume) as total_volume_30d\n",
    "                FROM stock_prices\n",
    "                WHERE date >= (SELECT MAX(date) FROM stock_prices) - INTERVAL '30 days'\n",
    "                GROUP BY symbol\n",
    "            ),\n",
    "            return_stats AS (\n",
    "                SELECT \n",
    "                    symbol,\n",
    "                    AVG(log_return) * 252 as annual_return,\n",
    "                    STDDEV(log_return) * SQRT(252) as annual_volatility\n",
    "                FROM daily_returns\n",
    "                WHERE log_return IS NOT NULL\n",
    "                  AND date >= (SELECT MAX(date) FROM stock_prices) - INTERVAL '252 days'\n",
    "                GROUP BY symbol\n",
    "            )\n",
    "            SELECT \n",
    "                l.symbol,\n",
    "                l.date as last_update,\n",
    "                l.close as current_price,\n",
    "                p.avg_price_30d,\n",
    "                CASE \n",
    "                    WHEN p.avg_price_30d > 0 \n",
    "                    THEN (l.close - p.avg_price_30d) / p.avg_price_30d \n",
    "                    ELSE 0 \n",
    "                END as pct_from_avg_30d,\n",
    "                p.min_price_30d,\n",
    "                p.max_price_30d,\n",
    "                r.annual_return,\n",
    "                r.annual_volatility,\n",
    "                CASE \n",
    "                    WHEN r.annual_volatility > 0 \n",
    "                    THEN r.annual_return / r.annual_volatility \n",
    "                    ELSE 0 \n",
    "                END as sharpe_ratio\n",
    "            FROM latest_data l\n",
    "            LEFT JOIN period_stats p ON l.symbol = p.symbol\n",
    "            LEFT JOIN return_stats r ON l.symbol = r.symbol\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    # Create each view with error handling\n",
    "    for view_name, view_sql in views.items():\n",
    "        try:\n",
    "            conn = psycopg2.connect(**config.get_connection_params())\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            \n",
    "            print(f\"Creating view: {view_name}...\", end='')\n",
    "            cur.execute(view_sql)\n",
    "            results['views']['created'].append(view_name)\n",
    "            print(\" ‚úÖ\")\n",
    "            \n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" ‚ùå Error: {str(e)[:80]}...\")\n",
    "            results['views']['failed'].append((view_name, str(e)))\n",
    "            if 'conn' in locals():\n",
    "                conn.close()\n",
    "    \n",
    "    # PART 2: CREATE CORRELATION TABLE (NOT VIEW)\n",
    "    print(\"\\nüìä PART 2: Setting Up Fixed Correlation Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Create correlation storage table\n",
    "        print(\"Creating correlation storage table...\", end='')\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS stock_correlations (\n",
    "                symbol1 VARCHAR(10),\n",
    "                symbol2 VARCHAR(10),\n",
    "                correlation NUMERIC(10,6),\n",
    "                observations INTEGER,\n",
    "                period_start DATE,\n",
    "                period_end DATE,\n",
    "                calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                PRIMARY KEY (symbol1, symbol2)\n",
    "            )\n",
    "        \"\"\")\n",
    "        print(\" ‚úÖ\")\n",
    "        \n",
    "        # Get list of symbols for correlation calculation\n",
    "        cur.execute(\"SELECT DISTINCT symbol FROM stock_prices ORDER BY symbol LIMIT 10\")\n",
    "        symbols = [row[0] for row in cur.fetchall()]\n",
    "        print(f\"Calculating correlations for {len(symbols)} symbols...\")\n",
    "        \n",
    "        # Clear old correlations\n",
    "        cur.execute(\"TRUNCATE stock_correlations\")\n",
    "        \n",
    "        # Calculate correlations for each pair\n",
    "        correlation_count = 0\n",
    "        total_pairs = len(symbols) * (len(symbols) - 1) // 2\n",
    "        \n",
    "        for i, symbol1 in enumerate(symbols):\n",
    "            for j, symbol2 in enumerate(symbols[i+1:], i+1):\n",
    "                # Progress indicator\n",
    "                if correlation_count % 5 == 0:\n",
    "                    progress = (correlation_count / total_pairs) * 100 if total_pairs > 0 else 0\n",
    "                    print(f\"\\r  Progress: {progress:.1f}% ({correlation_count}/{total_pairs})\", end='')\n",
    "                \n",
    "                # Calculate correlation with sufficient data\n",
    "                cur.execute(\"\"\"\n",
    "                    WITH paired_returns AS (\n",
    "                        SELECT \n",
    "                            r1.date,\n",
    "                            r1.log_return as return1,\n",
    "                            r2.log_return as return2\n",
    "                        FROM daily_returns r1\n",
    "                        JOIN daily_returns r2 ON r1.date = r2.date\n",
    "                        WHERE r1.symbol = %s \n",
    "                          AND r2.symbol = %s\n",
    "                          AND r1.log_return IS NOT NULL\n",
    "                          AND r2.log_return IS NOT NULL\n",
    "                    )\n",
    "                    SELECT \n",
    "                        CORR(return1, return2) as correlation,\n",
    "                        COUNT(*) as observations,\n",
    "                        MIN(date) as period_start,\n",
    "                        MAX(date) as period_end\n",
    "                    FROM paired_returns\n",
    "                    HAVING COUNT(*) >= 30\n",
    "                \"\"\", (symbol1, symbol2))\n",
    "                \n",
    "                result = cur.fetchone()\n",
    "                if result and result[0] is not None:\n",
    "                    cur.execute(\"\"\"\n",
    "                        INSERT INTO stock_correlations \n",
    "                        (symbol1, symbol2, correlation, observations, period_start, period_end)\n",
    "                        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                        ON CONFLICT (symbol1, symbol2) DO UPDATE SET\n",
    "                        correlation = EXCLUDED.correlation,\n",
    "                        observations = EXCLUDED.observations,\n",
    "                        calculated_at = CURRENT_TIMESTAMP\n",
    "                    \"\"\", (symbol1, symbol2, result[0], result[1], result[2], result[3]))\n",
    "                    correlation_count += 1\n",
    "        \n",
    "        print(f\"\\n‚úÖ Calculated {correlation_count} correlations\")\n",
    "        results['correlations']['count'] = correlation_count\n",
    "        results['correlations']['status'] = 'success'\n",
    "        \n",
    "        # Create simple correlation view\n",
    "        print(\"Creating correlation view...\", end='')\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW stock_correlation_view AS\n",
    "            SELECT \n",
    "                symbol1, \n",
    "                symbol2, \n",
    "                ROUND(CAST(correlation AS numeric), 4) as correlation, \n",
    "                observations\n",
    "            FROM stock_correlations\n",
    "            ORDER BY correlation DESC\n",
    "        \"\"\")\n",
    "        print(\" ‚úÖ\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Correlation calculation error: {e}\")\n",
    "        results['correlations']['status'] = 'failed'\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "    \n",
    "    # PART 3: TEST FIXED INFRASTRUCTURE\n",
    "    print(\"\\nüîç PART 3: Testing Fixed Infrastructure\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    test_queries = {\n",
    "        \"Market Summary (Top Movers)\": \"\"\"\n",
    "            SELECT symbol, \n",
    "                   ROUND(CAST(current_price AS numeric), 2) as current_price,\n",
    "                   ROUND(CAST(pct_from_avg_30d * 100 AS numeric), 2) as pct_change_30d\n",
    "            FROM market_summary_fixed\n",
    "            WHERE current_price IS NOT NULL\n",
    "            ORDER BY ABS(pct_from_avg_30d) DESC NULLS LAST\n",
    "            LIMIT 5\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Recent Volatility Leaders (Fixed)\": \"\"\"\n",
    "            SELECT symbol, \n",
    "                   date,\n",
    "                   ROUND(CAST(volatility_20d * 100 AS numeric), 1) as volatility_pct\n",
    "            FROM volatility_metrics_fixed\n",
    "            WHERE volatility_20d IS NOT NULL\n",
    "              AND date >= (SELECT MAX(date) FROM volatility_metrics_fixed) - INTERVAL '5 days'\n",
    "            ORDER BY volatility_20d DESC\n",
    "            LIMIT 5\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Top Correlations\": \"\"\"\n",
    "            SELECT symbol1, symbol2, \n",
    "                   correlation as corr,\n",
    "                   observations as obs\n",
    "            FROM stock_correlation_view\n",
    "            WHERE correlation IS NOT NULL\n",
    "            ORDER BY correlation DESC\n",
    "            LIMIT 5\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Moving Average Signals (Fixed)\": \"\"\"\n",
    "            SELECT symbol,\n",
    "                   ROUND(CAST(close AS numeric), 2) as price,\n",
    "                   ROUND(CAST(sma_twenty AS numeric), 2) as sma20,\n",
    "                   CASE \n",
    "                       WHEN close > sma_twenty THEN 'Above'\n",
    "                       WHEN close < sma_twenty THEN 'Below'\n",
    "                       ELSE 'At'\n",
    "                   END as signal\n",
    "            FROM moving_averages_fixed\n",
    "            WHERE date = (SELECT MAX(date) FROM moving_averages_fixed)\n",
    "              AND sma_twenty IS NOT NULL\n",
    "            ORDER BY symbol\n",
    "            LIMIT 5\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    for query_name, query_sql in test_queries.items():\n",
    "        print(f\"\\n{query_name}:\")\n",
    "        try:\n",
    "            conn = psycopg2.connect(**config.get_connection_params())\n",
    "            cur = conn.cursor()\n",
    "            \n",
    "            cur.execute(query_sql)\n",
    "            results_data = cur.fetchall()\n",
    "            \n",
    "            if results_data:\n",
    "                # Get column names\n",
    "                columns = [desc[0] for desc in cur.description]\n",
    "                \n",
    "                # Print header\n",
    "                header = \" | \".join(f\"{col[:12]:^12}\" for col in columns)\n",
    "                print(f\"  {header}\")\n",
    "                print(f\"  {'-' * len(header)}\")\n",
    "                \n",
    "                # Print data (limit to first 3 rows for readability)\n",
    "                for row in results_data[:3]:\n",
    "                    formatted_row = []\n",
    "                    for val in row:\n",
    "                        if isinstance(val, (int, float)):\n",
    "                            formatted_row.append(f\"{val:^12.2f}\")\n",
    "                        else:\n",
    "                            formatted_row.append(f\"{str(val)[:12]:^12}\")\n",
    "                    print(f\"  {' | '.join(formatted_row)}\")\n",
    "            else:\n",
    "                print(\"  No data available\")\n",
    "            \n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Query failed: {e}\")\n",
    "    \n",
    "    # FINAL SUMMARY\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä FIXED ANALYSIS INFRASTRUCTURE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nViews:\")\n",
    "    print(f\"  ‚úÖ Created: {len(results['views']['created'])}\")\n",
    "    print(f\"  ‚ùå Failed: {len(results['views']['failed'])}\")\n",
    "    if results['views']['created']:\n",
    "        print(f\"  Available views: {', '.join(results['views']['created'])}\")\n",
    "    \n",
    "    print(f\"\\nCorrelations:\")\n",
    "    print(f\"  Status: {results['correlations']['status']}\")\n",
    "    print(f\"  Calculated: {results['correlations']['count']} pairs\")\n",
    "    \n",
    "    print(\"\\nüìö Available Fixed Analysis Tools:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Views:\")\n",
    "    for view in results['views']['created']:\n",
    "        print(f\"  ‚úì {view}\")\n",
    "    print(\"  ‚úì stock_correlation_view\")\n",
    "    \n",
    "    print(\"\\nüìù Example Usage (Fixed):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"-- Get top correlations\")\n",
    "    print(\"SELECT * FROM stock_correlation_view LIMIT 10;\")\n",
    "    print(\"\")\n",
    "    print(\"-- Find volatile stocks\")\n",
    "    print(\"SELECT symbol, volatility_20d\")\n",
    "    print(\"FROM volatility_metrics_fixed \")\n",
    "    print(\"WHERE date = (SELECT MAX(date) FROM volatility_metrics_fixed)\")\n",
    "    print(\"  AND volatility_20d IS NOT NULL\")\n",
    "    print(\"ORDER BY volatility_20d DESC;\")\n",
    "    print(\"\")\n",
    "    print(\"-- Moving average analysis\")\n",
    "    print(\"SELECT symbol, close, sma_twenty, (close/sma_twenty - 1)*100 as pct_above_sma\")\n",
    "    print(\"FROM moving_averages_fixed \")\n",
    "    print(\"WHERE date = (SELECT MAX(date) FROM moving_averages_fixed)\")\n",
    "    print(\"  AND sma_twenty IS NOT NULL;\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Fixed analysis infrastructure setup complete!\")\n",
    "    return results\n",
    "\n",
    "# Run the fixed infrastructure setup\n",
    "print(\"üöÄ Starting FIXED infrastructure setup...\")\n",
    "fixed_results = create_fixed_analysis_infrastructure(db_config)\n",
    "\n",
    "# Quick validation\n",
    "print(\"\\nüîç Final Validation:\")\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config.get_connection_params())\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Test each created view\n",
    "    for view_name in fixed_results['views']['created']:\n",
    "        try:\n",
    "            cur.execute(f\"SELECT COUNT(*) FROM {view_name}\")\n",
    "            count = cur.fetchone()[0]\n",
    "            print(f\"  {view_name}: {count:,} records ‚úÖ\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {view_name}: Error - {e}\")\n",
    "    \n",
    "    # Test correlation table\n",
    "    try:\n",
    "        cur.execute(\"SELECT COUNT(*) FROM stock_correlations\")\n",
    "        corr_count = cur.fetchone()[0]\n",
    "        print(f\"  stock_correlations: {corr_count} pairs ‚úÖ\")\n",
    "    except Exception as e:\n",
    "        print(f\"  stock_correlations: Error - {e}\")\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  Validation error: {e}\")\n",
    "\n",
    "print(\"\\nüíæ FIXED Infrastructure setup complete! All PostgreSQL compatibility issues resolved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd84028-be6e-437d-a694-07ed13840458",
   "metadata": {},
   "source": [
    "SQL Practice Exercises for Financial Analysis\n",
    "=====================================================\n",
    "\n",
    "Now let's practice SQL with real financial queries.\n",
    "These exercises progress from basic to advanced.\n",
    "\n",
    "Try to solve each one before looking at the solution!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eec3d7f5-156d-4db2-ad81-20baea614068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù SQL Exercise Notebook\n",
      "============================================================\n",
      "\n",
      "üü¢ Exercise 1: Find Recent Prices (Beginner)\n",
      "----------------------------------------\n",
      "Question: Get the last 10 closing prices for Apple (AAPL)\n",
      "Hint: Use WHERE for filtering and ORDER BY with LIMIT\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "SELECT date, close\n",
      "FROM stock_prices\n",
      "WHERE symbol = 'AAPL'\n",
      "ORDER BY date DESC\n",
      "LIMIT 10;\n",
      "```\n",
      "\n",
      "üü¢ Exercise 2: Average Volume (Beginner)\n",
      "----------------------------------------\n",
      "Question: Calculate the average daily trading volume for each stock\n",
      "Hint: Use GROUP BY and AVG()\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "SELECT \n",
      "    symbol,\n",
      "    AVG(volume) as avg_daily_volume,\n",
      "    MIN(volume) as min_volume,\n",
      "    MAX(volume) as max_volume\n",
      "FROM stock_prices\n",
      "GROUP BY symbol\n",
      "ORDER BY avg_daily_volume DESC;\n",
      "```\n",
      "\n",
      "üü° Exercise 3: Monthly Performance (Intermediate)\n",
      "----------------------------------------\n",
      "Question: Calculate monthly returns for each stock in 2023\n",
      "Hint: Use DATE_TRUNC() and window functions\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "WITH monthly_prices AS (\n",
      "    SELECT \n",
      "        symbol,\n",
      "        DATE_TRUNC('month', date) as month,\n",
      "        FIRST_VALUE(open) OVER (PARTITION BY symbol, DATE_TRUNC('month', date) ORDER BY date) as month_open,\n",
      "        LAST_VALUE(close) OVER (PARTITION BY symbol, DATE_TRUNC('month', date) ORDER BY date \n",
      "                                RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as month_close\n",
      "    FROM stock_prices\n",
      "    WHERE EXTRACT(YEAR FROM date) = 2023\n",
      ")\n",
      "SELECT DISTINCT\n",
      "    symbol,\n",
      "    month,\n",
      "    month_open,\n",
      "    month_close,\n",
      "    (month_close - month_open) / month_open * 100 as monthly_return_pct\n",
      "FROM monthly_prices\n",
      "ORDER BY symbol, month;\n",
      "```\n",
      "\n",
      "üü° Exercise 4: Volatility Ranking (Intermediate)\n",
      "----------------------------------------\n",
      "Question: Rank stocks by their 30-day volatility\n",
      "Hint: Calculate standard deviation of returns\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "WITH daily_returns AS (\n",
      "    SELECT \n",
      "        symbol,\n",
      "        date,\n",
      "        (close - LAG(close) OVER (PARTITION BY symbol ORDER BY date)) / \n",
      "         LAG(close) OVER (PARTITION BY symbol ORDER BY date) as return\n",
      "    FROM stock_prices\n",
      "    WHERE date >= CURRENT_DATE - INTERVAL '30 days'\n",
      ")\n",
      "SELECT \n",
      "    symbol,\n",
      "    STDDEV(return) * SQRT(252) as annualized_volatility,\n",
      "    COUNT(*) as trading_days,\n",
      "    RANK() OVER (ORDER BY STDDEV(return) DESC) as volatility_rank\n",
      "FROM daily_returns\n",
      "WHERE return IS NOT NULL\n",
      "GROUP BY symbol\n",
      "HAVING COUNT(*) > 20  -- Ensure enough data points\n",
      "ORDER BY annualized_volatility DESC;\n",
      "```\n",
      "\n",
      "üî¥ Exercise 5: Portfolio Correlation (Advanced)\n",
      "----------------------------------------\n",
      "Question: Find which stocks move together (correlation > 0.7)\n",
      "Hint: Self-join daily returns and use CORR() function\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "WITH returns AS (\n",
      "    SELECT \n",
      "        date,\n",
      "        symbol,\n",
      "        (close - LAG(close) OVER (PARTITION BY symbol ORDER BY date)) / \n",
      "         LAG(close) OVER (PARTITION BY symbol ORDER BY date) as return\n",
      "    FROM stock_prices\n",
      ")\n",
      "SELECT \n",
      "    r1.symbol as symbol1,\n",
      "    r2.symbol as symbol2,\n",
      "    CORR(r1.return, r2.return) as correlation,\n",
      "    COUNT(*) as observations\n",
      "FROM returns r1\n",
      "JOIN returns r2 ON r1.date = r2.date\n",
      "WHERE r1.symbol < r2.symbol  -- Avoid duplicates\n",
      "  AND r1.return IS NOT NULL\n",
      "  AND r2.return IS NOT NULL\n",
      "GROUP BY r1.symbol, r2.symbol\n",
      "HAVING CORR(r1.return, r2.return) > 0.7\n",
      "   AND COUNT(*) > 100\n",
      "ORDER BY correlation DESC;\n",
      "```\n",
      "\n",
      "üî¥ Exercise 6: Economic Impact Analysis (Advanced)\n",
      "----------------------------------------\n",
      "Question: Analyze how stock returns correlate with GDP growth\n",
      "Hint: JOIN stock and economic data, calculate correlations\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "WITH monthly_stock_returns AS (\n",
      "    SELECT \n",
      "        DATE_TRUNC('month', date) as month,\n",
      "        symbol,\n",
      "        (MAX(close) - MIN(open)) / MIN(open) as monthly_return\n",
      "    FROM stock_prices\n",
      "    GROUP BY DATE_TRUNC('month', date), symbol\n",
      "),\n",
      "gdp_data AS (\n",
      "    SELECT \n",
      "        date,\n",
      "        value as gdp_growth\n",
      "    FROM economic_indicators\n",
      "    WHERE indicator = 'GDP_GROWTH'\n",
      ")\n",
      "SELECT \n",
      "    s.symbol,\n",
      "    CORR(s.monthly_return, g.gdp_growth) as return_gdp_correlation,\n",
      "    COUNT(*) as observations,\n",
      "    AVG(s.monthly_return) * 12 as avg_annual_return,\n",
      "    AVG(g.gdp_growth) as avg_gdp_growth\n",
      "FROM monthly_stock_returns s\n",
      "JOIN gdp_data g ON DATE_TRUNC('month', g.date) = s.month\n",
      "GROUP BY s.symbol\n",
      "HAVING COUNT(*) > 12  -- At least 1 year of data\n",
      "ORDER BY return_gdp_correlation DESC;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "def generate_sql_exercises():\n",
    "    \"\"\"\n",
    "    Generate a set of SQL exercises with solutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    exercises = [\n",
    "        {\n",
    "            'level': 'Beginner',\n",
    "            'title': 'Find Recent Prices',\n",
    "            'question': 'Get the last 10 closing prices for Apple (AAPL)',\n",
    "            'hint': 'Use WHERE for filtering and ORDER BY with LIMIT',\n",
    "            'solution': \"\"\"\n",
    "SELECT date, close\n",
    "FROM stock_prices\n",
    "WHERE symbol = 'AAPL'\n",
    "ORDER BY date DESC\n",
    "LIMIT 10;\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'level': 'Beginner',\n",
    "            'title': 'Average Volume',\n",
    "            'question': 'Calculate the average daily trading volume for each stock',\n",
    "            'hint': 'Use GROUP BY and AVG()',\n",
    "            'solution': \"\"\"\n",
    "SELECT \n",
    "    symbol,\n",
    "    AVG(volume) as avg_daily_volume,\n",
    "    MIN(volume) as min_volume,\n",
    "    MAX(volume) as max_volume\n",
    "FROM stock_prices\n",
    "GROUP BY symbol\n",
    "ORDER BY avg_daily_volume DESC;\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'level': 'Intermediate',\n",
    "            'title': 'Monthly Performance',\n",
    "            'question': 'Calculate monthly returns for each stock in 2023',\n",
    "            'hint': 'Use DATE_TRUNC() and window functions',\n",
    "            'solution': \"\"\"\n",
    "WITH monthly_prices AS (\n",
    "    SELECT \n",
    "        symbol,\n",
    "        DATE_TRUNC('month', date) as month,\n",
    "        FIRST_VALUE(open) OVER (PARTITION BY symbol, DATE_TRUNC('month', date) ORDER BY date) as month_open,\n",
    "        LAST_VALUE(close) OVER (PARTITION BY symbol, DATE_TRUNC('month', date) ORDER BY date \n",
    "                                RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as month_close\n",
    "    FROM stock_prices\n",
    "    WHERE EXTRACT(YEAR FROM date) = 2023\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    symbol,\n",
    "    month,\n",
    "    month_open,\n",
    "    month_close,\n",
    "    (month_close - month_open) / month_open * 100 as monthly_return_pct\n",
    "FROM monthly_prices\n",
    "ORDER BY symbol, month;\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'level': 'Intermediate',\n",
    "            'title': 'Volatility Ranking',\n",
    "            'question': 'Rank stocks by their 30-day volatility',\n",
    "            'hint': 'Calculate standard deviation of returns',\n",
    "            'solution': \"\"\"\n",
    "WITH daily_returns AS (\n",
    "    SELECT \n",
    "        symbol,\n",
    "        date,\n",
    "        (close - LAG(close) OVER (PARTITION BY symbol ORDER BY date)) / \n",
    "         LAG(close) OVER (PARTITION BY symbol ORDER BY date) as return\n",
    "    FROM stock_prices\n",
    "    WHERE date >= CURRENT_DATE - INTERVAL '30 days'\n",
    ")\n",
    "SELECT \n",
    "    symbol,\n",
    "    STDDEV(return) * SQRT(252) as annualized_volatility,\n",
    "    COUNT(*) as trading_days,\n",
    "    RANK() OVER (ORDER BY STDDEV(return) DESC) as volatility_rank\n",
    "FROM daily_returns\n",
    "WHERE return IS NOT NULL\n",
    "GROUP BY symbol\n",
    "HAVING COUNT(*) > 20  -- Ensure enough data points\n",
    "ORDER BY annualized_volatility DESC;\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'level': 'Advanced',\n",
    "            'title': 'Portfolio Correlation',\n",
    "            'question': 'Find which stocks move together (correlation > 0.7)',\n",
    "            'hint': 'Self-join daily returns and use CORR() function',\n",
    "            'solution': \"\"\"\n",
    "WITH returns AS (\n",
    "    SELECT \n",
    "        date,\n",
    "        symbol,\n",
    "        (close - LAG(close) OVER (PARTITION BY symbol ORDER BY date)) / \n",
    "         LAG(close) OVER (PARTITION BY symbol ORDER BY date) as return\n",
    "    FROM stock_prices\n",
    ")\n",
    "SELECT \n",
    "    r1.symbol as symbol1,\n",
    "    r2.symbol as symbol2,\n",
    "    CORR(r1.return, r2.return) as correlation,\n",
    "    COUNT(*) as observations\n",
    "FROM returns r1\n",
    "JOIN returns r2 ON r1.date = r2.date\n",
    "WHERE r1.symbol < r2.symbol  -- Avoid duplicates\n",
    "  AND r1.return IS NOT NULL\n",
    "  AND r2.return IS NOT NULL\n",
    "GROUP BY r1.symbol, r2.symbol\n",
    "HAVING CORR(r1.return, r2.return) > 0.7\n",
    "   AND COUNT(*) > 100\n",
    "ORDER BY correlation DESC;\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'level': 'Advanced',\n",
    "            'title': 'Economic Impact Analysis',\n",
    "            'question': 'Analyze how stock returns correlate with GDP growth',\n",
    "            'hint': 'JOIN stock and economic data, calculate correlations',\n",
    "            'solution': \"\"\"\n",
    "WITH monthly_stock_returns AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', date) as month,\n",
    "        symbol,\n",
    "        (MAX(close) - MIN(open)) / MIN(open) as monthly_return\n",
    "    FROM stock_prices\n",
    "    GROUP BY DATE_TRUNC('month', date), symbol\n",
    "),\n",
    "gdp_data AS (\n",
    "    SELECT \n",
    "        date,\n",
    "        value as gdp_growth\n",
    "    FROM economic_indicators\n",
    "    WHERE indicator = 'GDP_GROWTH'\n",
    ")\n",
    "SELECT \n",
    "    s.symbol,\n",
    "    CORR(s.monthly_return, g.gdp_growth) as return_gdp_correlation,\n",
    "    COUNT(*) as observations,\n",
    "    AVG(s.monthly_return) * 12 as avg_annual_return,\n",
    "    AVG(g.gdp_growth) as avg_gdp_growth\n",
    "FROM monthly_stock_returns s\n",
    "JOIN gdp_data g ON DATE_TRUNC('month', g.date) = s.month\n",
    "GROUP BY s.symbol\n",
    "HAVING COUNT(*) > 12  -- At least 1 year of data\n",
    "ORDER BY return_gdp_correlation DESC;\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create exercise notebook\n",
    "    print(\"üìù SQL Exercise Notebook\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, exercise in enumerate(exercises, 1):\n",
    "        print(f\"\\n{'üü¢' if exercise['level'] == 'Beginner' else 'üü°' if exercise['level'] == 'Intermediate' else 'üî¥'} Exercise {i}: {exercise['title']} ({exercise['level']})\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Question: {exercise['question']}\")\n",
    "        print(f\"Hint: {exercise['hint']}\")\n",
    "        print(\"\\nTry writing your query here first:\")\n",
    "        print(\"```sql\")\n",
    "        print(\"-- Your solution\\n\\n\")\n",
    "        print(\"```\")\n",
    "        \n",
    "        show_solution = input(\"\\nPress Enter to see solution (or 's' to skip): \")\n",
    "        if show_solution.lower() != 's':\n",
    "            print(\"\\nSolution:\")\n",
    "            print(\"```sql\")\n",
    "            print(exercise['solution'].strip())\n",
    "            print(\"```\")\n",
    "    \n",
    "    return exercises\n",
    "\n",
    "# Run SQL exercises\n",
    "exercises = generate_sql_exercises()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba3b6a-d2b3-4c31-917a-697165c0c9f4",
   "metadata": {},
   "source": [
    "Week 1 Complete - Integration and Next Steps\n",
    "====================================================\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "‚úÖ Set up PostgreSQL database\n",
    "‚úÖ Created proper table schemas\n",
    "‚úÖ Imported all financial datasets\n",
    "‚úÖ Created analysis views\n",
    "‚úÖ Validated data quality\n",
    "‚úÖ Practiced SQL queries\n",
    "\n",
    "For your 10-minute presentation, prepare:\n",
    "1. **Database Schema Diagram**: Show your understanding of table relationships\n",
    "2. **Interesting Query**: One SQL query that reveals a pattern\n",
    "3. **Data Quality Issue**: Any problem you found and how to fix it\n",
    "4. **Project Vision**: How this data supports your charter goals\n",
    "\n",
    "Let's wrap up and prepare for Week 2.\n",
    "\n",
    "Your data pipeline is ready! Next week we'll:\n",
    "1. Clean and validate data using pandas/polars\n",
    "2. Handle missing values and outliers\n",
    "3. Create derived features\n",
    "4. Build visualizations\n",
    "5. Generate insights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
