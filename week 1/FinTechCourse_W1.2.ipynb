{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27d0c8b-ebf8-43d9-a35e-f0e78c743f19",
   "metadata": {},
   "source": [
    "Environment Check and Prerequisites\n",
    "==========================================\n",
    "Before we start, let's understand what we're building:\n",
    "\n",
    "1. We have some mock financial data CSV files)\n",
    "2. We need to move this data into a \"real\" database (PostgreSQL)\n",
    "3. We'll learn SQL by querying our own financial data\n",
    "4. This mirrors real FinTech companies' data pipelines\n",
    "\n",
    "What you'll learn:\n",
    "- How to connect Python to PostgreSQL\n",
    "- How to import CSV files into database tables\n",
    "- How to validate data quality\n",
    "- How to write SQL queries for financial analysis\n",
    "\n",
    "Prerequisites Check:\n",
    "- PostgreSQL installed (version 14+)\n",
    "- pgAdmin installed (comes with PostgreSQL)\n",
    "- Python packages: pandas, numpy, psycopg2, sqlalchemy\n",
    "- Your generated data in 'mock_financial_data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01d8c19-dd27-47c0-b59d-5435c842572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐍 Python version: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n",
      "📊 Pandas version: 2.2.3\n",
      "🔢 NumPy version: 2.2.6\n",
      "\n",
      "✅ Found data directory: mock_financial_data\n",
      "📁 Files available: 7\n",
      "   - crypto_prices.csv: 4.46 MB\n",
      "   - customer_data.csv: 0.61 MB\n",
      "   - economic_indicators.csv: 0.02 MB\n",
      "   - portfolio_data.csv: 0.37 MB\n",
      "   - stock_prices.csv: 1.28 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Check Python version\n",
    "print(f\"🐍 Python version: {sys.version}\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")\n",
    "\n",
    "# Check if we have the synthetic data \n",
    "data_dir = 'mock_financial_data'\n",
    "if os.path.exists(data_dir):\n",
    "    print(f\"\\n✅ Found data directory: {data_dir}\")\n",
    "    files = os.listdir(data_dir)\n",
    "    print(f\"📁 Files available: {len(files)}\")\n",
    "    for file in sorted(files):\n",
    "        if file.endswith('.csv'):\n",
    "            size_mb = os.path.getsize(os.path.join(data_dir, file)) / (1024*1024)\n",
    "            print(f\"   - {file}: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n❌ Data directory not found!\")\n",
    "    print(\"Please run Week 0 data generation first\")\n",
    "    print(\"Run: generator.save_all_datasets()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97723fd5-3efb-46c0-a40b-07362903cf7b",
   "metadata": {},
   "source": [
    "Installing Database Connection Libraries\n",
    "===============================================\n",
    "We need special Python packages to talk to PostgreSQL:\n",
    "\n",
    "1. psycopg2: Low-level PostgreSQL adapter (like a translator)\n",
    "2. sqlalchemy: High-level database toolkit (makes complex operations easier)\n",
    "\n",
    "Think of it like:\n",
    "- psycopg2 = manual transmission (more control, more complex)\n",
    "- sqlalchemy = automatic transmission (easier to use, less control)\n",
    "\n",
    "If installation fails:\n",
    "- Windows: You might need Visual C++ build tools\n",
    "- Mac: You might need to install PostgreSQL first\n",
    "- Linux: You might need postgresql-dev package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33883c2f-4f41-41e1-9e04-01abc8d890f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Checking required packages...\n",
      "\n",
      "📦 Installing psycopg2-binary...\n",
      "✅ psycopg2-binary installed successfully\n",
      "✅ sqlalchemy already installed\n",
      "📦 Installing python-dotenv...\n",
      "✅ python-dotenv installed successfully\n",
      "\n",
      "✅ All required packages are ready!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Helper function to install packages\"\"\"\n",
    "    try:\n",
    "        __import__(package_name.replace('-', '_'))\n",
    "        print(f\"✅ {package_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"📦 Installing {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"✅ {package_name} installed successfully\")\n",
    "\n",
    "# Install required packages\n",
    "packages_needed = [\n",
    "    'psycopg2-binary',  # PostgreSQL adapter\n",
    "    'sqlalchemy',       # SQL toolkit\n",
    "    'python-dotenv'     # For secure password storage\n",
    "]\n",
    "\n",
    "print(\"🔧 Checking required packages...\\n\")\n",
    "for package in packages_needed:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n✅ All required packages are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc46e5-ed8b-4023-941e-37f2281432c2",
   "metadata": {},
   "source": [
    "Setting Up Database Connection\n",
    "=====================================\n",
    "\n",
    "IMPORTANT: Database connections are like phone calls:\n",
    "1. You need the right \"phone number\" (host, port, database name)\n",
    "2. You need to \"authenticate\" yourself (username, password)\n",
    "3. The connection can \"drop\" if not used properly\n",
    "4. Always \"hang up\" (close connection) when done\n",
    "\n",
    "Security Note:\n",
    "- NEVER hardcode passwords in your code\n",
    "- NEVER commit passwords to GitHub\n",
    "- Use environment variables or config files\n",
    "\n",
    "Let's create a safe connection setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a16ab92-dd2a-4a42-bc20-767385c75e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 PostgreSQL Connection Setup\n",
      "Host: localhost\n",
      "Port: 5432\n",
      "Database: fintech_db\n",
      "User: postgres\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter PostgreSQL password:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Connected to PostgreSQL!\n",
      "📊 Version: PostgreSQL 17.5 on x86_64-windows\n",
      "\n",
      "🎉 Database connection successful!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import getpass  # For secure password input\n",
    "\n",
    "# Database configuration class\n",
    "class DatabaseConfig:\n",
    "    \"\"\"\n",
    "    Stores database connection parameters.\n",
    "    In production, these would come from environment variables.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.host = 'localhost'      # Your computer\n",
    "        self.port = 5432            # Default PostgreSQL port\n",
    "        self.database = 'fintech_db' # Database name we'll create\n",
    "        self.user = 'postgres'       # Default PostgreSQL user\n",
    "        \n",
    "        # Secure password input\n",
    "        print(\"🔐 PostgreSQL Connection Setup\")\n",
    "        print(f\"Host: {self.host}\")\n",
    "        print(f\"Port: {self.port}\")\n",
    "        print(f\"Database: {self.database}\")\n",
    "        print(f\"User: {self.user}\")\n",
    "        \n",
    "        # Get password securely (won't show on screen)\n",
    "        self.password = getpass.getpass(\"Enter PostgreSQL password: \")\n",
    "        \n",
    "    def get_connection_string(self):\n",
    "        \"\"\"Create connection string for SQLAlchemy\"\"\"\n",
    "        return f\"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}\"\n",
    "    \n",
    "    def get_connection_params(self):\n",
    "        \"\"\"Get parameters for psycopg2\"\"\"\n",
    "        return {\n",
    "            'host': self.host,\n",
    "            'port': self.port,\n",
    "            'database': self.database,\n",
    "            'user': self.user,\n",
    "            'password': self.password\n",
    "        }\n",
    "\n",
    "# Create configuration\n",
    "db_config = DatabaseConfig()\n",
    "\n",
    "# Test connection\n",
    "def test_connection(config):\n",
    "    \"\"\"Test if we can connect to PostgreSQL\"\"\"\n",
    "    try:\n",
    "        # First, connect to PostgreSQL server (not specific database)\n",
    "        conn_params = config.get_connection_params()\n",
    "        conn_params['database'] = 'postgres'  # Default database\n",
    "        \n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Check PostgreSQL version\n",
    "        cur.execute(\"SELECT version();\")\n",
    "        version = cur.fetchone()[0]\n",
    "        print(f\"\\n✅ Connected to PostgreSQL!\")\n",
    "        print(f\"📊 Version: {version.split(',')[0]}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Connection failed: {e}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Is PostgreSQL running? Check pgAdmin\")\n",
    "        print(\"2. Is the password correct?\")\n",
    "        print(\"3. Is the port 5432 free?\")\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "if test_connection(db_config):\n",
    "    print(\"\\n🎉 Database connection successful!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Please fix connection issues before proceeding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df99d3-c1c6-4b55-9cd8-46e265968070",
   "metadata": {},
   "source": [
    "Creating Our FinTech Database\n",
    "====================================\n",
    "\n",
    "Databases are like filing cabinets:\n",
    "- PostgreSQL server = The entire office\n",
    "- Database = One filing cabinet  \n",
    "- Tables = Drawers in the cabinet\n",
    "- Rows = Individual files in drawers\n",
    "\n",
    "We'll create a dedicated database for our FinTech project.\n",
    "This keeps our data organized and separate from other projects.\n",
    "\n",
    "Important Concepts:\n",
    "- CREATE DATABASE: Makes a new database\n",
    "- DROP DATABASE: Deletes a database (careful!)\n",
    "- IF EXISTS / IF NOT EXISTS: Prevents errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d5d5a9-bd9d-429d-804b-b6f5306865a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Creating database 'fintech_db'...\n",
      "✅ Database 'fintech_db' created successfully!\n",
      "\n",
      "📋 All databases on this server:\n",
      "   - fintech_db: 7681 kB\n",
      "   - postgres: 11 MB\n",
      "\n",
      "✅ Database setup complete!\n"
     ]
    }
   ],
   "source": [
    "def create_fintech_database(config):\n",
    "    \"\"\"\n",
    "    Create the fintech_db database if it doesn't exist.\n",
    "    This is like creating a new filing cabinet for our project.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to PostgreSQL server (not a specific database)\n",
    "        conn_params = config.get_connection_params()\n",
    "        conn_params['database'] = 'postgres'  # Default database\n",
    "        \n",
    "        # Important: autocommit=True for CREATE DATABASE\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        conn.autocommit = True  # Required for CREATE DATABASE\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Check if database exists\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT 1 FROM pg_database WHERE datname = %s\n",
    "        \"\"\", (config.database,))\n",
    "        \n",
    "        exists = cur.fetchone()\n",
    "        \n",
    "        if exists:\n",
    "            print(f\"📊 Database '{config.database}' already exists\")\n",
    "            \n",
    "            # Get database size\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT pg_size_pretty(pg_database_size('{config.database}'))\n",
    "            \"\"\")\n",
    "            size = cur.fetchone()[0]\n",
    "            print(f\"📏 Current size: {size}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"🏗️ Creating database '{config.database}'...\")\n",
    "            cur.execute(f\"CREATE DATABASE {config.database}\")\n",
    "            print(f\"✅ Database '{config.database}' created successfully!\")\n",
    "        \n",
    "        # List all databases (for learning purposes)\n",
    "        print(\"\\n📋 All databases on this server:\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT datname, pg_size_pretty(pg_database_size(datname)) as size\n",
    "            FROM pg_database \n",
    "            WHERE datistemplate = false\n",
    "            ORDER BY datname;\n",
    "        \"\"\")\n",
    "        \n",
    "        for db_name, db_size in cur.fetchall():\n",
    "            print(f\"   - {db_name}: {db_size}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating database: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create our database\n",
    "if create_fintech_database(db_config):\n",
    "    print(\"\\n✅ Database setup complete!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Database creation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391717b4-ad60-41cb-a4e1-cccdd08a5774",
   "metadata": {},
   "source": [
    "Designing Database Tables (Schemas)\n",
    "==========================================\n",
    "\n",
    "A table schema is like a blueprint for data storage.\n",
    "Just like a form has specific fields, tables have columns with specific types.\n",
    "\n",
    "Data Types in PostgreSQL:\n",
    "- INTEGER: Whole numbers (-2B to +2B)\n",
    "- BIGINT: Very large whole numbers\n",
    "- NUMERIC(10,2): Decimal with 10 total digits, 2 after decimal\n",
    "- VARCHAR(50): Text up to 50 characters\n",
    "- TEXT: Unlimited text\n",
    "- DATE: Calendar date (no time)\n",
    "- TIMESTAMP: Date + time\n",
    "- BOOLEAN: True/False\n",
    "\n",
    "Primary Keys:\n",
    "- Unique identifier for each row\n",
    "- Like a social security number for data\n",
    "- Can be one column or combination of columns\n",
    "\n",
    "Indexes:\n",
    "- Like a book's index - helps find data faster\n",
    "- Trade-off: Faster reads, slower writes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655ffa8b-490e-4781-894b-9875b93342dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Analyzing CSV files to design table schemas...\n",
      "\n",
      "📄 Analyzing: crypto_prices.csv\n",
      "--------------------------------------------------\n",
      "Columns: 7\n",
      "Sample rows: 5\n",
      "\n",
      "Column Analysis:\n",
      "  - Timestamp            object     → TIMESTAMP       (sample: 2020-01-01 00:00:00)\n",
      "  - Symbol               object     → VARCHAR(4)      (sample: BTC)\n",
      "  - Open                 float64    → NUMERIC(20,4)   (sample: 37445.66)\n",
      "  - High                 float64    → NUMERIC(20,4)   (sample: 37641.73)\n",
      "  - Low                  float64    → NUMERIC(20,4)   (sample: 37445.66)\n",
      "  - Close                float64    → NUMERIC(20,4)   (sample: 37607.94)\n",
      "  - Volume               int64      → INTEGER         (sample: 513543)\n",
      "\n",
      "\n",
      "📄 Analyzing: customer_data.csv\n",
      "--------------------------------------------------\n",
      "Columns: 11\n",
      "Sample rows: 5\n",
      "\n",
      "Column Analysis:\n",
      "  - CustomerID           object     → VARCHAR(16)     (sample: CUST_000001)\n",
      "  - Age                  int64      → INTEGER         (sample: 66)\n",
      "  - Income               float64    → NUMERIC(20,4)   (sample: 20000.0)\n",
      "  - CreditScore          int64      → INTEGER         (sample: 621)\n",
      "  - AccountAgeDays       int64      → INTEGER         (sample: 764)\n",
      "  - AccountBalance       float64    → NUMERIC(20,4)   (sample: 1111.06)\n",
      "  - MonthlyTransactions  int64      → INTEGER         (sample: 17)\n",
      "  - AvgTransactionAmount float64    → NUMERIC(20,4)   (sample: 26.79)\n",
      "  - NumProducts          int64      → INTEGER         (sample: 2)\n",
      "  - HasLoan              bool       → BOOLEAN         (sample: False)\n",
      "  - RiskSegment          object     → VARCHAR(9)      (sample: Medium)\n",
      "\n",
      "\n",
      "📄 Analyzing: economic_indicators.csv\n",
      "--------------------------------------------------\n",
      "Columns: 3\n",
      "Sample rows: 5\n",
      "\n",
      "Column Analysis:\n",
      "  - Date                 object     → DATE            (sample: 2020-01-31)\n",
      "  - Indicator            object     → VARCHAR(28)     (sample: GDP_GROWTH)\n",
      "  - Value                float64    → NUMERIC(20,8)   (sample: 2.39)\n",
      "\n",
      "\n",
      "📄 Analyzing: portfolio_data.csv\n",
      "--------------------------------------------------\n",
      "Columns: 8\n",
      "Sample rows: 5\n",
      "\n",
      "Column Analysis:\n",
      "  - Date                 object     → DATE            (sample: 2020-01-31)\n",
      "  - PortfolioID          object     → VARCHAR(9)      (sample: PF_001)\n",
      "  - RiskLevel            object     → VARCHAR(12)     (sample: Moderate)\n",
      "  - TotalValue           float64    → NUMERIC(20,8)   (sample: 1242670.88)\n",
      "  - StockWeight          float64    → NUMERIC(7,4)    (sample: 0.569)\n",
      "  - BondWeight           float64    → NUMERIC(7,4)    (sample: 0.382)\n",
      "  - CashWeight           float64    → NUMERIC(7,4)    (sample: 0.049)\n",
      "  - MonthlyReturn        float64    → NUMERIC(7,4)    (sample: 0.0273)\n",
      "\n",
      "\n",
      "📄 Analyzing: stock_prices.csv\n",
      "--------------------------------------------------\n",
      "Columns: 7\n",
      "Sample rows: 5\n",
      "\n",
      "Column Analysis:\n",
      "  - Date                 object     → DATE            (sample: 2020-01-01)\n",
      "  - Symbol               object     → VARCHAR(6)      (sample: AAPL)\n",
      "  - Open                 float64    → NUMERIC(20,4)   (sample: 362.42)\n",
      "  - High                 float64    → NUMERIC(20,4)   (sample: 364.3)\n",
      "  - Low                  float64    → NUMERIC(20,4)   (sample: 362.42)\n",
      "  - Close                float64    → NUMERIC(20,4)   (sample: 364.09)\n",
      "  - Volume               int64      → INTEGER         (sample: 4598546)\n",
      "\n",
      "\n",
      "✅ Schema analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Let's examine what tables we need\n",
    "def analyze_csv_structure(data_dir='mock_financial_data'):\n",
    "    \"\"\"\n",
    "    Analyze our CSV files to understand what tables we need.\n",
    "    This helps us design appropriate database schemas.\n",
    "    \"\"\"\n",
    "    print(\"📊 Analyzing CSV files to design table schemas...\\n\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    table_designs = {}\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        print(f\"📄 Analyzing: {csv_file}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Read first few rows to understand structure\n",
    "        df = pd.read_csv(os.path.join(data_dir, csv_file), nrows=5)\n",
    "        \n",
    "        # Analyze columns\n",
    "        print(f\"Columns: {len(df.columns)}\")\n",
    "        print(f\"Sample rows: {len(df)}\")\n",
    "        print(\"\\nColumn Analysis:\")\n",
    "        \n",
    "        table_name = csv_file.replace('.csv', '').replace('_data', 's')\n",
    "        columns_info = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # Determine PostgreSQL data type\n",
    "            dtype = str(df[col].dtype)\n",
    "            sample_value = df[col].iloc[0] if len(df) > 0 else None\n",
    "            \n",
    "            if 'date' in col.lower() or 'time' in col.lower():\n",
    "                if 'timestamp' in col.lower():\n",
    "                    pg_type = 'TIMESTAMP'\n",
    "                else:\n",
    "                    pg_type = 'DATE'\n",
    "            elif dtype == 'object':\n",
    "                # Check if it's a string column\n",
    "                max_len = df[col].astype(str).str.len().max()\n",
    "                if max_len <= 50:\n",
    "                    pg_type = f'VARCHAR({int(max_len * 1.5)})'  # Add buffer\n",
    "                else:\n",
    "                    pg_type = 'TEXT'\n",
    "            elif 'int' in dtype:\n",
    "                if df[col].max() > 2147483647:  # Max INT value\n",
    "                    pg_type = 'BIGINT'\n",
    "                else:\n",
    "                    pg_type = 'INTEGER'\n",
    "            elif 'float' in dtype:\n",
    "                # Determine precision needed\n",
    "                if 'price' in col.lower() or 'value' in col.lower():\n",
    "                    pg_type = 'NUMERIC(20,8)'  # High precision for prices\n",
    "                elif 'weight' in col.lower() or 'return' in col.lower():\n",
    "                    pg_type = 'NUMERIC(7,4)'   # Percentages\n",
    "                else:\n",
    "                    pg_type = 'NUMERIC(20,4)'  # General decimal\n",
    "            elif 'bool' in dtype:\n",
    "                pg_type = 'BOOLEAN'\n",
    "            else:\n",
    "                pg_type = 'TEXT'  # Fallback\n",
    "            \n",
    "            columns_info.append({\n",
    "                'name': col,\n",
    "                'pandas_type': dtype,\n",
    "                'postgres_type': pg_type,\n",
    "                'sample': sample_value\n",
    "            })\n",
    "            \n",
    "            print(f\"  - {col:20} {dtype:10} → {pg_type:15} (sample: {sample_value})\")\n",
    "        \n",
    "        table_designs[table_name] = columns_info\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return table_designs\n",
    "\n",
    "# Analyze our data\n",
    "table_designs = analyze_csv_structure()\n",
    "print(\"✅ Schema analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca273c-bc25-4cf0-a507-db46b032e7e0",
   "metadata": {},
   "source": [
    "Cell 6: Creating Tables in PostgreSQL\n",
    "====================================\n",
    "\n",
    "Now we'll create the actual tables in our database.\n",
    "This is like setting up the drawers in our filing cabinet.\n",
    "\n",
    "SQL Commands we'll use:\n",
    "- CREATE TABLE: Makes a new table\n",
    "- PRIMARY KEY: Unique identifier for rows\n",
    "- NOT NULL: This column cannot be empty\n",
    "- CREATE INDEX: Speed up searches\n",
    "\n",
    "Best Practices:\n",
    "1. Always include created_at timestamp\n",
    "2. Use meaningful column names\n",
    "3. Choose appropriate data types\n",
    "4. Add indexes for frequently searched columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b1038e-4ef1-4683-b4c5-a60049fbcf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Creating database tables...\n",
      "\n",
      "📊 Creating table: stock_prices\n",
      "   Description: Daily stock price data (OHLCV format)\n",
      "   ✅ Table created successfully\n",
      "   📋 Columns: 8\n",
      "\n",
      "📊 Creating table: crypto_prices\n",
      "   Description: 6-hourly cryptocurrency price data\n",
      "   ✅ Table created successfully\n",
      "   📋 Columns: 8\n",
      "\n",
      "📊 Creating table: economic_indicators\n",
      "   Description: Monthly macroeconomic indicators\n",
      "   ✅ Table created successfully\n",
      "   📋 Columns: 5\n",
      "\n",
      "📊 Creating table: portfolio_holdings\n",
      "   Description: Monthly portfolio snapshots\n",
      "   ✅ Table created successfully\n",
      "   📋 Columns: 9\n",
      "\n",
      "📊 Creating table: customers\n",
      "   Description: Customer demographics and account information\n",
      "   ✅ Table created successfully\n",
      "   📋 Columns: 12\n",
      "\n",
      "\n",
      "📊 Database Schema Summary:\n",
      "------------------------------------------------------------\n",
      "Table: crypto_prices        Columns: 8\n",
      "Table: customers            Columns: 12\n",
      "Table: economic_indicators  Columns: 5\n",
      "Table: portfolio_holdings   Columns: 9\n",
      "Table: stock_prices         Columns: 8\n",
      "\n",
      "✅ All tables created successfully!\n",
      "🎉 Database schema is ready!\n"
     ]
    }
   ],
   "source": [
    "def create_tables(config):\n",
    "    \"\"\"\n",
    "    Create all necessary tables for our FinTech project.\n",
    "    Each table is designed for specific financial data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Table definitions with explanations\n",
    "    tables = {\n",
    "        'stock_prices': {\n",
    "            'description': 'Daily stock price data (OHLCV format)',\n",
    "            'sql': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS stock_prices (\n",
    "                    -- Primary key columns (unique identifier)\n",
    "                    date DATE NOT NULL,\n",
    "                    symbol VARCHAR(10) NOT NULL,\n",
    "                    \n",
    "                    -- Price data (OHLCV)\n",
    "                    open NUMERIC(10,2),      -- Opening price\n",
    "                    high NUMERIC(10,2),      -- Highest price of day\n",
    "                    low NUMERIC(10,2),       -- Lowest price of day\n",
    "                    close NUMERIC(10,2),     -- Closing price\n",
    "                    volume BIGINT,           -- Number of shares traded\n",
    "                    \n",
    "                    -- Metadata\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    \n",
    "                    -- Composite primary key (date + symbol must be unique)\n",
    "                    PRIMARY KEY (date, symbol)\n",
    "                );\n",
    "                \n",
    "                -- Indexes for faster queries\n",
    "                CREATE INDEX IF NOT EXISTS idx_stock_symbol ON stock_prices(symbol);\n",
    "                CREATE INDEX IF NOT EXISTS idx_stock_date ON stock_prices(date);\n",
    "                \n",
    "                -- Add comments for documentation\n",
    "                COMMENT ON TABLE stock_prices IS 'Daily stock price data in OHLCV format';\n",
    "                COMMENT ON COLUMN stock_prices.symbol IS 'Stock ticker symbol (e.g., AAPL, GOOGL)';\n",
    "                COMMENT ON COLUMN stock_prices.volume IS 'Number of shares traded during the day';\n",
    "            \"\"\"\n",
    "        },\n",
    "        \n",
    "        'crypto_prices': {\n",
    "            'description': '6-hourly cryptocurrency price data',\n",
    "            'sql': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS crypto_prices (\n",
    "                    -- Crypto trades 24/7, so we use timestamp\n",
    "                    timestamp TIMESTAMP NOT NULL,\n",
    "                    symbol VARCHAR(10) NOT NULL,\n",
    "                    \n",
    "                    -- Price data (higher precision for crypto)\n",
    "                    open NUMERIC(20,8),      -- 8 decimals for small altcoins\n",
    "                    high NUMERIC(20,8),\n",
    "                    low NUMERIC(20,8),\n",
    "                    close NUMERIC(20,8),\n",
    "                    volume BIGINT,\n",
    "                    \n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    PRIMARY KEY (timestamp, symbol)\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_crypto_symbol ON crypto_prices(symbol);\n",
    "                CREATE INDEX IF NOT EXISTS idx_crypto_timestamp ON crypto_prices(timestamp);\n",
    "                \n",
    "                COMMENT ON TABLE crypto_prices IS '6-hourly cryptocurrency price data';\n",
    "                COMMENT ON COLUMN crypto_prices.timestamp IS 'UTC timestamp of the price snapshot';\n",
    "            \"\"\"\n",
    "        },\n",
    "        \n",
    "        'economic_indicators': {\n",
    "            'description': 'Monthly macroeconomic indicators',\n",
    "            'sql': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS economic_indicators (\n",
    "                    date DATE NOT NULL,\n",
    "                    indicator VARCHAR(50) NOT NULL,\n",
    "                    value NUMERIC(20,4),\n",
    "                    unit VARCHAR(20),        -- %, USD, points, etc.\n",
    "                    \n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    PRIMARY KEY (date, indicator)\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_econ_indicator ON economic_indicators(indicator);\n",
    "                CREATE INDEX IF NOT EXISTS idx_econ_date ON economic_indicators(date);\n",
    "                \n",
    "                COMMENT ON TABLE economic_indicators IS 'Monthly macroeconomic indicators';\n",
    "                COMMENT ON COLUMN economic_indicators.indicator IS 'Indicator name (e.g., GDP_GROWTH, INFLATION_RATE)';\n",
    "                COMMENT ON COLUMN economic_indicators.unit IS 'Unit of measurement';\n",
    "            \"\"\"\n",
    "        },\n",
    "        \n",
    "        'portfolio_holdings': {\n",
    "            'description': 'Monthly portfolio snapshots',\n",
    "            'sql': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS portfolio_holdings (\n",
    "                    date DATE NOT NULL,\n",
    "                    portfolio_id VARCHAR(20) NOT NULL,\n",
    "                    \n",
    "                    -- Portfolio characteristics\n",
    "                    risk_level VARCHAR(20),   -- Conservative/Moderate/Aggressive\n",
    "                    total_value NUMERIC(20,2),\n",
    "                    \n",
    "                    -- Asset allocation (must sum to 1.0)\n",
    "                    stock_weight NUMERIC(5,4),  -- 0.0000 to 1.0000\n",
    "                    bond_weight NUMERIC(5,4),\n",
    "                    cash_weight NUMERIC(5,4),\n",
    "                    \n",
    "                    -- Performance\n",
    "                    monthly_return NUMERIC(7,4),  -- -0.9999 to 9.9999\n",
    "                    \n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    PRIMARY KEY (date, portfolio_id)\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_portfolio_id ON portfolio_holdings(portfolio_id);\n",
    "                CREATE INDEX IF NOT EXISTS idx_portfolio_risk ON portfolio_holdings(risk_level);\n",
    "                \n",
    "                COMMENT ON TABLE portfolio_holdings IS 'Monthly portfolio performance and allocation data';\n",
    "            \"\"\"\n",
    "        },\n",
    "        \n",
    "        'customers': {\n",
    "            'description': 'Customer demographics and account information',\n",
    "            'sql': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS customers (\n",
    "                    -- Customer identity\n",
    "                    customer_id VARCHAR(20) PRIMARY KEY,\n",
    "                    \n",
    "                    -- Demographics\n",
    "                    age INTEGER CHECK (age >= 18 AND age <= 120),\n",
    "                    income NUMERIC(12,2) CHECK (income >= 0),\n",
    "                    credit_score INTEGER CHECK (credit_score >= 300 AND credit_score <= 850),\n",
    "                    \n",
    "                    -- Account information\n",
    "                    account_age_days INTEGER CHECK (account_age_days >= 0),\n",
    "                    account_balance NUMERIC(20,2),\n",
    "                    \n",
    "                    -- Behavior metrics\n",
    "                    monthly_transactions INTEGER CHECK (monthly_transactions >= 0),\n",
    "                    avg_transaction_amount NUMERIC(12,2),\n",
    "                    num_products INTEGER CHECK (num_products >= 0),\n",
    "                    has_loan BOOLEAN,\n",
    "                    \n",
    "                    -- Risk classification\n",
    "                    risk_segment VARCHAR(20),\n",
    "                    \n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_customer_risk ON customers(risk_segment);\n",
    "                CREATE INDEX IF NOT EXISTS idx_customer_credit ON customers(credit_score);\n",
    "                \n",
    "                COMMENT ON TABLE customers IS 'Customer profiles for analytics and risk assessment';\n",
    "                COMMENT ON COLUMN customers.credit_score IS 'FICO credit score (300-850 range)';\n",
    "            \"\"\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Connect to our fintech database\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"🏗️ Creating database tables...\\n\")\n",
    "        \n",
    "        for table_name, table_info in tables.items():\n",
    "            print(f\"📊 Creating table: {table_name}\")\n",
    "            print(f\"   Description: {table_info['description']}\")\n",
    "            \n",
    "            # Execute the CREATE TABLE statement\n",
    "            cur.execute(table_info['sql'])\n",
    "            \n",
    "            # Check if table was created successfully\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT COUNT(*) \n",
    "                FROM information_schema.tables \n",
    "                WHERE table_schema = 'public' \n",
    "                AND table_name = %s\n",
    "            \"\"\", (table_name,))\n",
    "            \n",
    "            if cur.fetchone()[0] > 0:\n",
    "                print(f\"   ✅ Table created successfully\")\n",
    "                \n",
    "                # Get column count\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT COUNT(*) \n",
    "                    FROM information_schema.columns \n",
    "                    WHERE table_name = %s\n",
    "                \"\"\", (table_name,))\n",
    "                col_count = cur.fetchone()[0]\n",
    "                print(f\"   📋 Columns: {col_count}\")\n",
    "            else:\n",
    "                print(f\"   ❌ Table creation failed\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        # Commit all changes\n",
    "        conn.commit()\n",
    "        \n",
    "        # Show summary of created tables\n",
    "        print(\"\\n📊 Database Schema Summary:\")\n",
    "        print(\"-\" * 60)\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                table_name,\n",
    "                COUNT(*) as column_count\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema = 'public'\n",
    "            GROUP BY table_name\n",
    "            ORDER BY table_name;\n",
    "        \"\"\")\n",
    "        \n",
    "        for table, col_count in cur.fetchall():\n",
    "            print(f\"Table: {table:20} Columns: {col_count}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(\"\\n✅ All tables created successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating tables: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        return False\n",
    "\n",
    "# Create all tables\n",
    "if create_tables(db_config):\n",
    "    print(\"🎉 Database schema is ready!\")\n",
    "else:\n",
    "    print(\"⚠️ Please fix table creation errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5610bff-efa7-4459-8810-c90e33456b9f",
   "metadata": {},
   "source": [
    "Understanding the Data Import Process\n",
    "============================================\n",
    "\n",
    "Before we import data, let's understand what we're doing:\n",
    "\n",
    "CSV Files → PostgreSQL Tables\n",
    "\n",
    "Challenges:\n",
    "1. Data types might not match\n",
    "2. Dates need special formatting\n",
    "3. NULL values need handling\n",
    "4. Large files need batch processing\n",
    "\n",
    "Methods for importing data:\n",
    "1. COPY command (fastest, but less flexible)\n",
    "2. INSERT statements (slow, but most control)\n",
    "3. pandas.to_sql (balanced approach)\n",
    "\n",
    "We'll use pandas.to_sql because:\n",
    "- It handles data type conversion\n",
    "- It can append to existing tables\n",
    "- It provides progress feedback\n",
    "- It's easier to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae88414-86b7-4e62-ae96-01d628cff7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Previewing all CSV files before import...\n",
      "\n",
      "📄 Previewing: stock_prices.csv\n",
      "============================================================\n",
      "Total rows: 26,100\n",
      "Columns: ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "\n",
      "First 5 rows:\n",
      "         Date Symbol    Open    High     Low   Close    Volume\n",
      "0  2020-01-01   AAPL  362.42  364.30  362.42  364.09   4598546\n",
      "1  2020-01-02   AAPL  365.91  365.91  357.94  357.98    367620\n",
      "2  2020-01-03   AAPL  356.91  356.91  352.20  353.07  29745876\n",
      "3  2020-01-06   AAPL  353.57  353.57  345.71  349.30  10824733\n",
      "4  2020-01-07   AAPL  349.35  355.84  349.35  355.05   1553500\n",
      "\n",
      "Data Quality Checks:\n",
      "✅ No NULL values in sample\n",
      "\n",
      "Data types:\n",
      "  Date: object\n",
      "  Symbol: object\n",
      "  Open: float64\n",
      "  High: float64\n",
      "  Low: float64\n",
      "  Close: float64\n",
      "  Volume: int64\n",
      "\n",
      "Estimated memory needed: 4.44 MB\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next file... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Previewing: crypto_prices.csv\n",
      "============================================================\n",
      "Total rows: 73,050\n",
      "Columns: ['Timestamp', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "\n",
      "First 5 rows:\n",
      "             Timestamp Symbol      Open      High       Low     Close  Volume\n",
      "0  2020-01-01 00:00:00    BTC  37445.66  37641.73  37445.66  37607.94  513543\n",
      "1  2020-01-01 06:00:00    BTC  38265.57  38918.27  37993.31  38353.88   14045\n",
      "2  2020-01-01 12:00:00    BTC  39178.44  39178.44  36829.42  38377.20   20102\n",
      "3  2020-01-01 18:00:00    BTC  38204.61  38471.59  38198.89  38264.98  282156\n",
      "4  2020-01-02 00:00:00    BTC  39139.10  39139.10  35926.36  37411.67   76198\n",
      "\n",
      "Data Quality Checks:\n",
      "✅ No NULL values in sample\n",
      "\n",
      "Data types:\n",
      "  Timestamp: object\n",
      "  Symbol: object\n",
      "  Open: float64\n",
      "  High: float64\n",
      "  Low: float64\n",
      "  Close: float64\n",
      "  Volume: int64\n",
      "\n",
      "Estimated memory needed: 12.99 MB\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next file... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Previewing: economic_indicators.csv\n",
      "============================================================\n",
      "Total rows: 600\n",
      "Columns: ['Date', 'Indicator', 'Value']\n",
      "\n",
      "First 5 rows:\n",
      "         Date            Indicator  Value\n",
      "0  2020-01-31           GDP_GROWTH   2.39\n",
      "1  2020-01-31       INFLATION_RATE   1.95\n",
      "2  2020-01-31    UNEMPLOYMENT_RATE   5.10\n",
      "3  2020-01-31        INTEREST_RATE   1.31\n",
      "4  2020-01-31  CONSUMER_CONFIDENCE  94.59\n",
      "\n",
      "Data Quality Checks:\n",
      "✅ No NULL values in sample\n",
      "\n",
      "Data types:\n",
      "  Date: object\n",
      "  Indicator: object\n",
      "  Value: float64\n",
      "\n",
      "Estimated memory needed: 0.09 MB\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next file... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Previewing: portfolio_data.csv\n",
      "============================================================\n",
      "Total rows: 6,000\n",
      "Columns: ['Date', 'PortfolioID', 'RiskLevel', 'TotalValue', 'StockWeight', 'BondWeight', 'CashWeight', 'MonthlyReturn']\n",
      "\n",
      "First 5 rows:\n",
      "         Date PortfolioID RiskLevel  TotalValue  StockWeight  BondWeight  \\\n",
      "0  2020-01-31      PF_001  Moderate  1242670.88        0.569       0.382   \n",
      "1  2020-02-29      PF_001  Moderate  1297316.35        0.567       0.383   \n",
      "2  2020-03-31      PF_001  Moderate  1212010.39        0.562       0.388   \n",
      "3  2020-04-30      PF_001  Moderate  1225080.79        0.566       0.383   \n",
      "4  2020-05-31      PF_001  Moderate  1191190.49        0.578       0.372   \n",
      "\n",
      "   CashWeight  MonthlyReturn  \n",
      "0       0.049         0.0273  \n",
      "1       0.050         0.0440  \n",
      "2       0.050        -0.0658  \n",
      "3       0.050         0.0108  \n",
      "4       0.050        -0.0277  \n",
      "\n",
      "Data Quality Checks:\n",
      "✅ No NULL values in sample\n",
      "\n",
      "Data types:\n",
      "  Date: object\n",
      "  PortfolioID: object\n",
      "  RiskLevel: object\n",
      "  TotalValue: float64\n",
      "  StockWeight: float64\n",
      "  BondWeight: float64\n",
      "  CashWeight: float64\n",
      "  MonthlyReturn: float64\n",
      "\n",
      "Estimated memory needed: 1.36 MB\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next file... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Previewing: customer_data.csv\n",
      "============================================================\n",
      "Total rows: 10,000\n",
      "Columns: ['CustomerID', 'Age', 'Income', 'CreditScore', 'AccountAgeDays', 'AccountBalance', 'MonthlyTransactions', 'AvgTransactionAmount', 'NumProducts', 'HasLoan', 'RiskSegment']\n",
      "\n",
      "First 5 rows:\n",
      "    CustomerID  Age    Income  CreditScore  AccountAgeDays  AccountBalance  \\\n",
      "0  CUST_000001   66  20000.00          621             764         1111.06   \n",
      "1  CUST_000002   41  36815.04          590            1789       110834.08   \n",
      "2  CUST_000003   26  52834.47          748            1853          466.99   \n",
      "3  CUST_000004   62  56594.88          693            1176          190.74   \n",
      "4  CUST_000005   23  52210.34          578            1238         5254.55   \n",
      "\n",
      "   MonthlyTransactions  AvgTransactionAmount  NumProducts  HasLoan RiskSegment  \n",
      "0                   17                 26.79            2    False      Medium  \n",
      "1                   54                 72.19            1    False        High  \n",
      "2                   61                124.66            1    False      Medium  \n",
      "3                   45                 31.28            1    False      Medium  \n",
      "4                   44                 39.26            1    False        High  \n",
      "\n",
      "Data Quality Checks:\n",
      "✅ No NULL values in sample\n",
      "\n",
      "Data types:\n",
      "  CustomerID: object\n",
      "  Age: int64\n",
      "  Income: float64\n",
      "  CreditScore: int64\n",
      "  AccountAgeDays: int64\n",
      "  AccountBalance: float64\n",
      "  MonthlyTransactions: int64\n",
      "  AvgTransactionAmount: float64\n",
      "  NumProducts: int64\n",
      "  HasLoan: bool\n",
      "  RiskSegment: object\n",
      "\n",
      "Estimated memory needed: 1.96 MB\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next file... \n"
     ]
    }
   ],
   "source": [
    "def preview_csv_data(csv_file, data_dir='mock_financial_data', rows=5):\n",
    "    \"\"\"\n",
    "    Preview CSV data before import to spot potential issues.\n",
    "    This is like checking your ingredients before cooking.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"📄 Previewing: {csv_file}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Read first few rows\n",
    "    df = pd.read_csv(filepath, nrows=rows)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_rows = sum(1 for line in open(filepath)) - 1  # Subtract header\n",
    "    \n",
    "    print(f\"Total rows: {total_rows:,}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst {rows} rows:\")\n",
    "    print(df)\n",
    "    \n",
    "    # Check for potential issues\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_counts = df.isnull().sum()\n",
    "    if null_counts.any():\n",
    "        print(\"⚠️ Found NULL values:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "    else:\n",
    "        print(\"✅ No NULL values in sample\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\nData types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    estimated_total = memory_usage * (total_rows / rows)\n",
    "    print(f\"\\nEstimated memory needed: {estimated_total:.2f} MB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preview each CSV file\n",
    "csv_files = [\n",
    "    'stock_prices.csv',\n",
    "    'crypto_prices.csv', \n",
    "    'economic_indicators.csv',\n",
    "    'portfolio_data.csv',\n",
    "    'customer_data.csv'\n",
    "]\n",
    "\n",
    "print(\"🔍 Previewing all CSV files before import...\\n\")\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if os.path.exists(os.path.join('mock_financial_data', csv_file)):\n",
    "        preview_df = preview_csv_data(csv_file)\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "        # Wait for user to review\n",
    "        input(\"Press Enter to continue to next file...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06faa3eb-6424-4b24-884c-51ba2d2274d4",
   "metadata": {},
   "source": [
    "Importing Stock Price Data\n",
    "==================================\n",
    "\n",
    "Let's start with stock prices - our most important dataset.\n",
    "We'll import it step by step with proper error handling.\n",
    "\n",
    "Key considerations:\n",
    "1. Date formatting (pandas → PostgreSQL)\n",
    "2. Handling duplicates\n",
    "3. Transaction management\n",
    "4. Progress tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4689c41-a6e1-413c-99cf-38ecd3b55681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Importing Stock Price Data\n",
      "============================================================\n",
      "Step 1: Loading CSV file...\n",
      "✅ Loaded 26,100 rows\n",
      "\n",
      "Step 2: Preprocessing data...\n",
      "\n",
      "Step 3: Validating data...\n",
      "✅ All OHLC values are valid\n",
      "✅ No negative prices found\n",
      "\n",
      "Step 4: Importing to PostgreSQL...\n",
      "  Chunk 1/6: 19.2% complete\n",
      "  Chunk 2/6: 38.3% complete\n",
      "  Chunk 3/6: 57.5% complete\n",
      "  Chunk 4/6: 76.6% complete\n",
      "  Chunk 5/6: 95.8% complete\n",
      "  Chunk 6/6: 100.0% complete\n",
      "\n",
      "✅ Successfully imported 26,100 stock price records\n",
      "\n",
      "Step 5: Verifying import...\n",
      "Database summary:\n",
      "  Total rows: 26,100\n",
      "  Date range: 2020-01-01 to 2024-12-31\n",
      "  Unique symbols: 20\n",
      "\n",
      "Sample data from database:\n",
      "  (datetime.date(2024, 12, 31), 'AAPL', Decimal('421.30'), Decimal('425.83'), Decimal('421.30'), Decimal('425.73'), 1346229)\n",
      "  (datetime.date(2024, 12, 31), 'AMZN', Decimal('354.89'), Decimal('360.83'), Decimal('354.89'), Decimal('360.01'), 16759835)\n",
      "  (datetime.date(2024, 12, 31), 'BAC', Decimal('361.71'), Decimal('367.41'), Decimal('358.00'), Decimal('365.12'), 3052714)\n",
      "  (datetime.date(2024, 12, 31), 'DIS', Decimal('255.72'), Decimal('260.38'), Decimal('255.72'), Decimal('259.94'), 2703626)\n",
      "  (datetime.date(2024, 12, 31), 'GOOGL', Decimal('418.11'), Decimal('418.11'), Decimal('416.88'), Decimal('417.05'), 3577902)\n",
      "\n",
      "🎉 Stock price import complete!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "def import_stock_prices(config, data_dir='mock_financial_data'):\n",
    "    \"\"\"\n",
    "    Import stock price data with detailed logging.\n",
    "    This is our most critical dataset.\n",
    "    \"\"\"\n",
    "    csv_file = 'stock_prices.csv'\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"📈 Importing Stock Price Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load the CSV file\n",
    "        print(\"Step 1: Loading CSV file...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"✅ Loaded {len(df):,} rows\")\n",
    "        \n",
    "        # Step 2: Data preprocessing\n",
    "        print(\"\\nStep 2: Preprocessing data...\")\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        df['date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.drop('Date', axis=1)  # Remove original column\n",
    "        \n",
    "        # Ensure column names match database\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df.duplicated(subset=['date', 'symbol'])\n",
    "        if duplicates.any():\n",
    "            print(f\"⚠️ Found {duplicates.sum()} duplicate rows\")\n",
    "            df = df[~duplicates]\n",
    "            print(f\"✅ Removed duplicates, {len(df):,} rows remaining\")\n",
    "        \n",
    "        # Data validation\n",
    "        print(\"\\nStep 3: Validating data...\")\n",
    "        \n",
    "        # Check OHLC logic (High >= Low, etc.)\n",
    "        invalid_ohlc = df[\n",
    "            (df['high'] < df['low']) |\n",
    "            (df['high'] < df['open']) |\n",
    "            (df['high'] < df['close']) |\n",
    "            (df['low'] > df['open']) |\n",
    "            (df['low'] > df['close'])\n",
    "        ]\n",
    "        \n",
    "        if len(invalid_ohlc) > 0:\n",
    "            print(f\"⚠️ Found {len(invalid_ohlc)} rows with invalid OHLC\")\n",
    "            print(\"First few invalid rows:\")\n",
    "            print(invalid_ohlc.head())\n",
    "        else:\n",
    "            print(\"✅ All OHLC values are valid\")\n",
    "        \n",
    "        # Check for negative prices\n",
    "        negative_prices = df[(df[['open', 'high', 'low', 'close']] < 0).any(axis=1)]\n",
    "        if len(negative_prices) > 0:\n",
    "            print(f\"⚠️ Found {len(negative_prices)} rows with negative prices\")\n",
    "        else:\n",
    "            print(\"✅ No negative prices found\")\n",
    "        \n",
    "        # Step 4: Import to database\n",
    "        print(\"\\nStep 4: Importing to PostgreSQL...\")\n",
    "        \n",
    "        # Create SQLAlchemy engine\n",
    "        engine = create_engine(config.get_connection_string())\n",
    "        \n",
    "        # Import in chunks for better performance\n",
    "        chunk_size = 5000\n",
    "        total_chunks = len(df) // chunk_size + 1\n",
    "        \n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            chunk.to_sql(\n",
    "                'stock_prices',\n",
    "                engine,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi'\n",
    "            )\n",
    "            \n",
    "            # Progress update\n",
    "            current_chunk = i // chunk_size + 1\n",
    "            progress = (i + len(chunk)) / len(df) * 100\n",
    "            print(f\"  Chunk {current_chunk}/{total_chunks}: {progress:.1f}% complete\")\n",
    "        \n",
    "        print(f\"\\n✅ Successfully imported {len(df):,} stock price records\")\n",
    "        \n",
    "        # Step 5: Verify import\n",
    "        print(\"\\nStep 5: Verifying import...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Count imported rows\n",
    "        cur.execute(\"SELECT COUNT(*) FROM stock_prices\")\n",
    "        db_count = cur.fetchone()[0]\n",
    "        \n",
    "        # Get date range\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT MIN(date), MAX(date), COUNT(DISTINCT symbol)\n",
    "            FROM stock_prices\n",
    "        \"\"\")\n",
    "        min_date, max_date, symbol_count = cur.fetchone()\n",
    "        \n",
    "        print(f\"Database summary:\")\n",
    "        print(f\"  Total rows: {db_count:,}\")\n",
    "        print(f\"  Date range: {min_date} to {max_date}\")\n",
    "        print(f\"  Unique symbols: {symbol_count}\")\n",
    "        \n",
    "        # Sample data\n",
    "        print(\"\\nSample data from database:\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT date, symbol, open, high, low, close, volume\n",
    "            FROM stock_prices\n",
    "            ORDER BY date DESC, symbol\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            print(f\"  {row}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Import failed: {e}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Check if table exists\")\n",
    "        print(\"2. Check column names match\")\n",
    "        print(\"3. Check date format\")\n",
    "        print(\"4. Check for disk space\")\n",
    "        return False\n",
    "\n",
    "# Import stock prices\n",
    "if import_stock_prices(db_config):\n",
    "    print(\"\\n🎉 Stock price import complete!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Please check stock import errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccaa824-28ae-47be-8103-49e723f79d3d",
   "metadata": {},
   "source": [
    "Importing Cryptocurrency Price Data\n",
    "==========================================\n",
    "\n",
    "Cryptocurrency data has special characteristics:\n",
    "1. Timestamp instead of date (24/7 trading)\n",
    "2. Higher price precision (8 decimal places)\n",
    "3. More volatile (larger price swings)\n",
    "4. Different symbols (BTC, ETH vs AAPL, GOOGL)\n",
    "\n",
    "Let's handle these differences properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf1c934-43f4-49c6-9558-630bab255117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💎 Importing Cryptocurrency Price Data\n",
      "============================================================\n",
      "Step 1: Loading crypto data...\n",
      "✅ Loaded 73,050 rows\n",
      "\n",
      "Sample data:\n",
      "             Timestamp Symbol      Open      High       Low     Close  Volume\n",
      "0  2020-01-01 00:00:00    BTC  37445.66  37641.73  37445.66  37607.94  513543\n",
      "1  2020-01-01 06:00:00    BTC  38265.57  38918.27  37993.31  38353.88   14045\n",
      "2  2020-01-01 12:00:00    BTC  39178.44  39178.44  36829.42  38377.20   20102\n",
      "\n",
      "Step 2: Preprocessing crypto data...\n",
      "Most common time interval: 0 days 06:00:00\n",
      "\n",
      "Step 3: Crypto-specific validation...\n",
      "\n",
      "Checking price precision needs:\n",
      "  BTC: avg price $25457.43, max decimals: 2\n",
      "  ETH: avg price $1339.72, max decimals: 2\n",
      "  BNB: avg price $479.32, max decimals: 2\n",
      "  XRP: avg price $1.10, max decimals: 6\n",
      "  ADA: avg price $0.41, max decimals: 6\n",
      "\n",
      "Step 4: Importing to PostgreSQL...\n",
      "  Progress: [██████████████████████████████] 100.0%\n",
      "✅ Imported 73,050 crypto price records\n",
      "\n",
      "Step 5: Verification and analysis...\n",
      "\n",
      "Crypto Summary by Symbol:\n",
      "Symbol  Points           First Data            Last Data    Avg Price   Volatility\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   BTC   7,305  2020-01-01 00:00:00  2024-12-31 00:00:00 $  25,457.43 $   6,884.51\n",
      "   ETH   7,305  2020-01-01 00:00:00  2024-12-31 00:00:00 $   1,339.72 $   1,262.73\n",
      "   BNB   7,305  2020-01-01 00:00:00  2024-12-31 00:00:00 $     479.32 $     275.99\n",
      "   SOL   7,305  2020-01-01 00:00:00  2024-12-31 00:00:00 $      67.39 $      25.94\n",
      "   DOT   7,305  2020-01-01 00:00:00  2024-12-31 00:00:00 $      28.48 $      15.26\n",
      "   XRP   7,305  2020-01-01 00:00:00  2024-12-31 00:00:00 $       1.10 $       0.75\n",
      " MATIC   7,305  2020-01-01 00:00:00  2024-12-31 00:00:00 $       0.79 $       1.24\n",
      "   ADA   7,305  2020-01-01 00:00:00  2024-12-31 00:00:00 $       0.41 $       0.30\n",
      "  DOGE   7,305  2020-01-01 00:00:00  2024-12-31 00:00:00 $       0.15 $       0.08\n",
      "   TRX   7,305  2020-01-01 00:00:00  2024-12-31 00:00:00 $       0.14 $       0.07\n",
      "\n",
      "🎉 Crypto price import complete!\n"
     ]
    }
   ],
   "source": [
    "def import_crypto_prices(config, data_dir='mock_financial_data'):\n",
    "    \"\"\"\n",
    "    Import cryptocurrency price data with special handling\n",
    "    for timestamps and high precision values.\n",
    "    \"\"\"\n",
    "    csv_file = 'crypto_prices.csv'\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"💎 Importing Cryptocurrency Price Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load CSV\n",
    "        print(\"Step 1: Loading crypto data...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"✅ Loaded {len(df):,} rows\")\n",
    "        \n",
    "        # Show sample to understand structure\n",
    "        print(\"\\nSample data:\")\n",
    "        print(df.head(3))\n",
    "        \n",
    "        # Step 2: Preprocessing\n",
    "        print(\"\\nStep 2: Preprocessing crypto data...\")\n",
    "        \n",
    "        # Convert timestamp and ensure lowercase columns\n",
    "        df['timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df = df.drop('Timestamp', axis=1)\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Check timestamp frequency (should be 6-hourly)\n",
    "        time_diffs = df.groupby('symbol')['timestamp'].diff()\n",
    "        most_common_diff = time_diffs.mode()[0]\n",
    "        print(f\"Most common time interval: {most_common_diff}\")\n",
    "        \n",
    "        # Crypto-specific validation\n",
    "        print(\"\\nStep 3: Crypto-specific validation...\")\n",
    "        \n",
    "        # Check for extreme volatility (>50% in 6 hours)\n",
    "        df['price_change_pct'] = df.groupby('symbol')['close'].pct_change()\n",
    "        extreme_moves = df[df['price_change_pct'].abs() > 0.5]\n",
    "        \n",
    "        if len(extreme_moves) > 0:\n",
    "            print(f\"⚠️ Found {len(extreme_moves)} extreme price moves (>50%)\")\n",
    "            print(\"Largest moves:\")\n",
    "            print(extreme_moves.nlargest(5, 'price_change_pct')[['timestamp', 'symbol', 'price_change_pct']])\n",
    "        \n",
    "        # Remove temporary column\n",
    "        df = df.drop('price_change_pct', axis=1)\n",
    "        \n",
    "        # Check price precision\n",
    "        print(\"\\nChecking price precision needs:\")\n",
    "        for symbol in df['symbol'].unique()[:5]:  # Check first 5 symbols\n",
    "            symbol_data = df[df['symbol'] == symbol]\n",
    "            max_decimals = symbol_data['close'].apply(lambda x: len(str(x).split('.')[-1])).max()\n",
    "            avg_price = symbol_data['close'].mean()\n",
    "            print(f\"  {symbol}: avg price ${avg_price:.2f}, max decimals: {max_decimals}\")\n",
    "        \n",
    "        # Step 4: Import to database\n",
    "        print(\"\\nStep 4: Importing to PostgreSQL...\")\n",
    "        \n",
    "        engine = create_engine(config.get_connection_string())\n",
    "        \n",
    "        # Import in chunks (crypto data is larger)\n",
    "        chunk_size = 10000\n",
    "        total_rows = len(df)\n",
    "        \n",
    "        for i in range(0, total_rows, chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            \n",
    "            # Progress bar\n",
    "            progress = min((i + chunk_size) / total_rows * 100, 100)\n",
    "            bar_length = 30\n",
    "            filled = int(bar_length * progress / 100)\n",
    "            bar = '█' * filled + '░' * (bar_length - filled)\n",
    "            print(f\"\\r  Progress: [{bar}] {progress:.1f}%\", end='')\n",
    "            \n",
    "            chunk.to_sql(\n",
    "                'crypto_prices',\n",
    "                engine,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi'\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n✅ Imported {len(df):,} crypto price records\")\n",
    "        \n",
    "        # Step 5: Verify and analyze\n",
    "        print(\"\\nStep 5: Verification and analysis...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Get summary statistics\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                symbol,\n",
    "                COUNT(*) as data_points,\n",
    "                MIN(timestamp) as first_data,\n",
    "                MAX(timestamp) as last_data,\n",
    "                AVG(close) as avg_price,\n",
    "                STDDEV(close) as price_volatility\n",
    "            FROM crypto_prices\n",
    "            GROUP BY symbol\n",
    "            ORDER BY avg_price DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nCrypto Summary by Symbol:\")\n",
    "        print(f\"{'Symbol':>6} {'Points':>7} {'First Data':>20} {'Last Data':>20} {'Avg Price':>12} {'Volatility':>12}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            symbol, points, first, last, avg_price, volatility = row\n",
    "            print(f\"{symbol:>6} {points:>7,} {str(first):>20} {str(last):>20} ${avg_price:>11,.2f} ${volatility:>11,.2f}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Crypto import failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Import crypto data\n",
    "if import_crypto_prices(db_config):\n",
    "    print(\"\\n🎉 Crypto price import complete!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Please check crypto import errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ed44a-2202-448e-bd40-1560414a4d86",
   "metadata": {},
   "source": [
    " Importing Economic Indicators\n",
    "=====================================\n",
    "\n",
    "Economic indicators are different from price data:\n",
    "1. Monthly frequency (not daily)\n",
    "2. Different units (%, billions, index points)\n",
    "3. Some can be negative (trade balance)\n",
    "4. Wide value ranges (0.5% to millions)\n",
    "\n",
    "Understanding the indicators:\n",
    "- GDP_GROWTH: Quarterly GDP growth rate (%)\n",
    "- INFLATION_RATE: Monthly CPI change (%)\n",
    "- UNEMPLOYMENT_RATE: Monthly unemployment (%)\n",
    "- INTEREST_RATE: Federal funds rate (%)\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0928592f-b40a-4522-99f8-542f459d2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏛️ Importing Economic Indicators\n",
      "============================================================\n",
      "Step 1: Loading economic data...\n",
      "✅ Loaded 600 rows\n",
      "\n",
      "Unique indicators:\n",
      "  GDP_GROWTH: 60 observations, range: 1.46 to 3.59\n",
      "  INFLATION_RATE: 60 observations, range: 1.29 to 4.00\n",
      "  UNEMPLOYMENT_RATE: 60 observations, range: 3.11 to 6.04\n",
      "  INTEREST_RATE: 60 observations, range: 0.29 to 1.76\n",
      "  CONSUMER_CONFIDENCE: 60 observations, range: 85.65 to 119.37\n",
      "  RETAIL_SALES: 60 observations, range: -0.71 to 3.66\n",
      "  INDUSTRIAL_PRODUCTION: 60 observations, range: -0.72 to 3.03\n",
      "  HOUSING_STARTS: 60 observations, range: 1024707.47 to 1381179.57\n",
      "  TRADE_BALANCE: 60 observations, range: -111225.35 to -27215.41\n",
      "  MONEY_SUPPLY: 60 observations, range: 15399.62 to 21076.51\n",
      "\n",
      "Step 2: Preprocessing...\n",
      "\n",
      "Step 3: Economic validation...\n",
      "\n",
      "Step 4: Importing to PostgreSQL...\n",
      "✅ Imported 600 economic indicator records\n",
      "\n",
      "Step 5: Creating economic summary view...\n",
      "✅ Created pivot view for easier analysis\n",
      "\n",
      "Recent Economic Snapshot:\n",
      "date         | gdp_growth   | inflation_ra | unemployment | interest_rat | consumer_con\n",
      "------------------------------------------------------------------------------\n",
      "2024-12-31   |         1.84 |         2.69 |         4.61 |         1.16 |       112.72\n",
      "2024-11-30   |         1.93 |         2.55 |         4.88 |         1.14 |       102.61\n",
      "2024-10-31   |         1.80 |         2.63 |         5.11 |         1.54 |       106.57\n",
      "2024-09-30   |         2.12 |         2.19 |         5.35 |         1.41 |       109.12\n",
      "2024-08-31   |         2.18 |         1.86 |         5.39 |         1.09 |       102.13\n",
      "\n",
      "🎉 Economic indicators import complete!\n"
     ]
    }
   ],
   "source": [
    "def import_economic_indicators(config, data_dir='mock_financial_data'):\n",
    "    \"\"\"\n",
    "    Import economic indicators with proper unit handling.\n",
    "    \"\"\"\n",
    "    csv_file = 'economic_indicators.csv'\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"🏛️ Importing Economic Indicators\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"Step 1: Loading economic data...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"✅ Loaded {len(df):,} rows\")\n",
    "        \n",
    "        # Understand the structure\n",
    "        print(\"\\nUnique indicators:\")\n",
    "        indicators = df['Indicator'].unique()\n",
    "        for ind in indicators:\n",
    "            ind_data = df[df['Indicator'] == ind]\n",
    "            print(f\"  {ind}: {len(ind_data)} observations, \"\n",
    "                  f\"range: {ind_data['Value'].min():.2f} to {ind_data['Value'].max():.2f}\")\n",
    "        \n",
    "        # Preprocessing\n",
    "        print(\"\\nStep 2: Preprocessing...\")\n",
    "        df['date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.drop('Date', axis=1)\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Add unit information based on indicator type\n",
    "        unit_mapping = {\n",
    "            'GDP_GROWTH': '%',\n",
    "            'INFLATION_RATE': '%',\n",
    "            'UNEMPLOYMENT_RATE': '%',\n",
    "            'INTEREST_RATE': '%',\n",
    "            'CONSUMER_CONFIDENCE': 'index',\n",
    "            'RETAIL_SALES': '%',\n",
    "            'INDUSTRIAL_PRODUCTION': '%',\n",
    "            'HOUSING_STARTS': 'thousands',\n",
    "            'TRADE_BALANCE': 'millions USD',\n",
    "            'MONEY_SUPPLY': 'billions USD'\n",
    "        }\n",
    "        \n",
    "        df['unit'] = df['indicator'].map(unit_mapping)\n",
    "        \n",
    "        # Validate economic logic\n",
    "        print(\"\\nStep 3: Economic validation...\")\n",
    "        \n",
    "        # Check for unrealistic values\n",
    "        validations = {\n",
    "            'GDP_GROWTH': (-10, 20),          # Realistic GDP growth range\n",
    "            'INFLATION_RATE': (-5, 30),       # Deflation to hyperinflation\n",
    "            'UNEMPLOYMENT_RATE': (0, 30),     # 0% to depression levels\n",
    "            'INTEREST_RATE': (-2, 20),        # Negative to very high rates\n",
    "            'CONSUMER_CONFIDENCE': (0, 200),  # Index range\n",
    "        }\n",
    "        \n",
    "        for indicator, (min_val, max_val) in validations.items():\n",
    "            ind_data = df[df['indicator'] == indicator]\n",
    "            out_of_range = ind_data[(ind_data['value'] < min_val) | (ind_data['value'] > max_val)]\n",
    "            if len(out_of_range) > 0:\n",
    "                print(f\"⚠️ {indicator}: {len(out_of_range)} values outside expected range [{min_val}, {max_val}]\")\n",
    "        \n",
    "        # Import to database\n",
    "        print(\"\\nStep 4: Importing to PostgreSQL...\")\n",
    "        engine = create_engine(config.get_connection_string())\n",
    "        \n",
    "        df.to_sql(\n",
    "            'economic_indicators',\n",
    "            engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            method='multi'\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Imported {len(df):,} economic indicator records\")\n",
    "        \n",
    "        # Verify and create a summary view\n",
    "        print(\"\\nStep 5: Creating economic summary view...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Create a pivot view for easier analysis\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW economic_indicators_pivot AS\n",
    "            SELECT \n",
    "                date,\n",
    "                MAX(CASE WHEN indicator = 'GDP_GROWTH' THEN value END) as gdp_growth,\n",
    "                MAX(CASE WHEN indicator = 'INFLATION_RATE' THEN value END) as inflation_rate,\n",
    "                MAX(CASE WHEN indicator = 'UNEMPLOYMENT_RATE' THEN value END) as unemployment_rate,\n",
    "                MAX(CASE WHEN indicator = 'INTEREST_RATE' THEN value END) as interest_rate,\n",
    "                MAX(CASE WHEN indicator = 'CONSUMER_CONFIDENCE' THEN value END) as consumer_confidence\n",
    "            FROM economic_indicators\n",
    "            WHERE indicator IN ('GDP_GROWTH', 'INFLATION_RATE', 'UNEMPLOYMENT_RATE', \n",
    "                               'INTEREST_RATE', 'CONSUMER_CONFIDENCE')\n",
    "            GROUP BY date\n",
    "            ORDER BY date DESC;\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"✅ Created pivot view for easier analysis\")\n",
    "        \n",
    "        # Show recent economic snapshot\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT * FROM economic_indicators_pivot\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nRecent Economic Snapshot:\")\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        print(f\"{' | '.join(col[:12].ljust(12) for col in columns)}\")\n",
    "        print(\"-\" * (13 * len(columns)))\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            formatted_row = []\n",
    "            for i, val in enumerate(row):\n",
    "                if i == 0:  # Date\n",
    "                    formatted_row.append(str(val)[:12].ljust(12))\n",
    "                else:  # Numeric values\n",
    "                    formatted_row.append(f\"{val:>12.2f}\" if val else \" \" * 12)\n",
    "            print(\" | \".join(formatted_row))\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Economic indicators import failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Import economic data\n",
    "if import_economic_indicators(db_config):\n",
    "    print(\"\\n🎉 Economic indicators import complete!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Please check economic import errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061baf1f-5cfe-4df4-9e65-c2a095cb1204",
   "metadata": {},
   "source": [
    "Importing Portfolio Holdings Data\n",
    "=========================================\n",
    "\n",
    "Portfolio data represents:\n",
    "- Investment accounts with different risk profiles\n",
    "- Monthly snapshots of holdings\n",
    "- Asset allocation (stocks, bonds, cash)\n",
    "- Performance metrics\n",
    "\n",
    "Key validations:\n",
    "- Weights must sum to 1.0 (100%)\n",
    "- Returns should be realistic\n",
    "- Total value should be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99aa0cea-f25b-408d-8908-77ba2c7e6621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💼 Importing Portfolio Holdings Data\n",
      "============================================================\n",
      "Step 1: Loading portfolio data...\n",
      "✅ Loaded 6,000 rows\n",
      "\n",
      "Step 2: Preprocessing...\n",
      "\n",
      "Step 3: Portfolio validation...\n",
      "⚠️ Found 826 rows where weights don't sum to 1.0\n",
      "Sample errors:\n",
      "   portfolio_id       date  weight_sum\n",
      "3        PF_001 2020-04-30       0.999\n",
      "9        PF_001 2020-10-31       1.001\n",
      "15       PF_001 2021-04-30       1.001\n",
      "26       PF_001 2022-03-31       0.999\n",
      "38       PF_001 2023-03-31       0.999\n",
      "Fixing weight allocations...\n",
      "\n",
      "Performance Analysis by Risk Level:\n",
      "             monthly_return                           portfolio_id\n",
      "                       mean       std     min     max      nunique\n",
      "risk_level                                                        \n",
      "Aggressive         0.009203  0.057741 -0.1745  0.1898           21\n",
      "Conservative       0.004643  0.026346 -0.0817  0.0942           35\n",
      "Moderate           0.006048  0.038861 -0.1171  0.1410           44\n",
      "\n",
      "Step 4: Importing to PostgreSQL...\n",
      "✅ Imported 6,000 portfolio records\n",
      "\n",
      "Step 5: Creating portfolio analysis views...\n",
      "✅ Created portfolio performance summary view\n",
      "\n",
      "Portfolio Performance Summary:\n",
      "risk_level      | portfolio_count | avg_annual_retu | avg_annual_vola | avg_sharpe_rati | avg_total_growt\n",
      "------------------------------------------------------------------------------------------------\n",
      "Conservative    |              35 |          0.0557 |          0.0907 |          0.6143 |          0.4839\n",
      "Moderate        |              44 |          0.0726 |          0.1341 |          0.5414 |          0.6972\n",
      "Aggressive      |              21 |          0.1104 |          0.1993 |          0.5540 |          1.3366\n",
      "\n",
      "🎉 Portfolio data import complete!\n"
     ]
    }
   ],
   "source": [
    "def import_portfolio_data(config, data_dir='mock_financial_data'):\n",
    "    \"\"\"\n",
    "    Import portfolio holdings with allocation validation.\n",
    "    \"\"\"\n",
    "    csv_file = 'portfolio_data.csv'\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"💼 Importing Portfolio Holdings Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"Step 1: Loading portfolio data...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"✅ Loaded {len(df):,} rows\")\n",
    "        \n",
    "        # Basic preprocessing\n",
    "        print(\"\\nStep 2: Preprocessing...\")\n",
    "        df['date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.drop('Date', axis=1)\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Rename columns to match database schema\n",
    "        column_mapping = {\n",
    "            'portfolioid': 'portfolio_id',\n",
    "            'risklevel': 'risk_level',\n",
    "            'totalvalue': 'total_value',\n",
    "            'stockweight': 'stock_weight',\n",
    "            'bondweight': 'bond_weight',\n",
    "            'cashweight': 'cash_weight',\n",
    "            'monthlyreturn': 'monthly_return'\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Portfolio-specific validations\n",
    "        print(\"\\nStep 3: Portfolio validation...\")\n",
    "        \n",
    "        # Check if weights sum to 1.0\n",
    "        df['weight_sum'] = df['stock_weight'] + df['bond_weight'] + df['cash_weight']\n",
    "        weight_errors = df[abs(df['weight_sum'] - 1.0) > 0.001]  # Allow small rounding errors\n",
    "        \n",
    "        if len(weight_errors) > 0:\n",
    "            print(f\"⚠️ Found {len(weight_errors)} rows where weights don't sum to 1.0\")\n",
    "            print(\"Sample errors:\")\n",
    "            print(weight_errors[['portfolio_id', 'date', 'weight_sum']].head())\n",
    "            \n",
    "            # Fix by normalizing\n",
    "            print(\"Fixing weight allocations...\")\n",
    "            for idx in weight_errors.index:\n",
    "                total = df.loc[idx, 'weight_sum']\n",
    "                if total > 0:\n",
    "                    df.loc[idx, 'stock_weight'] /= total\n",
    "                    df.loc[idx, 'bond_weight'] /= total\n",
    "                    df.loc[idx, 'cash_weight'] /= total\n",
    "        \n",
    "        df = df.drop('weight_sum', axis=1)\n",
    "        \n",
    "        # Analyze performance by risk level\n",
    "        print(\"\\nPerformance Analysis by Risk Level:\")\n",
    "        risk_stats = df.groupby('risk_level').agg({\n",
    "            'monthly_return': ['mean', 'std', 'min', 'max'],\n",
    "            'portfolio_id': 'nunique'\n",
    "        })\n",
    "        \n",
    "        print(risk_stats)\n",
    "        \n",
    "        # Check for unrealistic returns (>100% or <-50% monthly)\n",
    "        extreme_returns = df[(df['monthly_return'] > 1.0) | (df['monthly_return'] < -0.5)]\n",
    "        if len(extreme_returns) > 0:\n",
    "            print(f\"\\n⚠️ Found {len(extreme_returns)} extreme monthly returns\")\n",
    "            \n",
    "        # Import to database\n",
    "        print(\"\\nStep 4: Importing to PostgreSQL...\")\n",
    "        engine = create_engine(config.get_connection_string())\n",
    "        \n",
    "        df.to_sql(\n",
    "            'portfolio_holdings',\n",
    "            engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Imported {len(df):,} portfolio records\")\n",
    "        \n",
    "        # Create useful analysis view\n",
    "        print(\"\\nStep 5: Creating portfolio analysis views...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Portfolio performance summary view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW portfolio_performance_summary AS\n",
    "            WITH returns_calc AS (\n",
    "                SELECT \n",
    "                    portfolio_id,\n",
    "                    risk_level,\n",
    "                    AVG(monthly_return) * 12 as annual_return,\n",
    "                    STDDEV(monthly_return) * SQRT(12) as annual_volatility,\n",
    "                    MIN(total_value) as min_value,\n",
    "                    MAX(total_value) as max_value,\n",
    "                    MAX(total_value) / MIN(total_value) - 1 as total_growth\n",
    "                FROM portfolio_holdings\n",
    "                GROUP BY portfolio_id, risk_level\n",
    "            )\n",
    "            SELECT \n",
    "                risk_level,\n",
    "                COUNT(*) as portfolio_count,\n",
    "                AVG(annual_return) as avg_annual_return,\n",
    "                AVG(annual_volatility) as avg_annual_volatility,\n",
    "                AVG(annual_return) / AVG(annual_volatility) as avg_sharpe_ratio,\n",
    "                AVG(total_growth) as avg_total_growth\n",
    "            FROM returns_calc\n",
    "            GROUP BY risk_level\n",
    "            ORDER BY \n",
    "                CASE risk_level \n",
    "                    WHEN 'Conservative' THEN 1 \n",
    "                    WHEN 'Moderate' THEN 2 \n",
    "                    WHEN 'Aggressive' THEN 3 \n",
    "                END;\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"✅ Created portfolio performance summary view\")\n",
    "        \n",
    "        # Show the summary\n",
    "        cur.execute(\"SELECT * FROM portfolio_performance_summary\")\n",
    "        \n",
    "        print(\"\\nPortfolio Performance Summary:\")\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        print(f\"{' | '.join(col[:15].ljust(15) for col in columns)}\")\n",
    "        print(\"-\" * (16 * len(columns)))\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            formatted_row = []\n",
    "            for i, val in enumerate(row):\n",
    "                if i == 0:  # Risk level\n",
    "                    formatted_row.append(str(val)[:15].ljust(15))\n",
    "                elif i == 1:  # Count\n",
    "                    formatted_row.append(f\"{val:>15d}\")\n",
    "                else:  # Numeric values\n",
    "                    formatted_row.append(f\"{val:>15.4f}\")\n",
    "            print(\" | \".join(formatted_row))\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Portfolio import failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Import portfolio data\n",
    "if import_portfolio_data(db_config):\n",
    "    print(\"\\n🎉 Portfolio data import complete!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Please check portfolio import errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834be39-a539-482d-a0de-dd2cd3642e92",
   "metadata": {},
   "source": [
    "Importing Customer Demographics Data\n",
    "============================================\n",
    "\n",
    "Customer data is sensitive and requires:\n",
    "1. Privacy considerations (even for mock data)\n",
    "2. Data type validation (age, credit scores)\n",
    "3. Business rule validation\n",
    "4. Risk segmentation accuracy\n",
    "\n",
    "This data supports:\n",
    "- Credit scoring models\n",
    "- Customer segmentation\n",
    "- Risk assessment\n",
    "- Marketing analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b923a4e-243e-4ae8-b5d3-195161651dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👥 Importing Customer Demographics Data\n",
      "============================================================\n",
      "Step 1: Loading customer data...\n",
      "✅ Loaded 10,000 customer records\n",
      "\n",
      "Step 2: Preprocessing...\n",
      "\n",
      "Step 3: Customer data validation...\n",
      "\n",
      "Customer Segmentation Analysis:\n",
      "             count  avg_age  avg_income  avg_credit  avg_balance  loan_rate\n",
      "risksegment                                                                \n",
      "High          1754    39.88    38930.80      546.63      6749.45       0.00\n",
      "Low           2855    39.70    44284.21      804.70      9378.98       0.05\n",
      "Medium        5391    39.85    41118.21      676.82      7221.16       0.02\n",
      "\n",
      "Data Quality Checks:\n",
      "✅ No duplicate customer IDs\n",
      "Income-Credit Score correlation: 0.090\n",
      "\n",
      "Step 4: Importing to PostgreSQL...\n",
      "  Imported chunk 1/6\n",
      "  Imported chunk 2/6\n",
      "  Imported chunk 3/6\n",
      "  Imported chunk 4/6\n",
      "  Imported chunk 5/6\n",
      "✅ Imported 10,000 customer records\n",
      "\n",
      "Step 5: Creating customer analysis views...\n",
      "✅ Created customer analysis views\n",
      "\n",
      "Customer Segment Summary:\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "    Risk     |  Count   |  Age   |   Income   | Credit  |  Balance   | Trans/Mo  | Loan % \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "    High     |  1,754   |  39.9  | $ 38,931   |   547   | $  6,749   |   40.5    |  0.4  %\n",
      "    Low      |  2,855   |  39.7  | $ 44,284   |   805   | $  9,379   |   41.3    |  4.6  %\n",
      "   Medium    |  5,391   |  39.9  | $ 41,118   |   677   | $  7,221   |   40.4    |  2.1  %\n",
      "\n",
      "🎉 Customer data import complete!\n"
     ]
    }
   ],
   "source": [
    "def import_customer_data(config, data_dir='mock_financial_data'):\n",
    "    \"\"\"\n",
    "    Import customer data with privacy and validation checks.\n",
    "    \"\"\"\n",
    "    csv_file = 'customer_data.csv'\n",
    "    filepath = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    print(f\"👥 Importing Customer Demographics Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"Step 1: Loading customer data...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"✅ Loaded {len(df):,} customer records\")\n",
    "        \n",
    "        # Preprocessing\n",
    "        print(\"\\nStep 2: Preprocessing...\")\n",
    "        \n",
    "        # Ensure column names match database\n",
    "        df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
    "        \n",
    "        # Convert boolean column\n",
    "        df['has_loan'] = df['hasloan']\n",
    "        df = df.drop('hasloan', axis=1)\n",
    "        \n",
    "        # Customer data validation\n",
    "        print(\"\\nStep 3: Customer data validation...\")\n",
    "        \n",
    "        # Age validation (18-120)\n",
    "        invalid_age = df[(df['age'] < 18) | (df['age'] > 120)]\n",
    "        if len(invalid_age) > 0:\n",
    "            print(f\"⚠️ Found {len(invalid_age)} customers with invalid age\")\n",
    "        \n",
    "        # Credit score validation (300-850)\n",
    "        invalid_credit = df[(df['creditscore'] < 300) | (df['creditscore'] > 850)]\n",
    "        if len(invalid_credit) > 0:\n",
    "            print(f\"⚠️ Found {len(invalid_credit)} customers with invalid credit scores\")\n",
    "        \n",
    "        # Income validation (positive)\n",
    "        negative_income = df[df['income'] < 0]\n",
    "        if len(negative_income) > 0:\n",
    "            print(f\"⚠️ Found {len(negative_income)} customers with negative income\")\n",
    "        \n",
    "        # Analyze customer segments\n",
    "        print(\"\\nCustomer Segmentation Analysis:\")\n",
    "        segment_stats = df.groupby('risksegment').agg({\n",
    "            'customerid': 'count',\n",
    "            'age': 'mean',\n",
    "            'income': 'mean',\n",
    "            'creditscore': 'mean',\n",
    "            'accountbalance': 'mean',\n",
    "            'has_loan': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        segment_stats.columns = ['count', 'avg_age', 'avg_income', 'avg_credit', 'avg_balance', 'loan_rate']\n",
    "        print(segment_stats)\n",
    "        \n",
    "        # Check for suspicious patterns\n",
    "        print(\"\\nData Quality Checks:\")\n",
    "        \n",
    "        # Check for duplicate customer IDs\n",
    "        duplicate_ids = df[df.duplicated('customerid', keep=False)]\n",
    "        if len(duplicate_ids) > 0:\n",
    "            print(f\"⚠️ Found {len(duplicate_ids)} duplicate customer IDs\")\n",
    "        else:\n",
    "            print(\"✅ No duplicate customer IDs\")\n",
    "        \n",
    "        # Check correlation between income and credit score\n",
    "        income_credit_corr = df['income'].corr(df['creditscore'])\n",
    "        print(f\"Income-Credit Score correlation: {income_credit_corr:.3f}\")\n",
    "        \n",
    "        # Fix column names for database\n",
    "        column_mapping = {\n",
    "            'customerid': 'customer_id',\n",
    "            'creditscore': 'credit_score',\n",
    "            'accountagedays': 'account_age_days',\n",
    "            'accountbalance': 'account_balance',\n",
    "            'monthlytransactions': 'monthly_transactions',\n",
    "            'avgtransactionamount': 'avg_transaction_amount',\n",
    "            'numproducts': 'num_products',\n",
    "            'risksegment': 'risk_segment'\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Import to database\n",
    "        print(\"\\nStep 4: Importing to PostgreSQL...\")\n",
    "        engine = create_engine(config.get_connection_string())\n",
    "        \n",
    "        # Import in chunks due to size\n",
    "        chunk_size = 2000\n",
    "        total_chunks = len(df) // chunk_size + 1\n",
    "        \n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            chunk.to_sql(\n",
    "                'customers',\n",
    "                engine,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi'\n",
    "            )\n",
    "            \n",
    "            current_chunk = i // chunk_size + 1\n",
    "            print(f\"  Imported chunk {current_chunk}/{total_chunks}\")\n",
    "        \n",
    "        print(f\"✅ Imported {len(df):,} customer records\")\n",
    "        \n",
    "        # Create analysis views\n",
    "        print(\"\\nStep 5: Creating customer analysis views...\")\n",
    "        \n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Customer segmentation view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW customer_segments AS\n",
    "            SELECT \n",
    "                risk_segment,\n",
    "                COUNT(*) as customer_count,\n",
    "                AVG(age) as avg_age,\n",
    "                AVG(income) as avg_income,\n",
    "                AVG(credit_score) as avg_credit_score,\n",
    "                AVG(account_balance) as avg_balance,\n",
    "                AVG(monthly_transactions) as avg_monthly_transactions,\n",
    "                SUM(CASE WHEN has_loan THEN 1 ELSE 0 END)::FLOAT / COUNT(*) as loan_penetration,\n",
    "                PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY income) as income_q1,\n",
    "                PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY income) as income_median,\n",
    "                PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY income) as income_q3\n",
    "            FROM customers\n",
    "            GROUP BY risk_segment;\n",
    "        \"\"\")\n",
    "        \n",
    "        # High-value customer view\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW high_value_customers AS\n",
    "            SELECT \n",
    "                customer_id,\n",
    "                age,\n",
    "                income,\n",
    "                credit_score,\n",
    "                account_balance,\n",
    "                num_products,\n",
    "                risk_segment\n",
    "            FROM customers\n",
    "            WHERE account_balance > (SELECT PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY account_balance) FROM customers)\n",
    "               OR income > (SELECT PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY income) FROM customers)\n",
    "            ORDER BY account_balance DESC;\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"✅ Created customer analysis views\")\n",
    "        \n",
    "        # Show segment summary\n",
    "        cur.execute(\"SELECT * FROM customer_segments ORDER BY risk_segment\")\n",
    "        \n",
    "        print(\"\\nCustomer Segment Summary:\")\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        \n",
    "        # Print header\n",
    "        print(\"\\n\" + \"-\" * 120)\n",
    "        print(f\"{'Risk':^12} | {'Count':^8} | {'Age':^6} | {'Income':^10} | {'Credit':^7} | {'Balance':^10} | {'Trans/Mo':^9} | {'Loan %':^7}\")\n",
    "        print(\"-\" * 120)\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            risk, count, age, income, credit, balance, trans, loan_pct = row[:8]\n",
    "            print(f\"{risk:^12} | {count:^8,} | {age:^6.1f} | ${income:^9,.0f} | {credit:^7.0f} | ${balance:^9,.0f} | {trans:^9.1f} | {loan_pct*100:^6.1f}%\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Customer import failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Import customer data\n",
    "if import_customer_data(db_config):\n",
    "    print(\"\\n🎉 Customer data import complete!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Please check customer import errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0efb1e-6120-4684-b453-3ed7a2006bdd",
   "metadata": {},
   "source": [
    "Comprehensive Data Validation\n",
    "=====================================\n",
    "\n",
    "Now that all data is imported, let's run comprehensive\n",
    "validation queries to ensure data integrity across tables.\n",
    "\n",
    "This includes:\n",
    "1. Referential integrity checks\n",
    "2. Data completeness analysis\n",
    "3. Anomaly detection\n",
    "4. Cross-table consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "722058a6-62d2-49ba-a13b-915b1c5cee55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Comprehensive Data Validation\n",
      "============================================================\n",
      "\n",
      "1. Table Row Counts:\n",
      "----------------------------------------\n",
      "  stock_prices        :     26,100 rows\n",
      "  crypto_prices       :     73,050 rows\n",
      "  economic_indicators :        600 rows\n",
      "  portfolio_holdings  :      6,000 rows\n",
      "  customers           :     10,000 rows\n",
      "\n",
      "2. Date Range Analysis:\n",
      "----------------------------------------\n",
      "  Stock prices: 2020-01-01 to 2024-12-31 (1305 days)\n",
      "  Economic data: 2020-01-31 to 2024-12-31 (60 months)\n",
      "\n",
      "3. Missing Data Check:\n",
      "----------------------------------------\n",
      "  Missing trading days: 0\n",
      "\n",
      "4. Data Integrity Checks:\n",
      "----------------------------------------\n",
      "  Invalid OHLC relationships: 0\n",
      "  Invalid portfolio weights: 0\n",
      "\n",
      "5. Statistical Anomaly Detection:\n",
      "----------------------------------------\n",
      "  Extreme daily returns (>20%): 0\n",
      "  Maximum daily return: 0.0%\n",
      "\n",
      "6. Cross-Table Consistency:\n",
      "----------------------------------------\n",
      "  Stock/Economic date overlap: Full overlap\n",
      "\n",
      "7. Data Quality Score:\n",
      "----------------------------------------\n",
      "  Total checks: 8\n",
      "  Passed: 8\n",
      "  Failed: 0\n",
      "  Quality Score: 100.0%\n",
      "\n",
      "📄 Detailed report saved to: data_quality_report.md\n"
     ]
    }
   ],
   "source": [
    "def run_data_validation(config):\n",
    "    \"\"\"\n",
    "    Run comprehensive validation queries across all tables.\n",
    "    Generate a data quality report.\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Running Comprehensive Data Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        validation_results = []\n",
    "        \n",
    "        # 1. Table Row Counts\n",
    "        print(\"\\n1. Table Row Counts:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        tables = ['stock_prices', 'crypto_prices', 'economic_indicators', \n",
    "                  'portfolio_holdings', 'customers']\n",
    "        \n",
    "        for table in tables:\n",
    "            cur.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "            count = cur.fetchone()[0]\n",
    "            print(f\"  {table:20}: {count:>10,} rows\")\n",
    "            validation_results.append({\n",
    "                'check': f'{table} row count',\n",
    "                'result': count,\n",
    "                'status': 'PASS' if count > 0 else 'FAIL'\n",
    "            })\n",
    "        \n",
    "        # 2. Date Range Consistency\n",
    "        print(\"\\n2. Date Range Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Stock prices date range\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT MIN(date) as min_date, MAX(date) as max_date,\n",
    "                   COUNT(DISTINCT date) as trading_days\n",
    "            FROM stock_prices\n",
    "        \"\"\")\n",
    "        stock_dates = cur.fetchone()\n",
    "        print(f\"  Stock prices: {stock_dates[0]} to {stock_dates[1]} ({stock_dates[2]} days)\")\n",
    "        \n",
    "        # Economic indicators date range\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT MIN(date) as min_date, MAX(date) as max_date,\n",
    "                   COUNT(DISTINCT date) as months\n",
    "            FROM economic_indicators\n",
    "        \"\"\")\n",
    "        econ_dates = cur.fetchone()\n",
    "        print(f\"  Economic data: {econ_dates[0]} to {econ_dates[1]} ({econ_dates[2]} months)\")\n",
    "        \n",
    "        # 3. Missing Data Analysis\n",
    "        print(\"\\n3. Missing Data Check:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check for missing trading days (excluding weekends)\n",
    "        cur.execute(\"\"\"\n",
    "            WITH expected_days AS (\n",
    "                SELECT generate_series(\n",
    "                    (SELECT MIN(date) FROM stock_prices),\n",
    "                    (SELECT MAX(date) FROM stock_prices),\n",
    "                    '1 day'::interval\n",
    "                )::date AS trading_date\n",
    "            ),\n",
    "            actual_days AS (\n",
    "                SELECT DISTINCT date FROM stock_prices\n",
    "            )\n",
    "            SELECT COUNT(*) as missing_days\n",
    "            FROM expected_days e\n",
    "            LEFT JOIN actual_days a ON e.trading_date = a.date\n",
    "            WHERE a.date IS NULL\n",
    "              AND EXTRACT(DOW FROM e.trading_date) NOT IN (0, 6)\n",
    "        \"\"\")\n",
    "        \n",
    "        missing_days = cur.fetchone()[0]\n",
    "        print(f\"  Missing trading days: {missing_days}\")\n",
    "        validation_results.append({\n",
    "            'check': 'Missing trading days',\n",
    "            'result': missing_days,\n",
    "            'status': 'PASS' if missing_days < 10 else 'WARNING'\n",
    "        })\n",
    "        \n",
    "        # 4. Data Integrity Checks\n",
    "        print(\"\\n4. Data Integrity Checks:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check OHLC consistency\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT COUNT(*) as invalid_ohlc\n",
    "            FROM stock_prices\n",
    "            WHERE high < low \n",
    "               OR high < open \n",
    "               OR high < close\n",
    "               OR low > open \n",
    "               OR low > close\n",
    "        \"\"\")\n",
    "        invalid_ohlc = cur.fetchone()[0]\n",
    "        print(f\"  Invalid OHLC relationships: {invalid_ohlc}\")\n",
    "        validation_results.append({\n",
    "            'check': 'OHLC consistency',\n",
    "            'result': invalid_ohlc,\n",
    "            'status': 'PASS' if invalid_ohlc == 0 else 'FAIL'\n",
    "        })\n",
    "        \n",
    "        # Check portfolio weight sums\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT COUNT(*) as invalid_weights\n",
    "            FROM portfolio_holdings\n",
    "            WHERE ABS((stock_weight + bond_weight + cash_weight) - 1.0) > 0.01\n",
    "        \"\"\")\n",
    "        invalid_weights = cur.fetchone()[0]\n",
    "        print(f\"  Invalid portfolio weights: {invalid_weights}\")\n",
    "        validation_results.append({\n",
    "            'check': 'Portfolio weight sums',\n",
    "            'result': invalid_weights,\n",
    "            'status': 'PASS' if invalid_weights == 0 else 'FAIL'\n",
    "        })\n",
    "        \n",
    "        # 5. Statistical Anomaly Detection\n",
    "        print(\"\\n5. Statistical Anomaly Detection:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check for extreme returns\n",
    "        cur.execute(\"\"\"\n",
    "            WITH daily_returns AS (\n",
    "                SELECT \n",
    "                    symbol,\n",
    "                    date,\n",
    "                    (close / LAG(close) OVER (PARTITION BY symbol ORDER BY date) - 1) as return\n",
    "                FROM stock_prices\n",
    "            )\n",
    "            SELECT \n",
    "                COUNT(*) as extreme_returns,\n",
    "                MAX(ABS(return)) as max_return\n",
    "            FROM daily_returns\n",
    "            WHERE ABS(return) > 0.20  -- 20% daily move\n",
    "        \"\"\")\n",
    "        result = cur.fetchone()\n",
    "        extreme_returns, max_return = result[0], result[1] if result[1] is not None else 0\n",
    "        print(f\"  Extreme daily returns (>20%): {extreme_returns}\")\n",
    "        print(f\"  Maximum daily return: {max_return*100:.1f}%\")\n",
    "        \n",
    "        # 6. Cross-Table Consistency\n",
    "        print(\"\\n6. Cross-Table Consistency:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check date overlap between tables\n",
    "        cur.execute(\"\"\"\n",
    "            WITH stock_dates AS (\n",
    "                SELECT MIN(date) as min_date, MAX(date) as max_date FROM stock_prices\n",
    "            ),\n",
    "            econ_dates AS (\n",
    "                SELECT MIN(date) as min_date, MAX(date) as max_date FROM economic_indicators\n",
    "            )\n",
    "            SELECT \n",
    "                CASE \n",
    "                    WHEN s.min_date <= e.min_date AND s.max_date >= e.max_date THEN 'Full overlap'\n",
    "                    WHEN s.max_date < e.min_date OR s.min_date > e.max_date THEN 'No overlap'\n",
    "                    ELSE 'Partial overlap'\n",
    "                END as date_overlap\n",
    "            FROM stock_dates s, econ_dates e\n",
    "        \"\"\")\n",
    "        date_overlap = cur.fetchone()[0]\n",
    "        print(f\"  Stock/Economic date overlap: {date_overlap}\")\n",
    "        \n",
    "        # 7. Generate Quality Score\n",
    "        print(\"\\n7. Data Quality Score:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        total_checks = len(validation_results)\n",
    "        passed_checks = sum(1 for r in validation_results if r['status'] == 'PASS')\n",
    "        quality_score = (passed_checks / total_checks) * 100 if total_checks > 0 else 0\n",
    "        \n",
    "        print(f\"  Total checks: {total_checks}\")\n",
    "        print(f\"  Passed: {passed_checks}\")\n",
    "        print(f\"  Failed: {total_checks - passed_checks}\")\n",
    "        print(f\"  Quality Score: {quality_score:.1f}%\")\n",
    "        \n",
    "        # Generate detailed report\n",
    "        report_content = f\"\"\"\n",
    "# Data Quality Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Summary\n",
    "- Quality Score: {quality_score:.1f}%\n",
    "- Total Checks: {total_checks}\n",
    "- Passed: {passed_checks}\n",
    "- Failed: {total_checks - passed_checks}\n",
    "\n",
    "## Detailed Results\n",
    "\"\"\"\n",
    "        \n",
    "        for result in validation_results:\n",
    "            status_emoji = \"PASS\" if result['status'] == 'PASS' else \"FAIL\"\n",
    "            report_content += f\"- {result['check']}: {result['result']} [{status_emoji}]\\n\"\n",
    "        \n",
    "        # Save report with UTF-8 encoding\n",
    "        with open('data_quality_report.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(report_content)\n",
    "        \n",
    "        print(f\"\\n📄 Detailed report saved to: data_quality_report.md\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return validation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Validation error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run validation\n",
    "validation_results = run_data_validation(db_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff45bf4-2556-43fc-8d98-17a3d675a41b",
   "metadata": {},
   "source": [
    "Creating Analysis Views and Performance Indexes\n",
    "=======================================================\n",
    "\n",
    "Views are like \"saved queries\" that simplify complex analysis.\n",
    "Indexes make queries faster by creating lookup tables.\n",
    "\n",
    "We'll create views for:\n",
    "1. Daily returns calculation\n",
    "2. Moving averages\n",
    "3. Volatility metrics\n",
    "4. Correlation matrices\n",
    "5. Portfolio performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86df0aa6-2523-4005-ac9a-5eb84f26d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting infrastructure setup...\n",
      "\n",
      "🏗️ Creating Complete Analysis Infrastructure\n",
      "============================================================\n",
      "\n",
      "🧹 PART 0: Cleaning Up Existing Objects\n",
      "----------------------------------------\n",
      "⚠️ Cleanup note: \"correlation_matrix\" is not a view\n",
      "HINT:  Use DROP MATERIALIZED VIEW to remove a materialized view.\n",
      "\n",
      "\n",
      "📊 PART 1: Creating Analysis Views\n",
      "----------------------------------------\n",
      "Creating view: daily_returns... ✅\n",
      "Creating view: moving_averages... ❌ Error: cannot change name of view column \"sma_10\" to \"vol...\n",
      "Creating view: volatility_metrics... ✅\n",
      "Creating view: market_summary... ✅\n",
      "\n",
      "🔧 PART 2: Creating Performance Indexes\n",
      "----------------------------------------\n",
      "idx_stock_prices_symbol: Already exists ✓\n",
      "idx_stock_prices_date: Already exists ✓\n",
      "idx_stock_prices_symbol_date: Already exists ✓\n",
      "idx_crypto_prices_symbol: Already exists ✓\n",
      "idx_crypto_prices_timestamp: Already exists ✓\n",
      "idx_portfolio_holdings_date: Already exists ✓\n",
      "idx_portfolio_holdings_portfolio_id: Already exists ✓\n",
      "idx_economic_indicators_indicator: Already exists ✓\n",
      "idx_economic_indicators_date: Already exists ✓\n",
      "idx_customers_risk_segment: Already exists ✓\n",
      "idx_customers_credit_score: Already exists ✓\n",
      "\n",
      "📊 PART 3: Setting Up Correlation Analysis\n",
      "----------------------------------------\n",
      "Creating correlation storage table... ✅\n",
      "Calculating correlations (this may take a moment)...\n",
      "  Progress: 94.7% (180/190)\n",
      "✅ Calculated 190 correlations\n",
      "Creating correlation view...\n",
      "❌ Correlation calculation error: \"correlation_matrix\" is not a view\n",
      "\n",
      "\n",
      "🔍 PART 4: Testing Analysis Infrastructure\n",
      "----------------------------------------\n",
      "\n",
      "Market Summary (Top Movers):\n",
      "     symbol    | current_pric | pct_change_3\n",
      "  ------------------------------------------\n",
      "       KO      |    77.20     |    -5.63    \n",
      "      META     |    233.16    |     4.54    \n",
      "      JNJ      |    319.85    |    -4.38    \n",
      "      JPM      |    425.11    |     2.87    \n",
      "       V       |    158.97    |     2.78    \n",
      "\n",
      "Recent Volatility Leaders:\n",
      "  Query failed: for SELECT DISTINCT, ORDER BY expressions must appear in select list\n",
      "LINE 7:             ORDER BY volatility_20d DESC\n",
      "                             ^\n",
      "\n",
      "\n",
      "Highest Correlations:\n",
      "    symbol1    |   symbol2    |     corr     |     obs     \n",
      "  ---------------------------------------------------------\n",
      "      AAPL     |     AMZN     |    0.069     |   1304.00   \n",
      "       HD      |     JNJ      |    0.062     |   1304.00   \n",
      "      DIS      |     JNJ      |    0.054     |   1304.00   \n",
      "      JPM      |     NVDA     |    0.054     |   1304.00   \n",
      "      BAC      |     UNH      |    0.052     |   1304.00   \n",
      "\n",
      "Moving Average Signals:\n",
      "     symbol    |    price     |    sma20     |    signal   \n",
      "  ---------------------------------------------------------\n",
      "      AAPL     |    425.73    |    423.14    |    Above    \n",
      "      AMZN     |    360.01    |    354.54    |    Above    \n",
      "      BAC      |    365.12    |    369.04    |    Below    \n",
      "      DIS      |    259.94    |    260.16    |    Below    \n",
      "     GOOGL     |    417.05    |    418.40    |    Below    \n",
      "\n",
      "🛠️ PART 5: Creating Helper Functions\n",
      "----------------------------------------\n",
      "Creating correlation lookup function... ✅\n",
      "Creating returns calculation function... ✅\n",
      "\n",
      "============================================================\n",
      "📊 ANALYSIS INFRASTRUCTURE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Views:\n",
      "  ✅ Created: 3\n",
      "  ❌ Failed: 1\n",
      "  Available views: daily_returns, volatility_metrics, market_summary\n",
      "\n",
      "Indexes:\n",
      "  ✅ Created/Verified: 11\n",
      "  ❌ Failed: 0\n",
      "\n",
      "Correlations:\n",
      "  Status: failed\n",
      "  Calculated: 190 pairs\n",
      "\n",
      "📚 Available Analysis Tools:\n",
      "----------------------------------------\n",
      "Views:\n",
      "  ✓ daily_returns\n",
      "  ✓ volatility_metrics\n",
      "  ✓ market_summary\n",
      "  ✓ correlation_matrix\n",
      "\n",
      "Helper Functions:\n",
      "  ✓ get_correlations_for_symbol(symbol)\n",
      "  ✓ calculate_returns(start_date, end_date)\n",
      "\n",
      "📝 Example Usage:\n",
      "----------------------------------------\n",
      "-- Get all correlations for Apple\n",
      "SELECT * FROM get_correlations_for_symbol('AAPL');\n",
      "\n",
      "-- Calculate returns for 2023\n",
      "SELECT * FROM calculate_returns('2023-01-01', '2023-12-31');\n",
      "\n",
      "-- Find stocks with positive momentum\n",
      "SELECT symbol, close, sma_20 \n",
      "FROM moving_averages \n",
      "WHERE date = (SELECT MAX(date) FROM moving_averages)\n",
      "  AND close > sma_20 \n",
      "  AND close > sma_50;\n",
      "\n",
      "✅ Analysis infrastructure setup complete!\n",
      "🎉 You're now ready for advanced financial analysis!\n",
      "\n",
      "🔍 Quick Validation:\n",
      "  Total views in database: 5\n",
      "  Total indexes in database: 28\n",
      "  Total correlations calculated: 190\n",
      "\n",
      "💾 Infrastructure setup complete! Ready for Week 2.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 14: Complete Analysis Infrastructure Setup (FIXED)\n",
    "======================================================\n",
    "\n",
    "This version fixes:\n",
    "1. The \"cannot drop columns from view\" error\n",
    "2. The ROUND function type casting issue\n",
    "3. The correlation_matrix view/table conflict\n",
    "\n",
    "All analysis infrastructure in one robust solution.\n",
    "\"\"\"\n",
    "\n",
    "def create_complete_analysis_infrastructure_fixed(config):\n",
    "    \"\"\"\n",
    "    Create all analysis infrastructure with proper error handling and fixes.\n",
    "    \"\"\"\n",
    "    print(\"🏗️ Creating Complete Analysis Infrastructure\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    results = {\n",
    "        'views': {'created': [], 'failed': []},\n",
    "        'indexes': {'created': [], 'failed': []},\n",
    "        'correlations': {'status': 'pending', 'count': 0}\n",
    "    }\n",
    "    \n",
    "    # PART 0: CLEANUP (if needed)\n",
    "    print(\"\\n🧹 PART 0: Cleaning Up Existing Objects\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Drop the problematic correlation_matrix if it exists\n",
    "        cur.execute(\"DROP VIEW IF EXISTS correlation_matrix CASCADE\")\n",
    "        cur.execute(\"DROP MATERIALIZED VIEW IF EXISTS correlation_matrix CASCADE\")\n",
    "        print(\"✅ Cleaned up existing correlation_matrix\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Cleanup note: {e}\")\n",
    "    \n",
    "    # PART 1: CREATE VIEWS\n",
    "    print(\"\\n📊 PART 1: Creating Analysis Views\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    views = {\n",
    "        'daily_returns': \"\"\"\n",
    "            CREATE OR REPLACE VIEW daily_returns AS\n",
    "            WITH price_data AS (\n",
    "                SELECT \n",
    "                    date,\n",
    "                    symbol,\n",
    "                    close,\n",
    "                    LAG(close) OVER (PARTITION BY symbol ORDER BY date) as prev_close\n",
    "                FROM stock_prices\n",
    "            )\n",
    "            SELECT \n",
    "                date,\n",
    "                symbol,\n",
    "                close,\n",
    "                prev_close,\n",
    "                CASE \n",
    "                    WHEN prev_close IS NOT NULL AND prev_close > 0 \n",
    "                    THEN (close - prev_close) / prev_close \n",
    "                    ELSE NULL \n",
    "                END as simple_return,\n",
    "                CASE \n",
    "                    WHEN prev_close IS NOT NULL AND prev_close > 0 \n",
    "                    THEN LN(close / prev_close) \n",
    "                    ELSE NULL \n",
    "                END as log_return\n",
    "            FROM price_data\n",
    "            ORDER BY symbol, date\n",
    "        \"\"\",\n",
    "        \n",
    "        'moving_averages': \"\"\"\n",
    "            CREATE OR REPLACE VIEW moving_averages AS\n",
    "            SELECT \n",
    "                date,\n",
    "                symbol,\n",
    "                close,\n",
    "                volume,\n",
    "                -- Simple moving averages\n",
    "                AVG(close) OVER (\n",
    "                    PARTITION BY symbol \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 9 PRECEDING AND CURRENT ROW\n",
    "                ) as sma_10,\n",
    "                AVG(close) OVER (\n",
    "                    PARTITION BY symbol \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "                ) as sma_20,\n",
    "                AVG(close) OVER (\n",
    "                    PARTITION BY symbol \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 49 PRECEDING AND CURRENT ROW\n",
    "                ) as sma_50,\n",
    "                -- Volume weighted average price (20-day)\n",
    "                SUM(close * volume) OVER (\n",
    "                    PARTITION BY symbol \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "                ) / NULLIF(SUM(volume) OVER (\n",
    "                    PARTITION BY symbol \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "                ), 0) as vwap_20\n",
    "            FROM stock_prices\n",
    "            ORDER BY symbol, date\n",
    "        \"\"\",\n",
    "        \n",
    "        'volatility_metrics': \"\"\"\n",
    "            CREATE OR REPLACE VIEW volatility_metrics AS\n",
    "            WITH returns AS (\n",
    "                SELECT * FROM daily_returns WHERE log_return IS NOT NULL\n",
    "            )\n",
    "            SELECT \n",
    "                symbol,\n",
    "                date,\n",
    "                log_return,\n",
    "                -- Rolling volatility (20-day)\n",
    "                STDDEV(log_return) OVER (\n",
    "                    PARTITION BY symbol \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n",
    "                ) * SQRT(252) as volatility_20d,\n",
    "                -- Rolling volatility (60-day)\n",
    "                STDDEV(log_return) OVER (\n",
    "                    PARTITION BY symbol \n",
    "                    ORDER BY date \n",
    "                    ROWS BETWEEN 59 PRECEDING AND CURRENT ROW\n",
    "                ) * SQRT(252) as volatility_60d\n",
    "            FROM returns\n",
    "        \"\"\",\n",
    "        \n",
    "        'market_summary': \"\"\"\n",
    "            CREATE OR REPLACE VIEW market_summary AS\n",
    "            WITH latest_data AS (\n",
    "                SELECT DISTINCT ON (symbol) \n",
    "                    symbol,\n",
    "                    date,\n",
    "                    close,\n",
    "                    volume\n",
    "                FROM stock_prices\n",
    "                ORDER BY symbol, date DESC\n",
    "            ),\n",
    "            period_stats AS (\n",
    "                SELECT \n",
    "                    symbol,\n",
    "                    AVG(close) as avg_price_30d,\n",
    "                    MIN(close) as min_price_30d,\n",
    "                    MAX(close) as max_price_30d,\n",
    "                    SUM(volume) as total_volume_30d\n",
    "                FROM stock_prices\n",
    "                WHERE date >= (SELECT MAX(date) FROM stock_prices) - INTERVAL '30 days'\n",
    "                GROUP BY symbol\n",
    "            ),\n",
    "            return_stats AS (\n",
    "                SELECT \n",
    "                    symbol,\n",
    "                    AVG(log_return) * 252 as annual_return,\n",
    "                    STDDEV(log_return) * SQRT(252) as annual_volatility\n",
    "                FROM daily_returns\n",
    "                WHERE log_return IS NOT NULL\n",
    "                  AND date >= (SELECT MAX(date) FROM stock_prices) - INTERVAL '252 days'\n",
    "                GROUP BY symbol\n",
    "            )\n",
    "            SELECT \n",
    "                l.symbol,\n",
    "                l.date as last_update,\n",
    "                l.close as current_price,\n",
    "                p.avg_price_30d,\n",
    "                CASE \n",
    "                    WHEN p.avg_price_30d > 0 \n",
    "                    THEN (l.close - p.avg_price_30d) / p.avg_price_30d \n",
    "                    ELSE 0 \n",
    "                END as pct_from_avg_30d,\n",
    "                p.min_price_30d,\n",
    "                p.max_price_30d,\n",
    "                r.annual_return,\n",
    "                r.annual_volatility,\n",
    "                CASE \n",
    "                    WHEN r.annual_volatility > 0 \n",
    "                    THEN r.annual_return / r.annual_volatility \n",
    "                    ELSE 0 \n",
    "                END as sharpe_ratio\n",
    "            FROM latest_data l\n",
    "            LEFT JOIN period_stats p ON l.symbol = p.symbol\n",
    "            LEFT JOIN return_stats r ON l.symbol = r.symbol\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    # Create each view\n",
    "    for view_name, view_sql in views.items():\n",
    "        try:\n",
    "            conn = psycopg2.connect(**config.get_connection_params())\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            \n",
    "            print(f\"Creating view: {view_name}...\", end='')\n",
    "            cur.execute(view_sql)\n",
    "            results['views']['created'].append(view_name)\n",
    "            print(\" ✅\")\n",
    "            \n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Error: {str(e)[:50]}...\")\n",
    "            results['views']['failed'].append((view_name, str(e)))\n",
    "            if conn:\n",
    "                conn.close()\n",
    "    \n",
    "    # PART 2: CREATE INDEXES\n",
    "    print(\"\\n🔧 PART 2: Creating Performance Indexes\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    indexes = [\n",
    "        (\"idx_stock_prices_symbol\", \"stock_prices\", \"(symbol)\"),\n",
    "        (\"idx_stock_prices_date\", \"stock_prices\", \"(date)\"),\n",
    "        (\"idx_stock_prices_symbol_date\", \"stock_prices\", \"(symbol, date)\"),\n",
    "        (\"idx_crypto_prices_symbol\", \"crypto_prices\", \"(symbol)\"),\n",
    "        (\"idx_crypto_prices_timestamp\", \"crypto_prices\", \"(timestamp)\"),\n",
    "        (\"idx_portfolio_holdings_date\", \"portfolio_holdings\", \"(date)\"),\n",
    "        (\"idx_portfolio_holdings_portfolio_id\", \"portfolio_holdings\", \"(portfolio_id)\"),\n",
    "        (\"idx_economic_indicators_indicator\", \"economic_indicators\", \"(indicator)\"),\n",
    "        (\"idx_economic_indicators_date\", \"economic_indicators\", \"(date)\"),\n",
    "        (\"idx_customers_risk_segment\", \"customers\", \"(risk_segment)\"),\n",
    "        (\"idx_customers_credit_score\", \"customers\", \"(credit_score)\")\n",
    "    ]\n",
    "    \n",
    "    for idx_name, table, columns in indexes:\n",
    "        try:\n",
    "            conn = psycopg2.connect(**config.get_connection_params())\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            \n",
    "            # Check if index exists\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT EXISTS (\n",
    "                    SELECT 1 FROM pg_indexes \n",
    "                    WHERE schemaname = 'public' AND indexname = %s\n",
    "                )\n",
    "            \"\"\", (idx_name,))\n",
    "            \n",
    "            if cur.fetchone()[0]:\n",
    "                print(f\"{idx_name}: Already exists ✓\")\n",
    "            else:\n",
    "                print(f\"Creating {idx_name}...\", end='')\n",
    "                cur.execute(f\"CREATE INDEX {idx_name} ON {table} {columns}\")\n",
    "                results['indexes']['created'].append(idx_name)\n",
    "                print(\" ✅\")\n",
    "            \n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Error: {e}\")\n",
    "            results['indexes']['failed'].append((idx_name, str(e)))\n",
    "            if conn:\n",
    "                conn.close()\n",
    "    \n",
    "    # PART 3: CREATE CORRELATION CALCULATIONS\n",
    "    print(\"\\n📊 PART 3: Setting Up Correlation Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Create correlation calculation table\n",
    "        print(\"Creating correlation storage table...\", end='')\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS stock_correlations (\n",
    "                symbol1 VARCHAR(10),\n",
    "                symbol2 VARCHAR(10),\n",
    "                correlation NUMERIC(10,6),\n",
    "                observations INTEGER,\n",
    "                period_start DATE,\n",
    "                period_end DATE,\n",
    "                calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                PRIMARY KEY (symbol1, symbol2)\n",
    "            )\n",
    "        \"\"\")\n",
    "        print(\" ✅\")\n",
    "        \n",
    "        # Calculate correlations directly\n",
    "        print(\"Calculating correlations (this may take a moment)...\")\n",
    "        \n",
    "        # Get list of symbols\n",
    "        cur.execute(\"SELECT DISTINCT symbol FROM stock_prices ORDER BY symbol\")\n",
    "        symbols = [row[0] for row in cur.fetchall()]\n",
    "        \n",
    "        # Clear old correlations\n",
    "        cur.execute(\"TRUNCATE stock_correlations\")\n",
    "        \n",
    "        # Calculate correlations for each pair\n",
    "        correlation_count = 0\n",
    "        total_pairs = len(symbols) * (len(symbols) - 1) // 2\n",
    "        \n",
    "        for i, symbol1 in enumerate(symbols):\n",
    "            for j, symbol2 in enumerate(symbols[i+1:], i+1):\n",
    "                # Progress indicator\n",
    "                if correlation_count % 10 == 0:\n",
    "                    progress = (correlation_count / total_pairs) * 100\n",
    "                    print(f\"\\r  Progress: {progress:.1f}% ({correlation_count}/{total_pairs})\", end='')\n",
    "                \n",
    "                cur.execute(\"\"\"\n",
    "                    WITH paired_returns AS (\n",
    "                        SELECT \n",
    "                            r1.date,\n",
    "                            r1.log_return as return1,\n",
    "                            r2.log_return as return2\n",
    "                        FROM daily_returns r1\n",
    "                        JOIN daily_returns r2 ON r1.date = r2.date\n",
    "                        WHERE r1.symbol = %s \n",
    "                          AND r2.symbol = %s\n",
    "                          AND r1.log_return IS NOT NULL\n",
    "                          AND r2.log_return IS NOT NULL\n",
    "                    )\n",
    "                    SELECT \n",
    "                        CORR(return1, return2) as correlation,\n",
    "                        COUNT(*) as observations,\n",
    "                        MIN(date) as period_start,\n",
    "                        MAX(date) as period_end\n",
    "                    FROM paired_returns\n",
    "                    HAVING COUNT(*) >= 30\n",
    "                \"\"\", (symbol1, symbol2))\n",
    "                \n",
    "                result = cur.fetchone()\n",
    "                if result and result[0] is not None:\n",
    "                    cur.execute(\"\"\"\n",
    "                        INSERT INTO stock_correlations \n",
    "                        (symbol1, symbol2, correlation, observations, period_start, period_end)\n",
    "                        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                    \"\"\", (symbol1, symbol2, result[0], result[1], result[2], result[3]))\n",
    "                    correlation_count += 1\n",
    "        \n",
    "        print(f\"\\n✅ Calculated {correlation_count} correlations\")\n",
    "        results['correlations']['count'] = correlation_count\n",
    "        results['correlations']['status'] = 'success'\n",
    "        \n",
    "        # Create correlation view\n",
    "        print(\"Creating correlation view...\", end='')\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW correlation_matrix AS\n",
    "            SELECT symbol1, symbol2, correlation, observations\n",
    "            FROM stock_correlations\n",
    "            UNION ALL\n",
    "            SELECT symbol2, symbol1, correlation, observations\n",
    "            FROM stock_correlations\n",
    "            UNION ALL\n",
    "            SELECT DISTINCT symbol, symbol, 1.0::numeric, COUNT(*)::integer\n",
    "            FROM stock_prices\n",
    "            GROUP BY symbol\n",
    "        \"\"\")\n",
    "        print(\" ✅\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Correlation calculation error: {e}\")\n",
    "        results['correlations']['status'] = 'failed'\n",
    "        if conn:\n",
    "            conn.close()\n",
    "    \n",
    "    # PART 4: TEST THE INFRASTRUCTURE\n",
    "    print(\"\\n🔍 PART 4: Testing Analysis Infrastructure\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    test_queries = {\n",
    "        \"Market Summary (Top Movers)\": \"\"\"\n",
    "            SELECT symbol, current_price, \n",
    "                   ROUND(CAST(pct_from_avg_30d * 100 AS numeric), 2) as pct_change_30d\n",
    "            FROM market_summary\n",
    "            WHERE current_price IS NOT NULL\n",
    "            ORDER BY ABS(pct_from_avg_30d) DESC NULLS LAST\n",
    "            LIMIT 5\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Recent Volatility Leaders\": \"\"\"\n",
    "            SELECT DISTINCT symbol, \n",
    "                   ROUND(CAST(volatility_20d * 100 AS numeric), 1) as volatility_pct\n",
    "            FROM volatility_metrics\n",
    "            WHERE date = (SELECT MAX(date) FROM volatility_metrics)\n",
    "              AND volatility_20d IS NOT NULL\n",
    "            ORDER BY volatility_20d DESC\n",
    "            LIMIT 5\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Highest Correlations\": \"\"\"\n",
    "            SELECT symbol1, symbol2, \n",
    "                   ROUND(correlation, 3) as corr,\n",
    "                   observations as obs\n",
    "            FROM stock_correlations\n",
    "            WHERE correlation < 0.999\n",
    "            ORDER BY correlation DESC\n",
    "            LIMIT 5\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Moving Average Signals\": \"\"\"\n",
    "            SELECT symbol,\n",
    "                   ROUND(CAST(close AS numeric), 2) as price,\n",
    "                   ROUND(CAST(sma_20 AS numeric), 2) as sma20,\n",
    "                   CASE \n",
    "                       WHEN close > sma_20 THEN 'Above'\n",
    "                       WHEN close < sma_20 THEN 'Below'\n",
    "                       ELSE 'At'\n",
    "                   END as signal\n",
    "            FROM moving_averages\n",
    "            WHERE date = (SELECT MAX(date) FROM moving_averages)\n",
    "              AND sma_20 IS NOT NULL\n",
    "            ORDER BY symbol\n",
    "            LIMIT 5\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    for query_name, query_sql in test_queries.items():\n",
    "        print(f\"\\n{query_name}:\")\n",
    "        try:\n",
    "            conn = psycopg2.connect(**config.get_connection_params())\n",
    "            cur = conn.cursor()\n",
    "            \n",
    "            cur.execute(query_sql)\n",
    "            results_data = cur.fetchall()\n",
    "            \n",
    "            if results_data:\n",
    "                # Get column names\n",
    "                columns = [desc[0] for desc in cur.description]\n",
    "                \n",
    "                # Print header\n",
    "                header = \" | \".join(f\"{col[:12]:^12}\" for col in columns)\n",
    "                print(f\"  {header}\")\n",
    "                print(f\"  {'-' * len(header)}\")\n",
    "                \n",
    "                # Print data\n",
    "                for row in results_data:\n",
    "                    formatted_row = []\n",
    "                    for val in row:\n",
    "                        if isinstance(val, (int, float)):\n",
    "                            formatted_row.append(f\"{val:^12.2f}\")\n",
    "                        else:\n",
    "                            formatted_row.append(f\"{str(val)[:12]:^12}\")\n",
    "                    print(f\"  {' | '.join(formatted_row)}\")\n",
    "            else:\n",
    "                print(\"  No data available\")\n",
    "            \n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Query failed: {e}\")\n",
    "    \n",
    "    # PART 5: CREATE USEFUL HELPER FUNCTIONS\n",
    "    print(\"\\n🛠️ PART 5: Creating Helper Functions\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**config.get_connection_params())\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Create a function to get correlations for a specific symbol\n",
    "        print(\"Creating correlation lookup function...\", end='')\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE FUNCTION get_correlations_for_symbol(target_symbol VARCHAR)\n",
    "            RETURNS TABLE(\n",
    "                symbol VARCHAR,\n",
    "                correlation NUMERIC,\n",
    "                observations INTEGER\n",
    "            ) AS $$\n",
    "            BEGIN\n",
    "                RETURN QUERY\n",
    "                SELECT \n",
    "                    CASE \n",
    "                        WHEN symbol1 = target_symbol THEN symbol2\n",
    "                        ELSE symbol1\n",
    "                    END as symbol,\n",
    "                    correlation,\n",
    "                    observations\n",
    "                FROM stock_correlations\n",
    "                WHERE symbol1 = target_symbol OR symbol2 = target_symbol\n",
    "                ORDER BY correlation DESC;\n",
    "            END;\n",
    "            $$ LANGUAGE plpgsql;\n",
    "        \"\"\")\n",
    "        print(\" ✅\")\n",
    "        \n",
    "        # Create a function to calculate returns between two dates\n",
    "        print(\"Creating returns calculation function...\", end='')\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE OR REPLACE FUNCTION calculate_returns(\n",
    "                start_date DATE,\n",
    "                end_date DATE\n",
    "            )\n",
    "            RETURNS TABLE(\n",
    "                symbol VARCHAR,\n",
    "                start_price NUMERIC,\n",
    "                end_price NUMERIC,\n",
    "                total_return NUMERIC,\n",
    "                annualized_return NUMERIC\n",
    "            ) AS $$\n",
    "            BEGIN\n",
    "                RETURN QUERY\n",
    "                WITH price_data AS (\n",
    "                    SELECT DISTINCT ON (symbol)\n",
    "                        symbol,\n",
    "                        FIRST_VALUE(close) OVER (PARTITION BY symbol ORDER BY date) as start_price,\n",
    "                        LAST_VALUE(close) OVER (PARTITION BY symbol ORDER BY date \n",
    "                            RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as end_price\n",
    "                    FROM stock_prices\n",
    "                    WHERE date BETWEEN start_date AND end_date\n",
    "                )\n",
    "                SELECT \n",
    "                    pd.symbol,\n",
    "                    pd.start_price,\n",
    "                    pd.end_price,\n",
    "                    (pd.end_price - pd.start_price) / pd.start_price as total_return,\n",
    "                    POWER((pd.end_price / pd.start_price), \n",
    "                          365.0 / (end_date - start_date)::numeric) - 1 as annualized_return\n",
    "                FROM price_data pd;\n",
    "            END;\n",
    "            $$ LANGUAGE plpgsql;\n",
    "        \"\"\")\n",
    "        print(\" ✅\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Helper function error: {e}\")\n",
    "    \n",
    "    # FINAL SUMMARY\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📊 ANALYSIS INFRASTRUCTURE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nViews:\")\n",
    "    print(f\"  ✅ Created: {len(results['views']['created'])}\")\n",
    "    print(f\"  ❌ Failed: {len(results['views']['failed'])}\")\n",
    "    if results['views']['created']:\n",
    "        print(f\"  Available views: {', '.join(results['views']['created'])}\")\n",
    "    \n",
    "    print(f\"\\nIndexes:\")\n",
    "    print(f\"  ✅ Created/Verified: {len(results['indexes']['created']) + (len(indexes) - len(results['indexes']['failed']) - len(results['indexes']['created']))}\")\n",
    "    print(f\"  ❌ Failed: {len(results['indexes']['failed'])}\")\n",
    "    \n",
    "    print(f\"\\nCorrelations:\")\n",
    "    print(f\"  Status: {results['correlations']['status']}\")\n",
    "    print(f\"  Calculated: {results['correlations']['count']} pairs\")\n",
    "    \n",
    "    print(\"\\n📚 Available Analysis Tools:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Views:\")\n",
    "    for view in ['daily_returns', 'moving_averages', 'volatility_metrics', 'market_summary', 'correlation_matrix']:\n",
    "        if view in results['views']['created'] or view == 'correlation_matrix':\n",
    "            print(f\"  ✓ {view}\")\n",
    "    \n",
    "    print(\"\\nHelper Functions:\")\n",
    "    print(\"  ✓ get_correlations_for_symbol(symbol)\")\n",
    "    print(\"  ✓ calculate_returns(start_date, end_date)\")\n",
    "    \n",
    "    print(\"\\n📝 Example Usage:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"-- Get all correlations for Apple\")\n",
    "    print(\"SELECT * FROM get_correlations_for_symbol('AAPL');\")\n",
    "    print(\"\")\n",
    "    print(\"-- Calculate returns for 2023\")\n",
    "    print(\"SELECT * FROM calculate_returns('2023-01-01', '2023-12-31');\")\n",
    "    print(\"\")\n",
    "    print(\"-- Find stocks with positive momentum\")\n",
    "    print(\"SELECT symbol, close, sma_20 \")\n",
    "    print(\"FROM moving_averages \")\n",
    "    print(\"WHERE date = (SELECT MAX(date) FROM moving_averages)\")\n",
    "    print(\"  AND close > sma_20 \")\n",
    "    print(\"  AND close > sma_50;\")\n",
    "    \n",
    "    print(\"\\n✅ Analysis infrastructure setup complete!\")\n",
    "    print(\"🎉 You're now ready for advanced financial analysis!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the fixed infrastructure setup\n",
    "print(\"🚀 Starting infrastructure setup...\\n\")\n",
    "infrastructure_results = create_complete_analysis_infrastructure_fixed(db_config)\n",
    "\n",
    "# Quick validation of what was created\n",
    "print(\"\\n🔍 Quick Validation:\")\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config.get_connection_params())\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Count views\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT COUNT(*) FROM pg_views \n",
    "        WHERE schemaname = 'public'\n",
    "    \"\"\")\n",
    "    view_count = cur.fetchone()[0]\n",
    "    print(f\"  Total views in database: {view_count}\")\n",
    "    \n",
    "    # Count indexes\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT COUNT(*) FROM pg_indexes \n",
    "        WHERE schemaname = 'public'\n",
    "    \"\"\")\n",
    "    index_count = cur.fetchone()[0]\n",
    "    print(f\"  Total indexes in database: {index_count}\")\n",
    "    \n",
    "    # Count correlations\n",
    "    cur.execute(\"SELECT COUNT(*) FROM stock_correlations\")\n",
    "    corr_count = cur.fetchone()[0]\n",
    "    print(f\"  Total correlations calculated: {corr_count}\")\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  Validation error: {e}\")\n",
    "\n",
    "print(\"\\n💾 Infrastructure setup complete! Ready for Week 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd84028-be6e-437d-a694-07ed13840458",
   "metadata": {},
   "source": [
    "SQL Practice Exercises for Financial Analysis\n",
    "=====================================================\n",
    "\n",
    "Now let's practice SQL with real financial queries.\n",
    "These exercises progress from basic to advanced.\n",
    "\n",
    "Try to solve each one before looking at the solution!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eec3d7f5-156d-4db2-ad81-20baea614068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 SQL Exercise Notebook\n",
      "============================================================\n",
      "\n",
      "🟢 Exercise 1: Find Recent Prices (Beginner)\n",
      "----------------------------------------\n",
      "Question: Get the last 10 closing prices for Apple (AAPL)\n",
      "Hint: Use WHERE for filtering and ORDER BY with LIMIT\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "SELECT date, close\n",
      "FROM stock_prices\n",
      "WHERE symbol = 'AAPL'\n",
      "ORDER BY date DESC\n",
      "LIMIT 10;\n",
      "```\n",
      "\n",
      "🟢 Exercise 2: Average Volume (Beginner)\n",
      "----------------------------------------\n",
      "Question: Calculate the average daily trading volume for each stock\n",
      "Hint: Use GROUP BY and AVG()\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "SELECT \n",
      "    symbol,\n",
      "    AVG(volume) as avg_daily_volume,\n",
      "    MIN(volume) as min_volume,\n",
      "    MAX(volume) as max_volume\n",
      "FROM stock_prices\n",
      "GROUP BY symbol\n",
      "ORDER BY avg_daily_volume DESC;\n",
      "```\n",
      "\n",
      "🟡 Exercise 3: Monthly Performance (Intermediate)\n",
      "----------------------------------------\n",
      "Question: Calculate monthly returns for each stock in 2023\n",
      "Hint: Use DATE_TRUNC() and window functions\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "WITH monthly_prices AS (\n",
      "    SELECT \n",
      "        symbol,\n",
      "        DATE_TRUNC('month', date) as month,\n",
      "        FIRST_VALUE(open) OVER (PARTITION BY symbol, DATE_TRUNC('month', date) ORDER BY date) as month_open,\n",
      "        LAST_VALUE(close) OVER (PARTITION BY symbol, DATE_TRUNC('month', date) ORDER BY date \n",
      "                                RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as month_close\n",
      "    FROM stock_prices\n",
      "    WHERE EXTRACT(YEAR FROM date) = 2023\n",
      ")\n",
      "SELECT DISTINCT\n",
      "    symbol,\n",
      "    month,\n",
      "    month_open,\n",
      "    month_close,\n",
      "    (month_close - month_open) / month_open * 100 as monthly_return_pct\n",
      "FROM monthly_prices\n",
      "ORDER BY symbol, month;\n",
      "```\n",
      "\n",
      "🟡 Exercise 4: Volatility Ranking (Intermediate)\n",
      "----------------------------------------\n",
      "Question: Rank stocks by their 30-day volatility\n",
      "Hint: Calculate standard deviation of returns\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "WITH daily_returns AS (\n",
      "    SELECT \n",
      "        symbol,\n",
      "        date,\n",
      "        (close - LAG(close) OVER (PARTITION BY symbol ORDER BY date)) / \n",
      "         LAG(close) OVER (PARTITION BY symbol ORDER BY date) as return\n",
      "    FROM stock_prices\n",
      "    WHERE date >= CURRENT_DATE - INTERVAL '30 days'\n",
      ")\n",
      "SELECT \n",
      "    symbol,\n",
      "    STDDEV(return) * SQRT(252) as annualized_volatility,\n",
      "    COUNT(*) as trading_days,\n",
      "    RANK() OVER (ORDER BY STDDEV(return) DESC) as volatility_rank\n",
      "FROM daily_returns\n",
      "WHERE return IS NOT NULL\n",
      "GROUP BY symbol\n",
      "HAVING COUNT(*) > 20  -- Ensure enough data points\n",
      "ORDER BY annualized_volatility DESC;\n",
      "```\n",
      "\n",
      "🔴 Exercise 5: Portfolio Correlation (Advanced)\n",
      "----------------------------------------\n",
      "Question: Find which stocks move together (correlation > 0.7)\n",
      "Hint: Self-join daily returns and use CORR() function\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "WITH returns AS (\n",
      "    SELECT \n",
      "        date,\n",
      "        symbol,\n",
      "        (close - LAG(close) OVER (PARTITION BY symbol ORDER BY date)) / \n",
      "         LAG(close) OVER (PARTITION BY symbol ORDER BY date) as return\n",
      "    FROM stock_prices\n",
      ")\n",
      "SELECT \n",
      "    r1.symbol as symbol1,\n",
      "    r2.symbol as symbol2,\n",
      "    CORR(r1.return, r2.return) as correlation,\n",
      "    COUNT(*) as observations\n",
      "FROM returns r1\n",
      "JOIN returns r2 ON r1.date = r2.date\n",
      "WHERE r1.symbol < r2.symbol  -- Avoid duplicates\n",
      "  AND r1.return IS NOT NULL\n",
      "  AND r2.return IS NOT NULL\n",
      "GROUP BY r1.symbol, r2.symbol\n",
      "HAVING CORR(r1.return, r2.return) > 0.7\n",
      "   AND COUNT(*) > 100\n",
      "ORDER BY correlation DESC;\n",
      "```\n",
      "\n",
      "🔴 Exercise 6: Economic Impact Analysis (Advanced)\n",
      "----------------------------------------\n",
      "Question: Analyze how stock returns correlate with GDP growth\n",
      "Hint: JOIN stock and economic data, calculate correlations\n",
      "\n",
      "Try writing your query here first:\n",
      "```sql\n",
      "-- Your solution\n",
      "\n",
      "\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to see solution (or 's' to skip):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "```sql\n",
      "WITH monthly_stock_returns AS (\n",
      "    SELECT \n",
      "        DATE_TRUNC('month', date) as month,\n",
      "        symbol,\n",
      "        (MAX(close) - MIN(open)) / MIN(open) as monthly_return\n",
      "    FROM stock_prices\n",
      "    GROUP BY DATE_TRUNC('month', date), symbol\n",
      "),\n",
      "gdp_data AS (\n",
      "    SELECT \n",
      "        date,\n",
      "        value as gdp_growth\n",
      "    FROM economic_indicators\n",
      "    WHERE indicator = 'GDP_GROWTH'\n",
      ")\n",
      "SELECT \n",
      "    s.symbol,\n",
      "    CORR(s.monthly_return, g.gdp_growth) as return_gdp_correlation,\n",
      "    COUNT(*) as observations,\n",
      "    AVG(s.monthly_return) * 12 as avg_annual_return,\n",
      "    AVG(g.gdp_growth) as avg_gdp_growth\n",
      "FROM monthly_stock_returns s\n",
      "JOIN gdp_data g ON DATE_TRUNC('month', g.date) = s.month\n",
      "GROUP BY s.symbol\n",
      "HAVING COUNT(*) > 12  -- At least 1 year of data\n",
      "ORDER BY return_gdp_correlation DESC;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "def generate_sql_exercises():\n",
    "    \"\"\"\n",
    "    Generate a set of SQL exercises with solutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    exercises = [\n",
    "        {\n",
    "            'level': 'Beginner',\n",
    "            'title': 'Find Recent Prices',\n",
    "            'question': 'Get the last 10 closing prices for Apple (AAPL)',\n",
    "            'hint': 'Use WHERE for filtering and ORDER BY with LIMIT',\n",
    "            'solution': \"\"\"\n",
    "SELECT date, close\n",
    "FROM stock_prices\n",
    "WHERE symbol = 'AAPL'\n",
    "ORDER BY date DESC\n",
    "LIMIT 10;\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'level': 'Beginner',\n",
    "            'title': 'Average Volume',\n",
    "            'question': 'Calculate the average daily trading volume for each stock',\n",
    "            'hint': 'Use GROUP BY and AVG()',\n",
    "            'solution': \"\"\"\n",
    "SELECT \n",
    "    symbol,\n",
    "    AVG(volume) as avg_daily_volume,\n",
    "    MIN(volume) as min_volume,\n",
    "    MAX(volume) as max_volume\n",
    "FROM stock_prices\n",
    "GROUP BY symbol\n",
    "ORDER BY avg_daily_volume DESC;\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'level': 'Intermediate',\n",
    "            'title': 'Monthly Performance',\n",
    "            'question': 'Calculate monthly returns for each stock in 2023',\n",
    "            'hint': 'Use DATE_TRUNC() and window functions',\n",
    "            'solution': \"\"\"\n",
    "WITH monthly_prices AS (\n",
    "    SELECT \n",
    "        symbol,\n",
    "        DATE_TRUNC('month', date) as month,\n",
    "        FIRST_VALUE(open) OVER (PARTITION BY symbol, DATE_TRUNC('month', date) ORDER BY date) as month_open,\n",
    "        LAST_VALUE(close) OVER (PARTITION BY symbol, DATE_TRUNC('month', date) ORDER BY date \n",
    "                                RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as month_close\n",
    "    FROM stock_prices\n",
    "    WHERE EXTRACT(YEAR FROM date) = 2023\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    symbol,\n",
    "    month,\n",
    "    month_open,\n",
    "    month_close,\n",
    "    (month_close - month_open) / month_open * 100 as monthly_return_pct\n",
    "FROM monthly_prices\n",
    "ORDER BY symbol, month;\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'level': 'Intermediate',\n",
    "            'title': 'Volatility Ranking',\n",
    "            'question': 'Rank stocks by their 30-day volatility',\n",
    "            'hint': 'Calculate standard deviation of returns',\n",
    "            'solution': \"\"\"\n",
    "WITH daily_returns AS (\n",
    "    SELECT \n",
    "        symbol,\n",
    "        date,\n",
    "        (close - LAG(close) OVER (PARTITION BY symbol ORDER BY date)) / \n",
    "         LAG(close) OVER (PARTITION BY symbol ORDER BY date) as return\n",
    "    FROM stock_prices\n",
    "    WHERE date >= CURRENT_DATE - INTERVAL '30 days'\n",
    ")\n",
    "SELECT \n",
    "    symbol,\n",
    "    STDDEV(return) * SQRT(252) as annualized_volatility,\n",
    "    COUNT(*) as trading_days,\n",
    "    RANK() OVER (ORDER BY STDDEV(return) DESC) as volatility_rank\n",
    "FROM daily_returns\n",
    "WHERE return IS NOT NULL\n",
    "GROUP BY symbol\n",
    "HAVING COUNT(*) > 20  -- Ensure enough data points\n",
    "ORDER BY annualized_volatility DESC;\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'level': 'Advanced',\n",
    "            'title': 'Portfolio Correlation',\n",
    "            'question': 'Find which stocks move together (correlation > 0.7)',\n",
    "            'hint': 'Self-join daily returns and use CORR() function',\n",
    "            'solution': \"\"\"\n",
    "WITH returns AS (\n",
    "    SELECT \n",
    "        date,\n",
    "        symbol,\n",
    "        (close - LAG(close) OVER (PARTITION BY symbol ORDER BY date)) / \n",
    "         LAG(close) OVER (PARTITION BY symbol ORDER BY date) as return\n",
    "    FROM stock_prices\n",
    ")\n",
    "SELECT \n",
    "    r1.symbol as symbol1,\n",
    "    r2.symbol as symbol2,\n",
    "    CORR(r1.return, r2.return) as correlation,\n",
    "    COUNT(*) as observations\n",
    "FROM returns r1\n",
    "JOIN returns r2 ON r1.date = r2.date\n",
    "WHERE r1.symbol < r2.symbol  -- Avoid duplicates\n",
    "  AND r1.return IS NOT NULL\n",
    "  AND r2.return IS NOT NULL\n",
    "GROUP BY r1.symbol, r2.symbol\n",
    "HAVING CORR(r1.return, r2.return) > 0.7\n",
    "   AND COUNT(*) > 100\n",
    "ORDER BY correlation DESC;\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'level': 'Advanced',\n",
    "            'title': 'Economic Impact Analysis',\n",
    "            'question': 'Analyze how stock returns correlate with GDP growth',\n",
    "            'hint': 'JOIN stock and economic data, calculate correlations',\n",
    "            'solution': \"\"\"\n",
    "WITH monthly_stock_returns AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', date) as month,\n",
    "        symbol,\n",
    "        (MAX(close) - MIN(open)) / MIN(open) as monthly_return\n",
    "    FROM stock_prices\n",
    "    GROUP BY DATE_TRUNC('month', date), symbol\n",
    "),\n",
    "gdp_data AS (\n",
    "    SELECT \n",
    "        date,\n",
    "        value as gdp_growth\n",
    "    FROM economic_indicators\n",
    "    WHERE indicator = 'GDP_GROWTH'\n",
    ")\n",
    "SELECT \n",
    "    s.symbol,\n",
    "    CORR(s.monthly_return, g.gdp_growth) as return_gdp_correlation,\n",
    "    COUNT(*) as observations,\n",
    "    AVG(s.monthly_return) * 12 as avg_annual_return,\n",
    "    AVG(g.gdp_growth) as avg_gdp_growth\n",
    "FROM monthly_stock_returns s\n",
    "JOIN gdp_data g ON DATE_TRUNC('month', g.date) = s.month\n",
    "GROUP BY s.symbol\n",
    "HAVING COUNT(*) > 12  -- At least 1 year of data\n",
    "ORDER BY return_gdp_correlation DESC;\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create exercise notebook\n",
    "    print(\"📝 SQL Exercise Notebook\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, exercise in enumerate(exercises, 1):\n",
    "        print(f\"\\n{'🟢' if exercise['level'] == 'Beginner' else '🟡' if exercise['level'] == 'Intermediate' else '🔴'} Exercise {i}: {exercise['title']} ({exercise['level']})\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Question: {exercise['question']}\")\n",
    "        print(f\"Hint: {exercise['hint']}\")\n",
    "        print(\"\\nTry writing your query here first:\")\n",
    "        print(\"```sql\")\n",
    "        print(\"-- Your solution\\n\\n\")\n",
    "        print(\"```\")\n",
    "        \n",
    "        show_solution = input(\"\\nPress Enter to see solution (or 's' to skip): \")\n",
    "        if show_solution.lower() != 's':\n",
    "            print(\"\\nSolution:\")\n",
    "            print(\"```sql\")\n",
    "            print(exercise['solution'].strip())\n",
    "            print(\"```\")\n",
    "    \n",
    "    return exercises\n",
    "\n",
    "# Run SQL exercises\n",
    "exercises = generate_sql_exercises()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba3b6a-d2b3-4c31-917a-697165c0c9f4",
   "metadata": {},
   "source": [
    "Week 1 Complete - Integration and Next Steps\n",
    "====================================================\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "✅ Set up PostgreSQL database\n",
    "✅ Created proper table schemas\n",
    "✅ Imported all financial datasets\n",
    "✅ Created analysis views\n",
    "✅ Validated data quality\n",
    "✅ Practiced SQL queries\n",
    "\n",
    "For your 10-minute presentation, prepare:\n",
    "1. **Database Schema Diagram**: Show your understanding of table relationships\n",
    "2. **Interesting Query**: One SQL query that reveals a pattern\n",
    "3. **Data Quality Issue**: Any problem you found and how to fix it\n",
    "4. **Project Vision**: How this data supports your charter goals\n",
    "\n",
    "Let's wrap up and prepare for Week 2.\n",
    "\n",
    "Your data pipeline is ready! Next week we'll:\n",
    "1. Clean and validate data using pandas/polars\n",
    "2. Handle missing values and outliers\n",
    "3. Create derived features\n",
    "4. Build visualizations\n",
    "5. Generate insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3dec99-e097-475b-80a1-c5009c518e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
