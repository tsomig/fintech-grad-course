{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0a4fd4-1331-4c2f-8baa-64ee57d08850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eabc6b8-b49c-4a77-91df-020d1808bd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Time Series Dataset\n",
      "==================================================\n",
      "Shape: (700, 200)\n",
      "Date Range: 2024-01-01 to 2025-11-30\n",
      "Price Range: $28.34 - $1884.89\n",
      "\n",
      "Dataset Preview:\n",
      "            ASSET_01  ASSET_02  ASSET_03  ASSET_04  ASSET_05  ASSET_06  \\\n",
      "2024-01-01     95.10     66.54     97.23    139.55     77.47    154.16   \n",
      "2024-01-02     95.97     66.90     96.84    138.36     77.08    152.61   \n",
      "2024-01-03     95.20     65.96     97.85    138.24     76.84    152.71   \n",
      "2024-01-04     96.28     65.65     97.62    140.46     77.83    153.68   \n",
      "2024-01-05     97.92     65.25     96.69    140.20     77.65    152.64   \n",
      "2024-01-06     97.09     64.97     94.59    143.59     77.74    152.93   \n",
      "2024-01-07    108.48     64.06     96.09    142.73     77.26    152.08   \n",
      "2024-01-08    107.99     65.54     97.12    143.62     79.09    152.82   \n",
      "2024-01-09    105.88     65.73     98.52    141.01     74.47    152.81   \n",
      "2024-01-10    105.28     64.79     97.80    142.74     74.39    156.77   \n",
      "\n",
      "            ASSET_07  ASSET_08  ASSET_09  ASSET_10  ...  ASSET_191  ASSET_192  \\\n",
      "2024-01-01     92.68    111.98    144.21    187.20  ...     134.22     105.71   \n",
      "2024-01-02     92.97    111.21    140.78    181.79  ...     133.08     109.95   \n",
      "2024-01-03     92.09    111.73    140.23    180.57  ...     134.40     109.56   \n",
      "2024-01-04     90.66    110.11    137.40    180.99  ...     134.65     108.31   \n",
      "2024-01-05     87.77    110.72    134.15    186.70  ...     134.82     111.58   \n",
      "2024-01-06     87.18    112.33    132.93    185.16  ...     135.57     111.03   \n",
      "2024-01-07     87.36    110.76    134.13    182.42  ...     137.45     111.32   \n",
      "2024-01-08     87.20    110.20    134.98    186.96  ...     130.95     114.27   \n",
      "2024-01-09     87.52    110.28    133.57    189.21  ...     130.69     114.56   \n",
      "2024-01-10     91.36    109.86    134.76    187.25  ...     130.91     114.15   \n",
      "\n",
      "            ASSET_193  ASSET_194  ASSET_195  ASSET_196  ASSET_197  ASSET_198  \\\n",
      "2024-01-01     156.42      75.52     141.44     194.76      55.51     190.42   \n",
      "2024-01-02     157.41      75.10     140.66     195.82      56.74     187.70   \n",
      "2024-01-03     152.90      75.55     141.27     201.22      57.04     188.78   \n",
      "2024-01-04     152.11      74.65     142.09     201.45      57.86     190.06   \n",
      "2024-01-05     153.30      74.91     142.84     199.08      58.67     188.93   \n",
      "2024-01-06     154.73      75.13     141.43     199.26      58.00     182.50   \n",
      "2024-01-07     151.40      74.18     142.32     199.92      57.44     180.51   \n",
      "2024-01-08     150.27      74.02     144.32     200.68      58.05     181.05   \n",
      "2024-01-09     150.69      74.34     145.37     198.91      56.03     181.21   \n",
      "2024-01-10     149.88      73.13     143.53     197.09      64.85     187.17   \n",
      "\n",
      "            ASSET_199  ASSET_200  \n",
      "2024-01-01     137.44     193.72  \n",
      "2024-01-02     135.51     189.04  \n",
      "2024-01-03     135.89     192.41  \n",
      "2024-01-04     135.13     193.76  \n",
      "2024-01-05     135.94     190.89  \n",
      "2024-01-06     139.48     190.22  \n",
      "2024-01-07     138.93     189.25  \n",
      "2024-01-08     139.65     188.73  \n",
      "2024-01-09     135.15     186.63  \n",
      "2024-01-10     135.81     184.77  \n",
      "\n",
      "[10 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "def generate_financial_timeseries(rows, cols, start_price=100):\n",
    "    \"\"\"\n",
    "    Generate realistic financial time series data with the following characteristics:\n",
    "    - Geometric Brownian Motion for price evolution\n",
    "    - Realistic volatility (15-25% annualized)\n",
    "    - Occasional jumps and mean reversion\n",
    "    - Correlation between some assets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters for realistic financial data\n",
    "    dt = 1/252  # Daily time step (252 trading days per year)\n",
    "    mu = 0.08   # Annual drift (8% expected return)\n",
    "    \n",
    "    # Initialize the price matrix\n",
    "    prices = np.zeros((rows, cols))\n",
    "    \n",
    "    # Set different starting prices for each asset (columns)\n",
    "    start_prices = np.random.uniform(50, 200, cols)\n",
    "    prices[0, :] = start_prices\n",
    "    \n",
    "    for t in range(1, rows):\n",
    "        for asset in range(cols):\n",
    "            # Individual asset volatility (15-25% annualized)\n",
    "            sigma = np.random.uniform(0.15, 0.25)\n",
    "            \n",
    "            # Add some correlation with previous asset (except first one)\n",
    "            if asset > 0:\n",
    "                correlation_factor = 0.3 * np.random.randn()\n",
    "                correlated_shock = correlation_factor * (prices[t-1, asset-1] / prices[t-2, asset-1] - 1) if t > 1 else 0\n",
    "            else:\n",
    "                correlated_shock = 0\n",
    "            \n",
    "            # Geometric Brownian Motion with occasional jumps\n",
    "            random_shock = np.random.randn()\n",
    "            \n",
    "            # Add occasional jump (5% probability)\n",
    "            if np.random.random() < 0.05:\n",
    "                jump = np.random.uniform(-0.1, 0.15)  # Jump between -10% and +15%\n",
    "            else:\n",
    "                jump = 0\n",
    "            \n",
    "            # Price evolution: S(t+1) = S(t) * exp((mu - 0.5*sigma^2)*dt + sigma*sqrt(dt)*Z + jump)\n",
    "            log_return = (mu - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * random_shock + jump + correlated_shock\n",
    "            \n",
    "            prices[t, asset] = prices[t-1, asset] * np.exp(log_return)\n",
    "    \n",
    "    # Round to 2 decimal places for realistic price formatting\n",
    "    prices = np.round(prices, 2)\n",
    "    \n",
    "    return prices\n",
    "\n",
    "rows = 700\n",
    "col = 200\n",
    "# Generate the dataset\n",
    "financial_data = generate_financial_timeseries(rows, col)\n",
    "\n",
    "# Create DataFrame with proper labeling\n",
    "dates = [datetime(2024, 1, 1) + timedelta(days=i) for i in range(rows)]\n",
    "asset_names = [f\"ASSET_{i+1:02d}\" for i in range(col)]\n",
    "\n",
    "df = pd.DataFrame(financial_data, \n",
    "                 index=pd.to_datetime(dates),\n",
    "                 columns=asset_names)\n",
    "\n",
    "print(\"Financial Time Series Dataset\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Date Range: {df.index[0].strftime('%Y-%m-%d')} to {df.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"Price Range: ${df.values.min():.2f} - ${df.values.max():.2f}\")\n",
    "print(\"\\nDataset Preview:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72940e4-71c4-4f21-9f28-5c0f601e2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Database password:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Financial time series data saved to server!\n",
      "ðŸ“Š Saved: 700 rows Ã— 201 columns\n"
     ]
    }
   ],
   "source": [
    "# SAVE the DATASET ons the server\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import getpass\n",
    "\n",
    "password = getpass.getpass(\"Database password: \")\n",
    "engine = create_engine(f\"postgresql://postgres:{password}@localhost:5432/fintech_db\")\n",
    "# Reset index to make dates a proper column\n",
    "df_to_save = df.reset_index()\n",
    "df_to_save = df_to_save.rename(columns={'index': 'date'})\n",
    "\n",
    "# Save to PostgreSQL server\n",
    "df_to_save.to_sql('asset_prices', engine, if_exists='replace', index=False)\n",
    "engine.dispose()\n",
    "print(\"âœ… Financial time series data saved to server!\")\n",
    "print(f\"ðŸ“Š Saved: {len(df_to_save)} rows Ã— {len(df_to_save.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "727adadc-5e86-48b3-951d-26614bda6a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Database password:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Financial time series data loaded from server!\n",
      "ðŸ“Š Loaded: (700, 200)\n",
      "ðŸ“… Date range: 2024-01-01 00:00:00 to 2025-11-30 00:00:00\n",
      "\n",
      "First 5 rows:\n",
      "            ASSET_01  ASSET_02  ASSET_03  ASSET_04  ASSET_05  ASSET_06  \\\n",
      "date                                                                     \n",
      "2024-01-01     95.10     66.54     97.23    139.55     77.47    154.16   \n",
      "2024-01-02     95.97     66.90     96.84    138.36     77.08    152.61   \n",
      "2024-01-03     95.20     65.96     97.85    138.24     76.84    152.71   \n",
      "2024-01-04     96.28     65.65     97.62    140.46     77.83    153.68   \n",
      "2024-01-05     97.92     65.25     96.69    140.20     77.65    152.64   \n",
      "\n",
      "            ASSET_07  ASSET_08  ASSET_09  ASSET_10  ...  ASSET_191  ASSET_192  \\\n",
      "date                                                ...                         \n",
      "2024-01-01     92.68    111.98    144.21    187.20  ...     134.22     105.71   \n",
      "2024-01-02     92.97    111.21    140.78    181.79  ...     133.08     109.95   \n",
      "2024-01-03     92.09    111.73    140.23    180.57  ...     134.40     109.56   \n",
      "2024-01-04     90.66    110.11    137.40    180.99  ...     134.65     108.31   \n",
      "2024-01-05     87.77    110.72    134.15    186.70  ...     134.82     111.58   \n",
      "\n",
      "            ASSET_193  ASSET_194  ASSET_195  ASSET_196  ASSET_197  ASSET_198  \\\n",
      "date                                                                           \n",
      "2024-01-01     156.42      75.52     141.44     194.76      55.51     190.42   \n",
      "2024-01-02     157.41      75.10     140.66     195.82      56.74     187.70   \n",
      "2024-01-03     152.90      75.55     141.27     201.22      57.04     188.78   \n",
      "2024-01-04     152.11      74.65     142.09     201.45      57.86     190.06   \n",
      "2024-01-05     153.30      74.91     142.84     199.08      58.67     188.93   \n",
      "\n",
      "            ASSET_199  ASSET_200  \n",
      "date                              \n",
      "2024-01-01     137.44     193.72  \n",
      "2024-01-02     135.51     189.04  \n",
      "2024-01-03     135.89     192.41  \n",
      "2024-01-04     135.13     193.76  \n",
      "2024-01-05     135.94     190.89  \n",
      "\n",
      "[5 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "# LOAD the DATASET from the server\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import getpass\n",
    "\n",
    "password = getpass.getpass(\"Database password: \")\n",
    "engine = create_engine(f\"postgresql://postgres:{password}@localhost:5432/fintech_db\")\n",
    "\n",
    "# Load from PostgreSQL server\n",
    "df_loaded = pd.read_sql(\"SELECT * FROM asset_prices\", engine)\n",
    "\n",
    "# Convert date column back to datetime and set as index\n",
    "df_loaded['date'] = pd.to_datetime(df_loaded['date'])\n",
    "df_loaded = df_loaded.set_index('date')\n",
    "\n",
    "df = df_loaded # rename\n",
    "engine.dispose()\n",
    "\n",
    "print(\"âœ… Financial time series data loaded from server!\")\n",
    "print(f\"ðŸ“Š Loaded: {df.shape}\")\n",
    "print(f\"ðŸ“… Date range: {df.index[0]} to {df.index[-1]}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4a4e9-f947-4ceb-a905-57d603c8154b",
   "metadata": {},
   "source": [
    "\n",
    "'''Strategies for Handling Missing Numeric Values'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac95667a-c787-4d79-b316-5a090d57701b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INTRODUCING 1000 RANDOM MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "Missing values introduced at positions (row, col):\n",
      " 1. Position (347, 71) - ASSET_72 on 2024-12-13\n",
      " 2. Position (183, 83) - ASSET_84 on 2024-07-02\n",
      " 3. Position (652, 163) - ASSET_164 on 2025-10-14\n",
      " 4. Position (212, 87) - ASSET_88 on 2024-07-31\n",
      " 5. Position (84, 192) - ASSET_193 on 2024-03-25\n",
      " 6. Position (478, 81) - ASSET_82 on 2025-04-23\n",
      " 7. Position (354, 51) - ASSET_52 on 2024-12-20\n",
      " 8. Position (636, 138) - ASSET_139 on 2025-09-28\n",
      " 9. Position (445, 129) - ASSET_130 on 2025-03-21\n",
      "10. Position (656, 117) - ASSET_118 on 2025-10-18\n",
      "11. Position (299, 100) - ASSET_101 on 2024-10-26\n",
      "12. Position (19, 174) - ASSET_175 on 2024-01-20\n",
      "13. Position (583, 26) - ASSET_27 on 2025-08-06\n",
      "14. Position (56, 122) - ASSET_123 on 2024-02-26\n",
      "15. Position (638, 111) - ASSET_112 on 2025-09-30\n",
      "16. Position (384, 176) - ASSET_177 on 2025-01-19\n",
      "17. Position (340, 75) - ASSET_76 on 2024-12-06\n",
      "18. Position (201, 57) - ASSET_58 on 2024-07-20\n",
      "19. Position (684, 149) - ASSET_150 on 2025-11-15\n",
      "20. Position (270, 60) - ASSET_61 on 2024-09-27\n",
      "21. Position (217, 19) - ASSET_20 on 2024-08-05\n",
      "22. Position (258, 44) - ASSET_45 on 2024-09-15\n",
      "23. Position (255, 168) - ASSET_169 on 2024-09-12\n",
      "24. Position ( 0, 169) - ASSET_170 on 2024-01-01\n",
      "25. Position ( 3, 81) - ASSET_82 on 2024-01-04\n",
      "26. Position (12, 192) - ASSET_193 on 2024-01-13\n",
      "27. Position (319, 135) - ASSET_136 on 2024-11-15\n",
      "28. Position (190, 194) - ASSET_195 on 2024-07-09\n",
      "29. Position (254, 178) - ASSET_179 on 2024-09-11\n",
      "30. Position (264, 29) - ASSET_30 on 2024-09-21\n",
      "31. Position (145, 134) - ASSET_135 on 2024-05-25\n",
      "32. Position (584,  7) - ASSET_08 on 2025-08-07\n",
      "33. Position (454, 16) - ASSET_17 on 2025-03-30\n",
      "34. Position (349, 132) - ASSET_133 on 2024-12-15\n",
      "35. Position (681, 186) - ASSET_187 on 2025-11-12\n",
      "36. Position (697, 50) - ASSET_51 on 2025-11-28\n",
      "37. Position (224, 55) - ASSET_56 on 2024-08-12\n",
      "38. Position (23, 99) - ASSET_100 on 2024-01-24\n",
      "39. Position (504, 23) - ASSET_24 on 2025-05-19\n",
      "40. Position (94, 75) - ASSET_76 on 2024-04-04\n",
      "41. Position (513, 168) - ASSET_169 on 2025-05-28\n",
      "42. Position (428, 82) - ASSET_83 on 2025-03-04\n",
      "43. Position (154, 49) - ASSET_50 on 2024-06-03\n",
      "44. Position (363, 157) - ASSET_158 on 2024-12-29\n",
      "45. Position (637, 166) - ASSET_167 on 2025-09-29\n",
      "46. Position (236, 103) - ASSET_104 on 2024-08-24\n",
      "47. Position (86, 150) - ASSET_151 on 2024-03-27\n",
      "48. Position (442, 76) - ASSET_77 on 2025-03-18\n",
      "49. Position (86, 20) - ASSET_21 on 2024-03-27\n",
      "50. Position (682, 156) - ASSET_157 on 2025-11-13\n",
      "51. Position (436,  6) - ASSET_07 on 2025-03-12\n",
      "52. Position (272, 140) - ASSET_141 on 2024-09-29\n",
      "53. Position (679, 188) - ASSET_189 on 2025-11-10\n",
      "54. Position (534, 165) - ASSET_166 on 2025-06-18\n",
      "55. Position (529, 61) - ASSET_62 on 2025-06-13\n",
      "56. Position (239, 133) - ASSET_134 on 2024-08-27\n",
      "57. Position (159, 113) - ASSET_114 on 2024-06-08\n",
      "58. Position (565,  4) - ASSET_05 on 2025-07-19\n",
      "59. Position (419, 102) - ASSET_103 on 2025-02-23\n",
      "60. Position (598, 59) - ASSET_60 on 2025-08-21\n",
      "61. Position (170, 98) - ASSET_99 on 2024-06-19\n",
      "62. Position (410, 88) - ASSET_89 on 2025-02-14\n",
      "63. Position (420, 13) - ASSET_14 on 2025-02-24\n",
      "64. Position (475, 133) - ASSET_134 on 2025-04-20\n",
      "65. Position (33, 187) - ASSET_188 on 2024-02-03\n",
      "66. Position (401, 84) - ASSET_85 on 2025-02-05\n",
      "67. Position (180, 75) - ASSET_76 on 2024-06-29\n",
      "68. Position (556, 90) - ASSET_91 on 2025-07-10\n",
      "69. Position (58, 42) - ASSET_43 on 2024-02-28\n",
      "70. Position (658, 120) - ASSET_121 on 2025-10-20\n",
      "71. Position (285, 47) - ASSET_48 on 2024-10-12\n",
      "72. Position (275, 99) - ASSET_100 on 2024-10-02\n",
      "73. Position (627, 43) - ASSET_44 on 2025-09-19\n",
      "74. Position (568, 30) - ASSET_31 on 2025-07-22\n",
      "75. Position (492, 77) - ASSET_78 on 2025-05-07\n",
      "76. Position (34, 106) - ASSET_107 on 2024-02-04\n",
      "77. Position (275, 62) - ASSET_63 on 2024-10-02\n",
      "78. Position (105, 179) - ASSET_180 on 2024-04-15\n",
      "79. Position (617, 134) - ASSET_135 on 2025-09-09\n",
      "80. Position (383, 177) - ASSET_178 on 2025-01-18\n",
      "81. Position (607, 192) - ASSET_193 on 2025-08-30\n",
      "82. Position (296,  8) - ASSET_09 on 2024-10-23\n",
      "83. Position (591, 23) - ASSET_24 on 2025-08-14\n",
      "84. Position (120, 173) - ASSET_174 on 2024-04-30\n",
      "85. Position (195, 143) - ASSET_144 on 2024-07-14\n",
      "86. Position (365, 176) - ASSET_177 on 2024-12-31\n",
      "87. Position (632, 88) - ASSET_89 on 2025-09-24\n",
      "88. Position (493, 171) - ASSET_172 on 2025-05-08\n",
      "89. Position (440, 55) - ASSET_56 on 2025-03-16\n",
      "90. Position (693, 27) - ASSET_28 on 2025-11-24\n",
      "91. Position (288, 199) - ASSET_200 on 2024-10-15\n",
      "92. Position (255, 101) - ASSET_102 on 2024-09-12\n",
      "93. Position (49, 30) - ASSET_31 on 2024-02-19\n",
      "94. Position (550, 148) - ASSET_149 on 2025-07-04\n",
      "95. Position (531, 85) - ASSET_86 on 2025-06-15\n",
      "96. Position (299, 109) - ASSET_110 on 2024-10-26\n",
      "97. Position (45, 38) - ASSET_39 on 2024-02-15\n",
      "98. Position (394, 191) - ASSET_192 on 2025-01-29\n",
      "99. Position (368, 167) - ASSET_168 on 2025-01-03\n",
      "100. Position (692, 117) - ASSET_118 on 2025-11-23\n",
      "101. Position (356, 160) - ASSET_161 on 2024-12-22\n",
      "102. Position (253,  8) - ASSET_09 on 2024-09-10\n",
      "103. Position (311, 58) - ASSET_59 on 2024-11-07\n",
      "104. Position (68, 20) - ASSET_21 on 2024-03-09\n",
      "105. Position (416, 29) - ASSET_30 on 2025-02-20\n",
      "106. Position (12, 99) - ASSET_100 on 2024-01-13\n",
      "107. Position (366, 181) - ASSET_182 on 2025-01-01\n",
      "108. Position (72, 93) - ASSET_94 on 2024-03-13\n",
      "109. Position (307, 188) - ASSET_189 on 2024-11-03\n",
      "110. Position (626, 145) - ASSET_146 on 2025-09-18\n",
      "111. Position (622, 127) - ASSET_128 on 2025-09-14\n",
      "112. Position (463, 128) - ASSET_129 on 2025-04-08\n",
      "113. Position (604, 67) - ASSET_68 on 2025-08-27\n",
      "114. Position (581, 40) - ASSET_41 on 2025-08-04\n",
      "115. Position (302, 173) - ASSET_174 on 2024-10-29\n",
      "116. Position (145, 165) - ASSET_166 on 2024-05-25\n",
      "117. Position (589,  8) - ASSET_09 on 2025-08-12\n",
      "118. Position (662, 33) - ASSET_34 on 2025-10-24\n",
      "119. Position (259, 146) - ASSET_147 on 2024-09-16\n",
      "120. Position (246, 53) - ASSET_54 on 2024-09-03\n",
      "121. Position (687, 97) - ASSET_98 on 2025-11-18\n",
      "122. Position (533, 17) - ASSET_18 on 2025-06-17\n",
      "123. Position (389, 57) - ASSET_58 on 2025-01-24\n",
      "124. Position (210, 30) - ASSET_31 on 2024-07-29\n",
      "125. Position (617, 63) - ASSET_64 on 2025-09-09\n",
      "126. Position (225,  2) - ASSET_03 on 2024-08-13\n",
      "127. Position (469, 78) - ASSET_79 on 2025-04-14\n",
      "128. Position (129, 51) - ASSET_52 on 2024-05-09\n",
      "129. Position (637, 55) - ASSET_56 on 2025-09-29\n",
      "130. Position (43, 32) - ASSET_33 on 2024-02-13\n",
      "131. Position (661, 180) - ASSET_181 on 2025-10-23\n",
      "132. Position (501, 22) - ASSET_23 on 2025-05-16\n",
      "133. Position (416, 188) - ASSET_189 on 2025-02-20\n",
      "134. Position (655, 182) - ASSET_183 on 2025-10-17\n",
      "135. Position (349,  9) - ASSET_10 on 2024-12-15\n",
      "136. Position (62, 10) - ASSET_11 on 2024-03-03\n",
      "137. Position (489, 103) - ASSET_104 on 2025-05-04\n",
      "138. Position (406, 188) - ASSET_189 on 2025-02-10\n",
      "139. Position (696, 109) - ASSET_110 on 2025-11-27\n",
      "140. Position (287, 142) - ASSET_143 on 2024-10-14\n",
      "141. Position (226, 82) - ASSET_83 on 2024-08-14\n",
      "142. Position (666, 48) - ASSET_49 on 2025-10-28\n",
      "143. Position (485, 180) - ASSET_181 on 2025-04-30\n",
      "144. Position (258,  3) - ASSET_04 on 2024-09-15\n",
      "145. Position (326, 195) - ASSET_196 on 2024-11-22\n",
      "146. Position (268, 199) - ASSET_200 on 2024-09-25\n",
      "147. Position (491, 88) - ASSET_89 on 2025-05-06\n",
      "148. Position (163, 93) - ASSET_94 on 2024-06-12\n",
      "149. Position (84, 27) - ASSET_28 on 2024-03-25\n",
      "150. Position (359, 77) - ASSET_78 on 2024-12-25\n",
      "151. Position (256, 149) - ASSET_150 on 2024-09-13\n",
      "152. Position (528, 53) - ASSET_54 on 2025-06-12\n",
      "153. Position (614, 151) - ASSET_152 on 2025-09-06\n",
      "154. Position (414, 38) - ASSET_39 on 2025-02-18\n",
      "155. Position (200, 54) - ASSET_55 on 2024-07-19\n",
      "156. Position (50, 123) - ASSET_124 on 2024-02-20\n",
      "157. Position (178, 82) - ASSET_83 on 2024-06-27\n",
      "158. Position (21,  4) - ASSET_05 on 2024-01-22\n",
      "159. Position (573, 69) - ASSET_70 on 2025-07-27\n",
      "160. Position (200, 180) - ASSET_181 on 2024-07-19\n",
      "161. Position (57, 56) - ASSET_57 on 2024-02-27\n",
      "162. Position (128, 147) - ASSET_148 on 2024-05-08\n",
      "163. Position (38, 68) - ASSET_69 on 2024-02-08\n",
      "164. Position (298, 80) - ASSET_81 on 2024-10-25\n",
      "165. Position (323, 106) - ASSET_107 on 2024-11-19\n",
      "166. Position (615, 45) - ASSET_46 on 2025-09-07\n",
      "167. Position (456, 122) - ASSET_123 on 2025-04-01\n",
      "168. Position (691, 177) - ASSET_178 on 2025-11-22\n",
      "169. Position (252, 39) - ASSET_40 on 2024-09-09\n",
      "170. Position (339, 125) - ASSET_126 on 2024-12-05\n",
      "171. Position (460, 117) - ASSET_118 on 2025-04-05\n",
      "172. Position (553, 136) - ASSET_137 on 2025-07-07\n",
      "173. Position (470, 183) - ASSET_184 on 2025-04-15\n",
      "174. Position (550, 161) - ASSET_162 on 2025-07-04\n",
      "175. Position (268, 141) - ASSET_142 on 2024-09-25\n",
      "176. Position (173, 11) - ASSET_12 on 2024-06-22\n",
      "177. Position (319, 31) - ASSET_32 on 2024-11-15\n",
      "178. Position (527, 97) - ASSET_98 on 2025-06-11\n",
      "179. Position (12, 199) - ASSET_200 on 2024-01-13\n",
      "180. Position (570, 36) - ASSET_37 on 2025-07-24\n",
      "181. Position (192, 12) - ASSET_13 on 2024-07-11\n",
      "182. Position (412, 125) - ASSET_126 on 2025-02-16\n",
      "183. Position (502, 154) - ASSET_155 on 2025-05-17\n",
      "184. Position (614, 51) - ASSET_52 on 2025-09-06\n",
      "185. Position (565, 176) - ASSET_177 on 2025-07-19\n",
      "186. Position (621, 56) - ASSET_57 on 2025-09-13\n",
      "187. Position (57, 180) - ASSET_181 on 2024-02-27\n",
      "188. Position (431, 117) - ASSET_118 on 2025-03-07\n",
      "189. Position (17, 76) - ASSET_77 on 2024-01-18\n",
      "190. Position (641, 160) - ASSET_161 on 2025-10-03\n",
      "191. Position (94, 84) - ASSET_85 on 2024-04-04\n",
      "192. Position (490, 106) - ASSET_107 on 2025-05-05\n",
      "193. Position (73, 16) - ASSET_17 on 2024-03-14\n",
      "194. Position (618, 193) - ASSET_194 on 2025-09-10\n",
      "195. Position (128, 81) - ASSET_82 on 2024-05-08\n",
      "196. Position (176, 180) - ASSET_181 on 2024-06-25\n",
      "197. Position (594, 103) - ASSET_104 on 2025-08-17\n",
      "198. Position (633, 150) - ASSET_151 on 2025-09-25\n",
      "199. Position (655, 126) - ASSET_127 on 2025-10-17\n",
      "200. Position (197, 56) - ASSET_57 on 2024-07-16\n",
      "201. Position (483, 197) - ASSET_198 on 2025-04-28\n",
      "202. Position (70, 148) - ASSET_149 on 2024-03-11\n",
      "203. Position (319, 120) - ASSET_121 on 2024-11-15\n",
      "204. Position (542, 129) - ASSET_130 on 2025-06-26\n",
      "205. Position (476, 143) - ASSET_144 on 2025-04-21\n",
      "206. Position (226, 195) - ASSET_196 on 2024-08-14\n",
      "207. Position (556, 176) - ASSET_177 on 2025-07-10\n",
      "208. Position (431, 55) - ASSET_56 on 2025-03-07\n",
      "209. Position (281, 36) - ASSET_37 on 2024-10-08\n",
      "210. Position (643, 77) - ASSET_78 on 2025-10-05\n",
      "211. Position (216,  6) - ASSET_07 on 2024-08-04\n",
      "212. Position (270, 97) - ASSET_98 on 2024-09-27\n",
      "213. Position (173, 40) - ASSET_41 on 2024-06-22\n",
      "214. Position (71, 101) - ASSET_102 on 2024-03-12\n",
      "215. Position (238, 87) - ASSET_88 on 2024-08-26\n",
      "216. Position (119, 44) - ASSET_45 on 2024-04-29\n",
      "217. Position (403, 90) - ASSET_91 on 2025-02-07\n",
      "218. Position (102, 192) - ASSET_193 on 2024-04-12\n",
      "219. Position (203, 48) - ASSET_49 on 2024-07-22\n",
      "220. Position (462, 153) - ASSET_154 on 2025-04-07\n",
      "221. Position (179, 92) - ASSET_93 on 2024-06-28\n",
      "222. Position (315, 79) - ASSET_80 on 2024-11-11\n",
      "223. Position (335, 157) - ASSET_158 on 2024-12-01\n",
      "224. Position (195, 14) - ASSET_15 on 2024-07-14\n",
      "225. Position (63, 108) - ASSET_109 on 2024-03-04\n",
      "226. Position (530, 192) - ASSET_193 on 2025-06-14\n",
      "227. Position (125, 11) - ASSET_12 on 2024-05-05\n",
      "228. Position (229, 38) - ASSET_39 on 2024-08-17\n",
      "229. Position (481, 115) - ASSET_116 on 2025-04-26\n",
      "230. Position (198, 197) - ASSET_198 on 2024-07-17\n",
      "231. Position (27, 100) - ASSET_101 on 2024-01-28\n",
      "232. Position (135,  8) - ASSET_09 on 2024-05-15\n",
      "233. Position (499, 186) - ASSET_187 on 2025-05-14\n",
      "234. Position (112, 85) - ASSET_86 on 2024-04-22\n",
      "235. Position (656, 145) - ASSET_146 on 2025-10-18\n",
      "236. Position (225, 76) - ASSET_77 on 2024-08-13\n",
      "237. Position (244, 93) - ASSET_94 on 2024-09-01\n",
      "238. Position (109, 140) - ASSET_141 on 2024-04-19\n",
      "239. Position (124, 153) - ASSET_154 on 2024-05-04\n",
      "240. Position (601, 32) - ASSET_33 on 2025-08-24\n",
      "241. Position (695, 166) - ASSET_167 on 2025-11-26\n",
      "242. Position (326,  0) - ASSET_01 on 2024-11-22\n",
      "243. Position (549,  9) - ASSET_10 on 2025-07-03\n",
      "244. Position (619, 90) - ASSET_91 on 2025-09-11\n",
      "245. Position (51, 53) - ASSET_54 on 2024-02-21\n",
      "246. Position (658, 163) - ASSET_164 on 2025-10-20\n",
      "247. Position (269, 33) - ASSET_34 on 2024-09-26\n",
      "248. Position (601, 105) - ASSET_106 on 2025-08-24\n",
      "249. Position (693, 63) - ASSET_64 on 2025-11-24\n",
      "250. Position (57, 128) - ASSET_129 on 2024-02-27\n",
      "251. Position (575, 194) - ASSET_195 on 2025-07-29\n",
      "252. Position (417, 150) - ASSET_151 on 2025-02-21\n",
      "253. Position (178, 41) - ASSET_42 on 2024-06-27\n",
      "254. Position (142, 132) - ASSET_133 on 2024-05-22\n",
      "255. Position (523, 113) - ASSET_114 on 2025-06-07\n",
      "256. Position (264, 150) - ASSET_151 on 2024-09-21\n",
      "257. Position (87, 67) - ASSET_68 on 2024-03-28\n",
      "258. Position (165, 185) - ASSET_186 on 2024-06-14\n",
      "259. Position (167, 33) - ASSET_34 on 2024-06-16\n",
      "260. Position (21, 41) - ASSET_42 on 2024-01-22\n",
      "261. Position (605, 35) - ASSET_36 on 2025-08-28\n",
      "262. Position (117, 141) - ASSET_142 on 2024-04-27\n",
      "263. Position (658,  1) - ASSET_02 on 2025-10-20\n",
      "264. Position (497, 140) - ASSET_141 on 2025-05-12\n",
      "265. Position (497, 171) - ASSET_172 on 2025-05-12\n",
      "266. Position (346, 187) - ASSET_188 on 2024-12-12\n",
      "267. Position (234, 187) - ASSET_188 on 2024-08-22\n",
      "268. Position (624, 81) - ASSET_82 on 2025-09-16\n",
      "269. Position (482, 126) - ASSET_127 on 2025-04-27\n",
      "270. Position (274, 35) - ASSET_36 on 2024-10-01\n",
      "271. Position (410, 176) - ASSET_177 on 2025-02-14\n",
      "272. Position (186, 18) - ASSET_19 on 2024-07-05\n",
      "273. Position (186, 49) - ASSET_50 on 2024-07-05\n",
      "274. Position (649, 85) - ASSET_86 on 2025-10-11\n",
      "275. Position (233, 162) - ASSET_163 on 2024-08-21\n",
      "276. Position (158, 52) - ASSET_53 on 2024-06-07\n",
      "277. Position (351, 135) - ASSET_136 on 2024-12-17\n",
      "278. Position (328, 123) - ASSET_124 on 2024-11-24\n",
      "279. Position (566, 175) - ASSET_176 on 2025-07-20\n",
      "280. Position (189,  6) - ASSET_07 on 2024-07-08\n",
      "281. Position (49, 97) - ASSET_98 on 2024-02-19\n",
      "282. Position (475,  1) - ASSET_02 on 2025-04-20\n",
      "283. Position (675, 97) - ASSET_98 on 2025-11-06\n",
      "284. Position (432, 86) - ASSET_87 on 2025-03-08\n",
      "285. Position (511, 158) - ASSET_159 on 2025-05-26\n",
      "286. Position (650, 178) - ASSET_179 on 2025-10-12\n",
      "287. Position (558, 113) - ASSET_114 on 2025-07-12\n",
      "288. Position (601, 124) - ASSET_125 on 2025-08-24\n",
      "289. Position (135, 167) - ASSET_168 on 2024-05-15\n",
      "290. Position (268, 22) - ASSET_23 on 2024-09-25\n",
      "291. Position (328, 122) - ASSET_123 on 2024-11-24\n",
      "292. Position (683, 145) - ASSET_146 on 2025-11-14\n",
      "293. Position (510, 76) - ASSET_77 on 2025-05-25\n",
      "294. Position (564, 87) - ASSET_88 on 2025-07-18\n",
      "295. Position (160,  3) - ASSET_04 on 2024-06-09\n",
      "296. Position (631, 101) - ASSET_102 on 2025-09-23\n",
      "297. Position (607, 159) - ASSET_160 on 2025-08-30\n",
      "298. Position (157, 185) - ASSET_186 on 2024-06-06\n",
      "299. Position (366, 58) - ASSET_59 on 2025-01-01\n",
      "300. Position (155, 47) - ASSET_48 on 2024-06-04\n",
      "301. Position (91, 47) - ASSET_48 on 2024-04-01\n",
      "302. Position (550, 68) - ASSET_69 on 2025-07-04\n",
      "303. Position (675, 21) - ASSET_22 on 2025-11-06\n",
      "304. Position (279, 107) - ASSET_108 on 2024-10-06\n",
      "305. Position (378, 17) - ASSET_18 on 2025-01-13\n",
      "306. Position (161, 55) - ASSET_56 on 2024-06-10\n",
      "307. Position (203, 186) - ASSET_187 on 2024-07-22\n",
      "308. Position (352, 57) - ASSET_58 on 2024-12-18\n",
      "309. Position (553, 122) - ASSET_123 on 2025-07-07\n",
      "310. Position (374, 13) - ASSET_14 on 2025-01-09\n",
      "311. Position (409, 106) - ASSET_107 on 2025-02-13\n",
      "312. Position (423, 180) - ASSET_181 on 2025-02-27\n",
      "313. Position (167, 144) - ASSET_145 on 2024-06-16\n",
      "314. Position (398, 105) - ASSET_106 on 2025-02-02\n",
      "315. Position (525, 191) - ASSET_192 on 2025-06-09\n",
      "316. Position (48, 121) - ASSET_122 on 2024-02-18\n",
      "317. Position (40, 117) - ASSET_118 on 2024-02-10\n",
      "318. Position (681, 84) - ASSET_85 on 2025-11-12\n",
      "319. Position ( 6, 118) - ASSET_119 on 2024-01-07\n",
      "320. Position (697, 128) - ASSET_129 on 2025-11-28\n",
      "321. Position (149, 199) - ASSET_200 on 2024-05-29\n",
      "322. Position (291, 173) - ASSET_174 on 2024-10-18\n",
      "323. Position (484, 49) - ASSET_50 on 2025-04-29\n",
      "324. Position (385, 191) - ASSET_192 on 2025-01-20\n",
      "325. Position (579, 25) - ASSET_26 on 2025-08-02\n",
      "326. Position (207, 172) - ASSET_173 on 2024-07-26\n",
      "327. Position (215, 143) - ASSET_144 on 2024-08-03\n",
      "328. Position (373, 117) - ASSET_118 on 2025-01-08\n",
      "329. Position (356, 187) - ASSET_188 on 2024-12-22\n",
      "330. Position (381, 157) - ASSET_158 on 2025-01-16\n",
      "331. Position (243, 128) - ASSET_129 on 2024-08-31\n",
      "332. Position (147, 109) - ASSET_110 on 2024-05-27\n",
      "333. Position (271, 60) - ASSET_61 on 2024-09-28\n",
      "334. Position (275, 124) - ASSET_125 on 2024-10-02\n",
      "335. Position (515, 132) - ASSET_133 on 2025-05-30\n",
      "336. Position (506,  0) - ASSET_01 on 2025-05-21\n",
      "337. Position (253, 174) - ASSET_175 on 2024-09-10\n",
      "338. Position (222, 91) - ASSET_92 on 2024-08-10\n",
      "339. Position (580, 44) - ASSET_45 on 2025-08-03\n",
      "340. Position (277, 37) - ASSET_38 on 2024-10-04\n",
      "341. Position (67, 65) - ASSET_66 on 2024-03-08\n",
      "342. Position (633, 10) - ASSET_11 on 2025-09-25\n",
      "343. Position (186, 72) - ASSET_73 on 2024-07-05\n",
      "344. Position (131, 133) - ASSET_134 on 2024-05-11\n",
      "345. Position (112, 30) - ASSET_31 on 2024-04-22\n",
      "346. Position (626, 95) - ASSET_96 on 2025-09-18\n",
      "347. Position (210, 121) - ASSET_122 on 2024-07-29\n",
      "348. Position (100,  7) - ASSET_08 on 2024-04-10\n",
      "349. Position (108, 44) - ASSET_45 on 2024-04-18\n",
      "350. Position (432, 26) - ASSET_27 on 2025-03-08\n",
      "351. Position (466, 163) - ASSET_164 on 2025-04-11\n",
      "352. Position (255, 84) - ASSET_85 on 2024-09-12\n",
      "353. Position (645, 87) - ASSET_88 on 2025-10-07\n",
      "354. Position (181, 90) - ASSET_91 on 2024-06-30\n",
      "355. Position (350, 89) - ASSET_90 on 2024-12-16\n",
      "356. Position (43, 124) - ASSET_125 on 2024-02-13\n",
      "357. Position (430, 186) - ASSET_187 on 2025-03-06\n",
      "358. Position (673, 38) - ASSET_39 on 2025-11-04\n",
      "359. Position (426, 170) - ASSET_171 on 2025-03-02\n",
      "360. Position (144, 66) - ASSET_67 on 2024-05-24\n",
      "361. Position (474, 17) - ASSET_18 on 2025-04-19\n",
      "362. Position (225, 42) - ASSET_43 on 2024-08-13\n",
      "363. Position (165, 46) - ASSET_47 on 2024-06-14\n",
      "364. Position (655,  4) - ASSET_05 on 2025-10-17\n",
      "365. Position (482, 181) - ASSET_182 on 2025-04-27\n",
      "366. Position (692, 27) - ASSET_28 on 2025-11-23\n",
      "367. Position (469, 126) - ASSET_127 on 2025-04-14\n",
      "368. Position (565, 162) - ASSET_163 on 2025-07-19\n",
      "369. Position (492, 72) - ASSET_73 on 2025-05-07\n",
      "370. Position (202, 145) - ASSET_146 on 2024-07-21\n",
      "371. Position (66, 159) - ASSET_160 on 2024-03-07\n",
      "372. Position (192, 38) - ASSET_39 on 2024-07-11\n",
      "373. Position (654, 65) - ASSET_66 on 2025-10-16\n",
      "374. Position (351, 129) - ASSET_130 on 2024-12-17\n",
      "375. Position (236, 94) - ASSET_95 on 2024-08-24\n",
      "376. Position (690, 66) - ASSET_67 on 2025-11-21\n",
      "377. Position (653, 90) - ASSET_91 on 2025-10-15\n",
      "378. Position (480,  8) - ASSET_09 on 2025-04-25\n",
      "379. Position (262, 186) - ASSET_187 on 2024-09-19\n",
      "380. Position (337, 185) - ASSET_186 on 2024-12-03\n",
      "381. Position (492, 116) - ASSET_117 on 2025-05-07\n",
      "382. Position (168, 42) - ASSET_43 on 2024-06-17\n",
      "383. Position (648, 91) - ASSET_92 on 2025-10-10\n",
      "384. Position (393, 118) - ASSET_119 on 2025-01-28\n",
      "385. Position (653, 156) - ASSET_157 on 2025-10-15\n",
      "386. Position (564, 68) - ASSET_69 on 2025-07-18\n",
      "387. Position (99, 28) - ASSET_29 on 2024-04-09\n",
      "388. Position (364, 113) - ASSET_114 on 2024-12-30\n",
      "389. Position (596, 49) - ASSET_50 on 2025-08-19\n",
      "390. Position (373, 162) - ASSET_163 on 2025-01-08\n",
      "391. Position (649, 84) - ASSET_85 on 2025-10-11\n",
      "392. Position (249, 56) - ASSET_57 on 2024-09-06\n",
      "393. Position (381, 182) - ASSET_183 on 2025-01-16\n",
      "394. Position (166, 188) - ASSET_189 on 2024-06-15\n",
      "395. Position (62, 181) - ASSET_182 on 2024-03-03\n",
      "396. Position (372, 55) - ASSET_56 on 2025-01-07\n",
      "397. Position (694, 31) - ASSET_32 on 2025-11-25\n",
      "398. Position (482, 169) - ASSET_170 on 2025-04-27\n",
      "399. Position (110, 41) - ASSET_42 on 2024-04-20\n",
      "400. Position (199, 188) - ASSET_189 on 2024-07-18\n",
      "401. Position (504,  1) - ASSET_02 on 2025-05-19\n",
      "402. Position (465, 151) - ASSET_152 on 2025-04-10\n",
      "403. Position (625, 61) - ASSET_62 on 2025-09-17\n",
      "404. Position (455, 199) - ASSET_200 on 2025-03-31\n",
      "405. Position (79, 20) - ASSET_21 on 2024-03-20\n",
      "406. Position (136, 110) - ASSET_111 on 2024-05-16\n",
      "407. Position (545, 55) - ASSET_56 on 2025-06-29\n",
      "408. Position (62, 147) - ASSET_148 on 2024-03-03\n",
      "409. Position (627, 131) - ASSET_132 on 2025-09-19\n",
      "410. Position (642, 115) - ASSET_116 on 2025-10-04\n",
      "411. Position (116, 29) - ASSET_30 on 2024-04-26\n",
      "412. Position (517,  5) - ASSET_06 on 2025-06-01\n",
      "413. Position (698, 143) - ASSET_144 on 2025-11-29\n",
      "414. Position (525, 154) - ASSET_155 on 2025-06-09\n",
      "415. Position (526, 45) - ASSET_46 on 2025-06-10\n",
      "416. Position (359, 140) - ASSET_141 on 2024-12-25\n",
      "417. Position (317, 161) - ASSET_162 on 2024-11-13\n",
      "418. Position (138, 148) - ASSET_149 on 2024-05-18\n",
      "419. Position (582, 180) - ASSET_181 on 2025-08-05\n",
      "420. Position (299, 182) - ASSET_183 on 2024-10-26\n",
      "421. Position (82, 135) - ASSET_136 on 2024-03-23\n",
      "422. Position (258, 128) - ASSET_129 on 2024-09-15\n",
      "423. Position (18, 14) - ASSET_15 on 2024-01-19\n",
      "424. Position (412, 196) - ASSET_197 on 2025-02-16\n",
      "425. Position (261, 31) - ASSET_32 on 2024-09-18\n",
      "426. Position (485, 49) - ASSET_50 on 2025-04-30\n",
      "427. Position ( 2, 37) - ASSET_38 on 2024-01-03\n",
      "428. Position (169, 119) - ASSET_120 on 2024-06-18\n",
      "429. Position (616, 172) - ASSET_173 on 2025-09-08\n",
      "430. Position (545, 92) - ASSET_93 on 2025-06-29\n",
      "431. Position (656, 80) - ASSET_81 on 2025-10-18\n",
      "432. Position (527,  9) - ASSET_10 on 2025-06-11\n",
      "433. Position (92, 42) - ASSET_43 on 2024-04-02\n",
      "434. Position (290, 47) - ASSET_48 on 2024-10-17\n",
      "435. Position (512, 184) - ASSET_185 on 2025-05-27\n",
      "436. Position (107, 39) - ASSET_40 on 2024-04-17\n",
      "437. Position (414, 51) - ASSET_52 on 2025-02-18\n",
      "438. Position (261, 30) - ASSET_31 on 2024-09-18\n",
      "439. Position (567, 112) - ASSET_113 on 2025-07-21\n",
      "440. Position (679, 119) - ASSET_120 on 2025-11-10\n",
      "441. Position (653, 54) - ASSET_55 on 2025-10-15\n",
      "442. Position (628, 41) - ASSET_42 on 2025-09-20\n",
      "443. Position (337, 149) - ASSET_150 on 2024-12-03\n",
      "444. Position (283, 50) - ASSET_51 on 2024-10-10\n",
      "445. Position (661, 168) - ASSET_169 on 2025-10-23\n",
      "446. Position (44,  0) - ASSET_01 on 2024-02-14\n",
      "447. Position (224, 93) - ASSET_94 on 2024-08-12\n",
      "448. Position (424, 31) - ASSET_32 on 2025-02-28\n",
      "449. Position (688, 39) - ASSET_40 on 2025-11-19\n",
      "450. Position (376, 154) - ASSET_155 on 2025-01-11\n",
      "451. Position (652, 55) - ASSET_56 on 2025-10-14\n",
      "452. Position (208, 99) - ASSET_100 on 2024-07-27\n",
      "453. Position (669, 17) - ASSET_18 on 2025-10-31\n",
      "454. Position (145, 139) - ASSET_140 on 2024-05-25\n",
      "455. Position (347, 104) - ASSET_105 on 2024-12-13\n",
      "456. Position (483, 152) - ASSET_153 on 2025-04-28\n",
      "457. Position (439, 159) - ASSET_160 on 2025-03-15\n",
      "458. Position (172, 121) - ASSET_122 on 2024-06-21\n",
      "459. Position (609, 80) - ASSET_81 on 2025-09-01\n",
      "460. Position (696,  7) - ASSET_08 on 2025-11-27\n",
      "461. Position (355, 19) - ASSET_20 on 2024-12-21\n",
      "462. Position (305, 140) - ASSET_141 on 2024-11-01\n",
      "463. Position (307, 92) - ASSET_93 on 2024-11-03\n",
      "464. Position (640, 119) - ASSET_120 on 2025-10-02\n",
      "465. Position (328, 169) - ASSET_170 on 2024-11-24\n",
      "466. Position (178, 135) - ASSET_136 on 2024-06-27\n",
      "467. Position (26, 30) - ASSET_31 on 2024-01-27\n",
      "468. Position ( 4, 189) - ASSET_190 on 2024-01-05\n",
      "469. Position (552,  5) - ASSET_06 on 2025-07-06\n",
      "470. Position (471, 48) - ASSET_49 on 2025-04-16\n",
      "471. Position (51, 194) - ASSET_195 on 2024-02-21\n",
      "472. Position ( 4, 86) - ASSET_87 on 2024-01-05\n",
      "473. Position (82, 120) - ASSET_121 on 2024-03-23\n",
      "474. Position (395, 179) - ASSET_180 on 2025-01-30\n",
      "475. Position (279,  4) - ASSET_05 on 2024-10-06\n",
      "476. Position (242,  4) - ASSET_05 on 2024-08-30\n",
      "477. Position (367, 124) - ASSET_125 on 2025-01-02\n",
      "478. Position (471, 103) - ASSET_104 on 2025-04-16\n",
      "479. Position (410, 149) - ASSET_150 on 2025-02-14\n",
      "480. Position (54, 94) - ASSET_95 on 2024-02-24\n",
      "481. Position (589, 41) - ASSET_42 on 2025-08-12\n",
      "482. Position (280, 160) - ASSET_161 on 2024-10-07\n",
      "483. Position (622, 84) - ASSET_85 on 2025-09-14\n",
      "484. Position (601, 148) - ASSET_149 on 2025-08-24\n",
      "485. Position (399, 63) - ASSET_64 on 2025-02-03\n",
      "486. Position (583, 191) - ASSET_192 on 2025-08-06\n",
      "487. Position (303, 155) - ASSET_156 on 2024-10-30\n",
      "488. Position (566, 184) - ASSET_185 on 2025-07-20\n",
      "489. Position (481, 187) - ASSET_188 on 2025-04-26\n",
      "490. Position (228, 178) - ASSET_179 on 2024-08-16\n",
      "491. Position (582, 76) - ASSET_77 on 2025-08-05\n",
      "492. Position (502, 160) - ASSET_161 on 2025-05-17\n",
      "493. Position (567, 149) - ASSET_150 on 2025-07-21\n",
      "494. Position (337, 87) - ASSET_88 on 2024-12-03\n",
      "495. Position (693, 194) - ASSET_195 on 2025-11-24\n",
      "496. Position (354, 191) - ASSET_192 on 2024-12-20\n",
      "497. Position (209, 148) - ASSET_149 on 2024-07-28\n",
      "498. Position (530, 155) - ASSET_156 on 2025-06-14\n",
      "499. Position (154, 190) - ASSET_191 on 2024-06-03\n",
      "500. Position (23, 55) - ASSET_56 on 2024-01-24\n",
      "501. Position (346, 164) - ASSET_165 on 2024-12-12\n",
      "502. Position (364, 167) - ASSET_168 on 2024-12-30\n",
      "503. Position (120, 24) - ASSET_25 on 2024-04-30\n",
      "504. Position (684, 95) - ASSET_96 on 2025-11-15\n",
      "505. Position (155, 64) - ASSET_65 on 2024-06-04\n",
      "506. Position (203, 156) - ASSET_157 on 2024-07-22\n",
      "507. Position (339, 16) - ASSET_17 on 2024-12-05\n",
      "508. Position (476, 125) - ASSET_126 on 2025-04-21\n",
      "509. Position (523, 23) - ASSET_24 on 2025-06-07\n",
      "510. Position (256, 142) - ASSET_143 on 2024-09-13\n",
      "511. Position (133, 56) - ASSET_57 on 2024-05-13\n",
      "512. Position (81, 103) - ASSET_104 on 2024-03-22\n",
      "513. Position (463, 174) - ASSET_175 on 2025-04-08\n",
      "514. Position (310, 140) - ASSET_141 on 2024-11-06\n",
      "515. Position (162, 120) - ASSET_121 on 2024-06-11\n",
      "516. Position (434, 54) - ASSET_55 on 2025-03-10\n",
      "517. Position (95, 29) - ASSET_30 on 2024-04-05\n",
      "518. Position (96, 58) - ASSET_59 on 2024-04-06\n",
      "519. Position (169, 132) - ASSET_133 on 2024-06-18\n",
      "520. Position (285, 51) - ASSET_52 on 2024-10-12\n",
      "521. Position (474, 159) - ASSET_160 on 2025-04-19\n",
      "522. Position (431, 185) - ASSET_186 on 2025-03-07\n",
      "523. Position (313, 134) - ASSET_135 on 2024-11-09\n",
      "524. Position (500, 41) - ASSET_42 on 2025-05-15\n",
      "525. Position (238, 160) - ASSET_161 on 2024-08-26\n",
      "526. Position (258, 140) - ASSET_141 on 2024-09-15\n",
      "527. Position (50, 199) - ASSET_200 on 2024-02-20\n",
      "528. Position (28, 142) - ASSET_143 on 2024-01-29\n",
      "529. Position (80, 132) - ASSET_133 on 2024-03-21\n",
      "530. Position (479, 187) - ASSET_188 on 2025-04-24\n",
      "531. Position (463, 78) - ASSET_79 on 2025-04-08\n",
      "532. Position (118, 96) - ASSET_97 on 2024-04-28\n",
      "533. Position (245, 93) - ASSET_94 on 2024-09-02\n",
      "534. Position (131, 95) - ASSET_96 on 2024-05-11\n",
      "535. Position (26, 88) - ASSET_89 on 2024-01-27\n",
      "536. Position (559, 179) - ASSET_180 on 2025-07-13\n",
      "537. Position (386, 116) - ASSET_117 on 2025-01-21\n",
      "538. Position (163, 136) - ASSET_137 on 2024-06-12\n",
      "539. Position (292, 151) - ASSET_152 on 2024-10-19\n",
      "540. Position (441,  4) - ASSET_05 on 2025-03-17\n",
      "541. Position (161, 56) - ASSET_57 on 2024-06-10\n",
      "542. Position (398, 195) - ASSET_196 on 2025-02-02\n",
      "543. Position (468, 114) - ASSET_115 on 2025-04-13\n",
      "544. Position (273, 41) - ASSET_42 on 2024-09-30\n",
      "545. Position (173, 116) - ASSET_117 on 2024-06-22\n",
      "546. Position (174, 187) - ASSET_188 on 2024-06-23\n",
      "547. Position (682, 14) - ASSET_15 on 2025-11-13\n",
      "548. Position (644, 64) - ASSET_65 on 2025-10-06\n",
      "549. Position (585, 192) - ASSET_193 on 2025-08-08\n",
      "550. Position (57, 55) - ASSET_56 on 2024-02-27\n",
      "551. Position (498, 130) - ASSET_131 on 2025-05-13\n",
      "552. Position (481, 152) - ASSET_153 on 2025-04-26\n",
      "553. Position (355, 42) - ASSET_43 on 2024-12-21\n",
      "554. Position (171, 133) - ASSET_134 on 2024-06-20\n",
      "555. Position (656,  5) - ASSET_06 on 2025-10-18\n",
      "556. Position (312, 142) - ASSET_143 on 2024-11-08\n",
      "557. Position (74, 12) - ASSET_13 on 2024-03-15\n",
      "558. Position (355, 140) - ASSET_141 on 2024-12-21\n",
      "559. Position (79, 53) - ASSET_54 on 2024-03-20\n",
      "560. Position (136, 35) - ASSET_36 on 2024-05-16\n",
      "561. Position (602, 77) - ASSET_78 on 2025-08-25\n",
      "562. Position (456, 173) - ASSET_174 on 2025-04-01\n",
      "563. Position (389,  3) - ASSET_04 on 2025-01-24\n",
      "564. Position (73, 69) - ASSET_70 on 2024-03-14\n",
      "565. Position (98,  8) - ASSET_09 on 2024-04-08\n",
      "566. Position (367, 132) - ASSET_133 on 2025-01-02\n",
      "567. Position (669, 142) - ASSET_143 on 2025-10-31\n",
      "568. Position (384, 110) - ASSET_111 on 2025-01-19\n",
      "569. Position (360,  4) - ASSET_05 on 2024-12-26\n",
      "570. Position (66, 47) - ASSET_48 on 2024-03-07\n",
      "571. Position (230, 71) - ASSET_72 on 2024-08-18\n",
      "572. Position (697, 180) - ASSET_181 on 2025-11-28\n",
      "573. Position (535, 158) - ASSET_159 on 2025-06-19\n",
      "574. Position (492, 39) - ASSET_40 on 2025-05-07\n",
      "575. Position (488, 80) - ASSET_81 on 2025-05-03\n",
      "576. Position (358, 102) - ASSET_103 on 2024-12-24\n",
      "577. Position (138, 70) - ASSET_71 on 2024-05-18\n",
      "578. Position (548, 63) - ASSET_64 on 2025-07-02\n",
      "579. Position (340, 192) - ASSET_193 on 2024-12-06\n",
      "580. Position (40, 169) - ASSET_170 on 2024-02-10\n",
      "581. Position (474, 183) - ASSET_184 on 2025-04-19\n",
      "582. Position (206, 95) - ASSET_96 on 2024-07-25\n",
      "583. Position (594, 49) - ASSET_50 on 2025-08-17\n",
      "584. Position (542, 134) - ASSET_135 on 2025-06-26\n",
      "585. Position (184, 153) - ASSET_154 on 2024-07-03\n",
      "586. Position (276, 148) - ASSET_149 on 2024-10-03\n",
      "587. Position ( 4,  6) - ASSET_07 on 2024-01-05\n",
      "588. Position (135, 36) - ASSET_37 on 2024-05-15\n",
      "589. Position (629, 141) - ASSET_142 on 2025-09-21\n",
      "590. Position (57, 38) - ASSET_39 on 2024-02-27\n",
      "591. Position (303, 10) - ASSET_11 on 2024-10-30\n",
      "592. Position (628, 151) - ASSET_152 on 2025-09-20\n",
      "593. Position (161, 86) - ASSET_87 on 2024-06-10\n",
      "594. Position (695, 170) - ASSET_171 on 2025-11-26\n",
      "595. Position (400, 35) - ASSET_36 on 2025-02-04\n",
      "596. Position (153, 33) - ASSET_34 on 2024-06-02\n",
      "597. Position (321, 35) - ASSET_36 on 2024-11-17\n",
      "598. Position (515, 188) - ASSET_189 on 2025-05-30\n",
      "599. Position (227, 191) - ASSET_192 on 2024-08-15\n",
      "600. Position (10, 192) - ASSET_193 on 2024-01-11\n",
      "601. Position (438, 85) - ASSET_86 on 2025-03-14\n",
      "602. Position (111, 34) - ASSET_35 on 2024-04-21\n",
      "603. Position (572, 104) - ASSET_105 on 2025-07-26\n",
      "604. Position (675, 59) - ASSET_60 on 2025-11-06\n",
      "605. Position (340, 189) - ASSET_190 on 2024-12-06\n",
      "606. Position (447, 32) - ASSET_33 on 2025-03-23\n",
      "607. Position (564, 53) - ASSET_54 on 2025-07-18\n",
      "608. Position (380, 52) - ASSET_53 on 2025-01-15\n",
      "609. Position (396, 98) - ASSET_99 on 2025-01-31\n",
      "610. Position (504, 17) - ASSET_18 on 2025-05-19\n",
      "611. Position ( 7,  1) - ASSET_02 on 2024-01-08\n",
      "612. Position (453, 124) - ASSET_125 on 2025-03-29\n",
      "613. Position (345, 133) - ASSET_134 on 2024-12-11\n",
      "614. Position (157, 91) - ASSET_92 on 2024-06-06\n",
      "615. Position (166, 75) - ASSET_76 on 2024-06-15\n",
      "616. Position (76, 143) - ASSET_144 on 2024-03-17\n",
      "617. Position (487, 168) - ASSET_169 on 2025-05-02\n",
      "618. Position (497, 76) - ASSET_77 on 2025-05-12\n",
      "619. Position (368, 72) - ASSET_73 on 2025-01-03\n",
      "620. Position (579, 165) - ASSET_166 on 2025-08-02\n",
      "621. Position (593, 183) - ASSET_184 on 2025-08-16\n",
      "622. Position (266, 32) - ASSET_33 on 2024-09-23\n",
      "623. Position (503, 129) - ASSET_130 on 2025-05-18\n",
      "624. Position (571, 27) - ASSET_28 on 2025-07-25\n",
      "625. Position (70, 25) - ASSET_26 on 2024-03-11\n",
      "626. Position (133, 110) - ASSET_111 on 2024-05-13\n",
      "627. Position (545, 187) - ASSET_188 on 2025-06-29\n",
      "628. Position (456,  7) - ASSET_08 on 2025-04-01\n",
      "629. Position (289, 134) - ASSET_135 on 2024-10-16\n",
      "630. Position (304, 125) - ASSET_126 on 2024-10-31\n",
      "631. Position (222, 122) - ASSET_123 on 2024-08-10\n",
      "632. Position (250, 104) - ASSET_105 on 2024-09-07\n",
      "633. Position (444, 186) - ASSET_187 on 2025-03-20\n",
      "634. Position (446, 59) - ASSET_60 on 2025-03-22\n",
      "635. Position (377, 43) - ASSET_44 on 2025-01-12\n",
      "636. Position (220, 20) - ASSET_21 on 2024-08-08\n",
      "637. Position (520, 33) - ASSET_34 on 2025-06-04\n",
      "638. Position (532, 133) - ASSET_134 on 2025-06-16\n",
      "639. Position (645, 19) - ASSET_20 on 2025-10-07\n",
      "640. Position (143, 113) - ASSET_114 on 2024-05-23\n",
      "641. Position (609, 166) - ASSET_167 on 2025-09-01\n",
      "642. Position (327,  6) - ASSET_07 on 2024-11-23\n",
      "643. Position (343, 129) - ASSET_130 on 2024-12-09\n",
      "644. Position (298, 195) - ASSET_196 on 2024-10-25\n",
      "645. Position (675, 101) - ASSET_102 on 2025-11-06\n",
      "646. Position (473, 104) - ASSET_105 on 2025-04-18\n",
      "647. Position (411, 162) - ASSET_163 on 2025-02-15\n",
      "648. Position (388, 144) - ASSET_145 on 2025-01-23\n",
      "649. Position (666, 116) - ASSET_117 on 2025-10-28\n",
      "650. Position (620, 80) - ASSET_81 on 2025-09-12\n",
      "651. Position (195, 191) - ASSET_192 on 2024-07-14\n",
      "652. Position (316, 18) - ASSET_19 on 2024-11-12\n",
      "653. Position (226, 78) - ASSET_79 on 2024-08-14\n",
      "654. Position (84, 77) - ASSET_78 on 2024-03-25\n",
      "655. Position (547, 191) - ASSET_192 on 2025-07-01\n",
      "656. Position (43, 76) - ASSET_77 on 2024-02-13\n",
      "657. Position (248,  5) - ASSET_06 on 2024-09-05\n",
      "658. Position (267, 43) - ASSET_44 on 2024-09-24\n",
      "659. Position (281, 157) - ASSET_158 on 2024-10-08\n",
      "660. Position (125, 164) - ASSET_165 on 2024-05-05\n",
      "661. Position (346, 149) - ASSET_150 on 2024-12-12\n",
      "662. Position (271, 187) - ASSET_188 on 2024-09-28\n",
      "663. Position (421, 195) - ASSET_196 on 2025-02-25\n",
      "664. Position (580, 31) - ASSET_32 on 2025-08-03\n",
      "665. Position (672, 149) - ASSET_150 on 2025-11-03\n",
      "666. Position (321, 108) - ASSET_109 on 2024-11-17\n",
      "667. Position (451, 37) - ASSET_38 on 2025-03-27\n",
      "668. Position (10, 83) - ASSET_84 on 2024-01-11\n",
      "669. Position (412,  2) - ASSET_03 on 2025-02-16\n",
      "670. Position (636, 103) - ASSET_104 on 2025-09-28\n",
      "671. Position (87,  2) - ASSET_03 on 2024-03-28\n",
      "672. Position (162, 113) - ASSET_114 on 2024-06-11\n",
      "673. Position (436, 113) - ASSET_114 on 2025-03-12\n",
      "674. Position (605, 88) - ASSET_89 on 2025-08-28\n",
      "675. Position (264, 174) - ASSET_175 on 2024-09-21\n",
      "676. Position (104, 196) - ASSET_197 on 2024-04-14\n",
      "677. Position (653, 39) - ASSET_40 on 2025-10-15\n",
      "678. Position (416, 116) - ASSET_117 on 2025-02-20\n",
      "679. Position (297, 123) - ASSET_124 on 2024-10-24\n",
      "680. Position (199, 145) - ASSET_146 on 2024-07-18\n",
      "681. Position (228, 50) - ASSET_51 on 2024-08-16\n",
      "682. Position (217, 146) - ASSET_147 on 2024-08-05\n",
      "683. Position (58,  9) - ASSET_10 on 2024-02-28\n",
      "684. Position (206, 124) - ASSET_125 on 2024-07-25\n",
      "685. Position (86, 143) - ASSET_144 on 2024-03-27\n",
      "686. Position (242, 124) - ASSET_125 on 2024-08-30\n",
      "687. Position (119, 39) - ASSET_40 on 2024-04-29\n",
      "688. Position (338, 194) - ASSET_195 on 2024-12-04\n",
      "689. Position (472, 25) - ASSET_26 on 2025-04-17\n",
      "690. Position (691, 188) - ASSET_189 on 2025-11-22\n",
      "691. Position (554, 15) - ASSET_16 on 2025-07-08\n",
      "692. Position (365, 57) - ASSET_58 on 2024-12-31\n",
      "693. Position (698, 114) - ASSET_115 on 2025-11-29\n",
      "694. Position (357, 151) - ASSET_152 on 2024-12-23\n",
      "695. Position (609, 89) - ASSET_90 on 2025-09-01\n",
      "696. Position (692, 88) - ASSET_89 on 2025-11-23\n",
      "697. Position (18, 37) - ASSET_38 on 2024-01-19\n",
      "698. Position (187, 130) - ASSET_131 on 2024-07-06\n",
      "699. Position (266, 24) - ASSET_25 on 2024-09-23\n",
      "700. Position (528,  8) - ASSET_09 on 2025-06-12\n",
      "701. Position (513, 46) - ASSET_47 on 2025-05-28\n",
      "702. Position (122, 111) - ASSET_112 on 2024-05-02\n",
      "703. Position (505, 149) - ASSET_150 on 2025-05-20\n",
      "704. Position (39, 61) - ASSET_62 on 2024-02-09\n",
      "705. Position (47, 92) - ASSET_93 on 2024-02-17\n",
      "706. Position (587, 110) - ASSET_111 on 2025-08-10\n",
      "707. Position (286, 146) - ASSET_147 on 2024-10-13\n",
      "708. Position (503, 104) - ASSET_105 on 2025-05-18\n",
      "709. Position (650, 62) - ASSET_63 on 2025-10-12\n",
      "710. Position (232, 148) - ASSET_149 on 2024-08-20\n",
      "711. Position (567, 25) - ASSET_26 on 2025-07-21\n",
      "712. Position (350, 168) - ASSET_169 on 2024-12-16\n",
      "713. Position (79, 84) - ASSET_85 on 2024-03-20\n",
      "714. Position (617, 193) - ASSET_194 on 2025-09-09\n",
      "715. Position (535, 141) - ASSET_142 on 2025-06-19\n",
      "716. Position (571, 160) - ASSET_161 on 2025-07-25\n",
      "717. Position (19, 115) - ASSET_116 on 2024-01-20\n",
      "718. Position (544, 135) - ASSET_136 on 2025-06-28\n",
      "719. Position (406, 17) - ASSET_18 on 2025-02-10\n",
      "720. Position (144, 86) - ASSET_87 on 2024-05-24\n",
      "721. Position (503, 42) - ASSET_43 on 2025-05-18\n",
      "722. Position (655, 88) - ASSET_89 on 2025-10-17\n",
      "723. Position (609, 170) - ASSET_171 on 2025-09-01\n",
      "724. Position (110, 104) - ASSET_105 on 2024-04-20\n",
      "725. Position (591, 104) - ASSET_105 on 2025-08-14\n",
      "726. Position (625, 101) - ASSET_102 on 2025-09-17\n",
      "727. Position (390, 195) - ASSET_196 on 2025-01-25\n",
      "728. Position (368, 174) - ASSET_175 on 2025-01-03\n",
      "729. Position (119,  9) - ASSET_10 on 2024-04-29\n",
      "730. Position (661, 199) - ASSET_200 on 2025-10-23\n",
      "731. Position (279, 18) - ASSET_19 on 2024-10-06\n",
      "732. Position (579, 174) - ASSET_175 on 2025-08-02\n",
      "733. Position (416, 132) - ASSET_133 on 2025-02-20\n",
      "734. Position (598, 117) - ASSET_118 on 2025-08-21\n",
      "735. Position (183, 122) - ASSET_123 on 2024-07-02\n",
      "736. Position (203, 80) - ASSET_81 on 2024-07-22\n",
      "737. Position (194, 67) - ASSET_68 on 2024-07-13\n",
      "738. Position (487, 39) - ASSET_40 on 2025-05-02\n",
      "739. Position (185, 189) - ASSET_190 on 2024-07-04\n",
      "740. Position (552, 54) - ASSET_55 on 2025-07-06\n",
      "741. Position (545, 74) - ASSET_75 on 2025-06-29\n",
      "742. Position (437, 120) - ASSET_121 on 2025-03-13\n",
      "743. Position (127, 118) - ASSET_119 on 2024-05-07\n",
      "744. Position (275, 88) - ASSET_89 on 2024-10-02\n",
      "745. Position (619, 50) - ASSET_51 on 2025-09-11\n",
      "746. Position (67, 72) - ASSET_73 on 2024-03-08\n",
      "747. Position (85, 81) - ASSET_82 on 2024-03-26\n",
      "748. Position (229, 128) - ASSET_129 on 2024-08-17\n",
      "749. Position (368, 192) - ASSET_193 on 2025-01-03\n",
      "750. Position (474, 188) - ASSET_189 on 2025-04-19\n",
      "751. Position (626, 186) - ASSET_187 on 2025-09-18\n",
      "752. Position (189, 107) - ASSET_108 on 2024-07-08\n",
      "753. Position (38, 138) - ASSET_139 on 2024-02-08\n",
      "754. Position (658, 129) - ASSET_130 on 2025-10-20\n",
      "755. Position (312, 149) - ASSET_150 on 2024-11-08\n",
      "756. Position (91, 165) - ASSET_166 on 2024-04-01\n",
      "757. Position (182, 72) - ASSET_73 on 2024-07-01\n",
      "758. Position (308, 43) - ASSET_44 on 2024-11-04\n",
      "759. Position (84, 153) - ASSET_154 on 2024-03-25\n",
      "760. Position (37, 81) - ASSET_82 on 2024-02-07\n",
      "761. Position (564, 148) - ASSET_149 on 2025-07-18\n",
      "762. Position (206, 27) - ASSET_28 on 2024-07-25\n",
      "763. Position (305, 119) - ASSET_120 on 2024-11-01\n",
      "764. Position (288, 132) - ASSET_133 on 2024-10-15\n",
      "765. Position (80, 80) - ASSET_81 on 2024-03-21\n",
      "766. Position (226, 90) - ASSET_91 on 2024-08-14\n",
      "767. Position (353, 87) - ASSET_88 on 2024-12-19\n",
      "768. Position (185, 148) - ASSET_149 on 2024-07-04\n",
      "769. Position (396, 67) - ASSET_68 on 2025-01-31\n",
      "770. Position (88, 182) - ASSET_183 on 2024-03-29\n",
      "771. Position (114, 109) - ASSET_110 on 2024-04-24\n",
      "772. Position (653, 98) - ASSET_99 on 2025-10-15\n",
      "773. Position (320, 175) - ASSET_176 on 2024-11-16\n",
      "774. Position (268, 83) - ASSET_84 on 2024-09-25\n",
      "775. Position (495, 192) - ASSET_193 on 2025-05-10\n",
      "776. Position (188, 125) - ASSET_126 on 2024-07-07\n",
      "777. Position (522, 159) - ASSET_160 on 2025-06-06\n",
      "778. Position (202,  5) - ASSET_06 on 2024-07-21\n",
      "779. Position (12, 108) - ASSET_109 on 2024-01-13\n",
      "780. Position (117, 12) - ASSET_13 on 2024-04-27\n",
      "781. Position (448, 94) - ASSET_95 on 2025-03-24\n",
      "782. Position (23, 121) - ASSET_122 on 2024-01-24\n",
      "783. Position (288, 26) - ASSET_27 on 2024-10-15\n",
      "784. Position (641, 29) - ASSET_30 on 2025-10-03\n",
      "785. Position (263, 149) - ASSET_150 on 2024-09-20\n",
      "786. Position (317, 182) - ASSET_183 on 2024-11-13\n",
      "787. Position (545, 151) - ASSET_152 on 2025-06-29\n",
      "788. Position (674, 105) - ASSET_106 on 2025-11-05\n",
      "789. Position (79, 119) - ASSET_120 on 2024-03-20\n",
      "790. Position (252, 42) - ASSET_43 on 2024-09-09\n",
      "791. Position (221, 153) - ASSET_154 on 2024-08-09\n",
      "792. Position (302, 186) - ASSET_187 on 2024-10-29\n",
      "793. Position (174, 68) - ASSET_69 on 2024-06-23\n",
      "794. Position (565, 135) - ASSET_136 on 2025-07-19\n",
      "795. Position (350, 185) - ASSET_186 on 2024-12-16\n",
      "796. Position (447, 93) - ASSET_94 on 2025-03-23\n",
      "797. Position (49, 36) - ASSET_37 on 2024-02-19\n",
      "798. Position (57, 133) - ASSET_134 on 2024-02-27\n",
      "799. Position (454, 30) - ASSET_31 on 2025-03-30\n",
      "800. Position (419, 135) - ASSET_136 on 2025-02-23\n",
      "801. Position (122, 197) - ASSET_198 on 2024-05-02\n",
      "802. Position (90, 108) - ASSET_109 on 2024-03-31\n",
      "803. Position (451, 197) - ASSET_198 on 2025-03-27\n",
      "804. Position (496, 43) - ASSET_44 on 2025-05-11\n",
      "805. Position (34, 141) - ASSET_142 on 2024-02-04\n",
      "806. Position (420, 194) - ASSET_195 on 2025-02-24\n",
      "807. Position (401, 120) - ASSET_121 on 2025-02-05\n",
      "808. Position (386, 131) - ASSET_132 on 2025-01-21\n",
      "809. Position (22, 121) - ASSET_122 on 2024-01-23\n",
      "810. Position (575,  2) - ASSET_03 on 2025-07-29\n",
      "811. Position (433, 121) - ASSET_122 on 2025-03-09\n",
      "812. Position (366, 54) - ASSET_55 on 2025-01-01\n",
      "813. Position (289, 47) - ASSET_48 on 2024-10-16\n",
      "814. Position (334, 103) - ASSET_104 on 2024-11-30\n",
      "815. Position (507, 118) - ASSET_119 on 2025-05-22\n",
      "816. Position (354, 65) - ASSET_66 on 2024-12-20\n",
      "817. Position (632, 105) - ASSET_106 on 2025-09-24\n",
      "818. Position (295, 140) - ASSET_141 on 2024-10-22\n",
      "819. Position (112, 34) - ASSET_35 on 2024-04-22\n",
      "820. Position (505, 135) - ASSET_136 on 2025-05-20\n",
      "821. Position (565, 147) - ASSET_148 on 2025-07-19\n",
      "822. Position (352, 29) - ASSET_30 on 2024-12-18\n",
      "823. Position (86, 95) - ASSET_96 on 2024-03-27\n",
      "824. Position (61, 184) - ASSET_185 on 2024-03-02\n",
      "825. Position (447, 90) - ASSET_91 on 2025-03-23\n",
      "826. Position (597, 61) - ASSET_62 on 2025-08-20\n",
      "827. Position (143, 123) - ASSET_124 on 2024-05-23\n",
      "828. Position (472, 110) - ASSET_111 on 2025-04-17\n",
      "829. Position (239, 76) - ASSET_77 on 2024-08-27\n",
      "830. Position (200, 82) - ASSET_83 on 2024-07-19\n",
      "831. Position (407, 131) - ASSET_132 on 2025-02-11\n",
      "832. Position (335, 78) - ASSET_79 on 2024-12-01\n",
      "833. Position (595, 113) - ASSET_114 on 2025-08-18\n",
      "834. Position (335, 176) - ASSET_177 on 2024-12-01\n",
      "835. Position (618, 54) - ASSET_55 on 2025-09-10\n",
      "836. Position (406, 121) - ASSET_122 on 2025-02-10\n",
      "837. Position (412, 68) - ASSET_69 on 2025-02-16\n",
      "838. Position (270, 156) - ASSET_157 on 2024-09-27\n",
      "839. Position (355, 130) - ASSET_131 on 2024-12-21\n",
      "840. Position (426, 155) - ASSET_156 on 2025-03-02\n",
      "841. Position (628, 191) - ASSET_192 on 2025-09-20\n",
      "842. Position (420, 163) - ASSET_164 on 2025-02-24\n",
      "843. Position (406, 136) - ASSET_137 on 2025-02-10\n",
      "844. Position (474, 125) - ASSET_126 on 2025-04-19\n",
      "845. Position (94, 199) - ASSET_200 on 2024-04-04\n",
      "846. Position (50, 188) - ASSET_189 on 2024-02-20\n",
      "847. Position (290, 80) - ASSET_81 on 2024-10-17\n",
      "848. Position (605, 197) - ASSET_198 on 2025-08-28\n",
      "849. Position (469, 179) - ASSET_180 on 2025-04-14\n",
      "850. Position (65, 62) - ASSET_63 on 2024-03-06\n",
      "851. Position (148, 108) - ASSET_109 on 2024-05-28\n",
      "852. Position (557, 194) - ASSET_195 on 2025-07-11\n",
      "853. Position (506, 164) - ASSET_165 on 2025-05-21\n",
      "854. Position (49, 155) - ASSET_156 on 2024-02-19\n",
      "855. Position (273, 109) - ASSET_110 on 2024-09-30\n",
      "856. Position (184, 107) - ASSET_108 on 2024-07-03\n",
      "857. Position (657, 67) - ASSET_68 on 2025-10-19\n",
      "858. Position (617, 71) - ASSET_72 on 2025-09-09\n",
      "859. Position (103, 147) - ASSET_148 on 2024-04-13\n",
      "860. Position (303, 139) - ASSET_140 on 2024-10-30\n",
      "861. Position (184, 129) - ASSET_130 on 2024-07-03\n",
      "862. Position (451, 131) - ASSET_132 on 2025-03-27\n",
      "863. Position (193, 33) - ASSET_34 on 2024-07-12\n",
      "864. Position (431, 15) - ASSET_16 on 2025-03-07\n",
      "865. Position (22, 198) - ASSET_199 on 2024-01-23\n",
      "866. Position (291, 54) - ASSET_55 on 2024-10-18\n",
      "867. Position (638, 83) - ASSET_84 on 2025-09-30\n",
      "868. Position (583, 59) - ASSET_60 on 2025-08-06\n",
      "869. Position (193, 153) - ASSET_154 on 2024-07-12\n",
      "870. Position (322, 148) - ASSET_149 on 2024-11-18\n",
      "871. Position (486,  3) - ASSET_04 on 2025-05-01\n",
      "872. Position (326, 53) - ASSET_54 on 2024-11-22\n",
      "873. Position (640, 44) - ASSET_45 on 2025-10-02\n",
      "874. Position (378, 148) - ASSET_149 on 2025-01-13\n",
      "875. Position (634, 137) - ASSET_138 on 2025-09-26\n",
      "876. Position (275, 51) - ASSET_52 on 2024-10-02\n",
      "877. Position (591, 161) - ASSET_162 on 2025-08-14\n",
      "878. Position (286, 141) - ASSET_142 on 2024-10-13\n",
      "879. Position (353, 96) - ASSET_97 on 2024-12-19\n",
      "880. Position (299, 83) - ASSET_84 on 2024-10-26\n",
      "881. Position (355, 123) - ASSET_124 on 2024-12-21\n",
      "882. Position (509, 146) - ASSET_147 on 2025-05-24\n",
      "883. Position (209, 188) - ASSET_189 on 2024-07-28\n",
      "884. Position (266, 57) - ASSET_58 on 2024-09-23\n",
      "885. Position (345, 23) - ASSET_24 on 2024-12-11\n",
      "886. Position (656, 76) - ASSET_77 on 2025-10-18\n",
      "887. Position (94, 41) - ASSET_42 on 2024-04-04\n",
      "888. Position (327, 66) - ASSET_67 on 2024-11-23\n",
      "889. Position (256,  3) - ASSET_04 on 2024-09-13\n",
      "890. Position (471, 29) - ASSET_30 on 2025-04-16\n",
      "891. Position (280, 103) - ASSET_104 on 2024-10-07\n",
      "892. Position ( 9, 107) - ASSET_108 on 2024-01-10\n",
      "893. Position (265, 85) - ASSET_86 on 2024-09-22\n",
      "894. Position (542, 10) - ASSET_11 on 2025-06-26\n",
      "895. Position (27, 26) - ASSET_27 on 2024-01-28\n",
      "896. Position (286, 163) - ASSET_164 on 2024-10-13\n",
      "897. Position (628, 86) - ASSET_87 on 2025-09-20\n",
      "898. Position (523, 33) - ASSET_34 on 2025-06-07\n",
      "899. Position (262, 119) - ASSET_120 on 2024-09-19\n",
      "900. Position (143,  5) - ASSET_06 on 2024-05-23\n",
      "901. Position (212, 111) - ASSET_112 on 2024-07-31\n",
      "902. Position (40, 22) - ASSET_23 on 2024-02-10\n",
      "903. Position (523, 175) - ASSET_176 on 2025-06-07\n",
      "904. Position (54, 86) - ASSET_87 on 2024-02-24\n",
      "905. Position (260, 96) - ASSET_97 on 2024-09-17\n",
      "906. Position (46, 77) - ASSET_78 on 2024-02-16\n",
      "907. Position (171, 183) - ASSET_184 on 2024-06-20\n",
      "908. Position (463, 12) - ASSET_13 on 2025-04-08\n",
      "909. Position (471, 101) - ASSET_102 on 2025-04-16\n",
      "910. Position (331, 173) - ASSET_174 on 2024-11-27\n",
      "911. Position (362, 93) - ASSET_94 on 2024-12-28\n",
      "912. Position (42, 120) - ASSET_121 on 2024-02-12\n",
      "913. Position (586, 141) - ASSET_142 on 2025-08-09\n",
      "914. Position (373, 177) - ASSET_178 on 2025-01-08\n",
      "915. Position (259, 55) - ASSET_56 on 2024-09-16\n",
      "916. Position (661, 179) - ASSET_180 on 2025-10-23\n",
      "917. Position (142, 127) - ASSET_128 on 2024-05-22\n",
      "918. Position (647, 35) - ASSET_36 on 2025-10-09\n",
      "919. Position (316, 184) - ASSET_185 on 2024-11-12\n",
      "920. Position (546, 98) - ASSET_99 on 2025-06-30\n",
      "921. Position (29, 85) - ASSET_86 on 2024-01-30\n",
      "922. Position (166, 11) - ASSET_12 on 2024-06-15\n",
      "923. Position (158, 154) - ASSET_155 on 2024-06-07\n",
      "924. Position (652, 182) - ASSET_183 on 2025-10-14\n",
      "925. Position (108, 70) - ASSET_71 on 2024-04-18\n",
      "926. Position (490, 10) - ASSET_11 on 2025-05-05\n",
      "927. Position (265, 151) - ASSET_152 on 2024-09-22\n",
      "928. Position (430, 188) - ASSET_189 on 2025-03-06\n",
      "929. Position (359,  4) - ASSET_05 on 2024-12-25\n",
      "930. Position (423, 25) - ASSET_26 on 2025-02-27\n",
      "931. Position (403, 157) - ASSET_158 on 2025-02-07\n",
      "932. Position (92,  9) - ASSET_10 on 2024-04-02\n",
      "933. Position (112, 19) - ASSET_20 on 2024-04-22\n",
      "934. Position (647, 28) - ASSET_29 on 2025-10-09\n",
      "935. Position (289, 149) - ASSET_150 on 2024-10-16\n",
      "936. Position (327, 62) - ASSET_63 on 2024-11-23\n",
      "937. Position (408, 96) - ASSET_97 on 2025-02-12\n",
      "938. Position (608, 108) - ASSET_109 on 2025-08-31\n",
      "939. Position (626, 72) - ASSET_73 on 2025-09-18\n",
      "940. Position (485, 119) - ASSET_120 on 2025-04-30\n",
      "941. Position (578, 102) - ASSET_103 on 2025-08-01\n",
      "942. Position (669, 182) - ASSET_183 on 2025-10-31\n",
      "943. Position (102, 47) - ASSET_48 on 2024-04-12\n",
      "944. Position (690, 84) - ASSET_85 on 2025-11-21\n",
      "945. Position (674, 155) - ASSET_156 on 2025-11-05\n",
      "946. Position (245, 129) - ASSET_130 on 2024-09-02\n",
      "947. Position (454, 45) - ASSET_46 on 2025-03-30\n",
      "948. Position (655, 62) - ASSET_63 on 2025-10-17\n",
      "949. Position (394, 87) - ASSET_88 on 2025-01-29\n",
      "950. Position (537, 24) - ASSET_25 on 2025-06-21\n",
      "951. Position (235, 179) - ASSET_180 on 2024-08-23\n",
      "952. Position (106, 50) - ASSET_51 on 2024-04-16\n",
      "953. Position (75, 81) - ASSET_82 on 2024-03-16\n",
      "954. Position (291, 82) - ASSET_83 on 2024-10-18\n",
      "955. Position (638, 150) - ASSET_151 on 2025-09-30\n",
      "956. Position (639, 132) - ASSET_133 on 2025-10-01\n",
      "957. Position (311, 126) - ASSET_127 on 2024-11-07\n",
      "958. Position (263, 154) - ASSET_155 on 2024-09-20\n",
      "959. Position (652, 71) - ASSET_72 on 2025-10-14\n",
      "960. Position (486, 101) - ASSET_102 on 2025-05-01\n",
      "961. Position (646, 55) - ASSET_56 on 2025-10-08\n",
      "962. Position (332, 35) - ASSET_36 on 2024-11-28\n",
      "963. Position (453, 66) - ASSET_67 on 2025-03-29\n",
      "964. Position (613, 110) - ASSET_111 on 2025-09-05\n",
      "965. Position (249, 102) - ASSET_103 on 2024-09-06\n",
      "966. Position (245, 62) - ASSET_63 on 2024-09-02\n",
      "967. Position (60, 114) - ASSET_115 on 2024-03-01\n",
      "968. Position (259, 22) - ASSET_23 on 2024-09-16\n",
      "969. Position (430, 78) - ASSET_79 on 2025-03-06\n",
      "970. Position (72, 34) - ASSET_35 on 2024-03-13\n",
      "971. Position (338, 19) - ASSET_20 on 2024-12-04\n",
      "972. Position (55, 147) - ASSET_148 on 2024-02-25\n",
      "973. Position (686, 146) - ASSET_147 on 2025-11-17\n",
      "974. Position (391, 41) - ASSET_42 on 2025-01-26\n",
      "975. Position (326, 161) - ASSET_162 on 2024-11-22\n",
      "976. Position (330, 170) - ASSET_171 on 2024-11-26\n",
      "977. Position (199, 87) - ASSET_88 on 2024-07-18\n",
      "978. Position (278, 105) - ASSET_106 on 2024-10-05\n",
      "979. Position (193, 112) - ASSET_113 on 2024-07-12\n",
      "980. Position (332, 120) - ASSET_121 on 2024-11-28\n",
      "981. Position (230, 68) - ASSET_69 on 2024-08-18\n",
      "982. Position (201, 103) - ASSET_104 on 2024-07-20\n",
      "983. Position (175, 183) - ASSET_184 on 2024-06-24\n",
      "984. Position (88, 118) - ASSET_119 on 2024-03-29\n",
      "985. Position (79,  4) - ASSET_05 on 2024-03-20\n",
      "986. Position (10, 113) - ASSET_114 on 2024-01-11\n",
      "987. Position (324,  1) - ASSET_02 on 2024-11-20\n",
      "988. Position (556, 172) - ASSET_173 on 2025-07-10\n",
      "989. Position (446, 109) - ASSET_110 on 2025-03-22\n",
      "990. Position (505, 155) - ASSET_156 on 2025-05-20\n",
      "991. Position (695, 183) - ASSET_184 on 2025-11-26\n",
      "992. Position (54, 168) - ASSET_169 on 2024-02-24\n",
      "993. Position (26, 137) - ASSET_138 on 2024-01-27\n",
      "994. Position (305, 77) - ASSET_78 on 2024-11-01\n",
      "995. Position (246, 71) - ASSET_72 on 2024-09-03\n",
      "996. Position (422, 125) - ASSET_126 on 2025-02-26\n",
      "997. Position (51, 28) - ASSET_29 on 2024-02-21\n",
      "998. Position (306, 42) - ASSET_43 on 2024-11-02\n",
      "999. Position (131, 132) - ASSET_133 on 2024-05-11\n",
      "1000. Position (548, 151) - ASSET_152 on 2025-07-02\n",
      "\n",
      "DataFrame with missing values:\n",
      "Missing values count: 1000\n",
      "\n",
      "NumPy array with missing values:\n",
      "Missing values count: ASSET_01     3\n",
      "ASSET_02     5\n",
      "ASSET_03     4\n",
      "ASSET_04     5\n",
      "ASSET_05     9\n",
      "            ..\n",
      "ASSET_196    6\n",
      "ASSET_197    2\n",
      "ASSET_198    5\n",
      "ASSET_199    1\n",
      "ASSET_200    8\n",
      "Length: 200, dtype: int64\n",
      "Missing values count: 1000\n",
      "Shape: (700, 200)\n"
     ]
    }
   ],
   "source": [
    "# MISSING VALUES FUNCTION = create them\n",
    "def introduce_missing_values(data, n_missing, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Work with a copy to preserve original data\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data_with_missing = data.copy()\n",
    "        rows, cols = data.shape\n",
    "    else:\n",
    "        data_with_missing = data.copy().astype(float)  # Convert to float to allow NaN\n",
    "        rows, cols = data.shape\n",
    "    \n",
    "    # Generate random positions for missing values\n",
    "    total_positions = rows * cols\n",
    "    missing_indices = np.random.choice(total_positions, size=n_missing, replace=False)\n",
    "    \n",
    "    # Convert flat indices to (row, col) positions\n",
    "    missing_positions = [(idx // cols, idx % cols) for idx in missing_indices]\n",
    "    \n",
    "    # Introduce missing values\n",
    "    for row, col in missing_positions:\n",
    "        if isinstance(data_with_missing, pd.DataFrame):\n",
    "            data_with_missing.iloc[row, col] = np.nan\n",
    "        else:\n",
    "            data_with_missing[row, col] = np.nan\n",
    "    \n",
    "    return data_with_missing, missing_positions\n",
    "\n",
    "# Apply missing values to both DataFrame and numpy array\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTRODUCING 1000 RANDOM MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For DataFrame\n",
    "df_missing, missing_pos_df = introduce_missing_values(df, n_missing = 1000)\n",
    "\n",
    "# For numpy array\n",
    "financial_data_missing, missing_pos_array = introduce_missing_values(df, n_missing = 1000)\n",
    "\n",
    "print(f\"\\nMissing values introduced at positions (row, col):\")\n",
    "for i, (row, col) in enumerate(missing_pos_df, 1):\n",
    "    asset_name = df.columns[col] if col < len(df.columns) else f\"Asset_{col}\"\n",
    "    date_str = df.index[row].strftime('%Y-%m-%d') if row < len(df.index) else f\"Day_{row}\"\n",
    "    print(f\"{i:2d}. Position ({row:2d}, {col:2d}) - {asset_name} on {date_str}\")\n",
    "\n",
    "print(f\"\\nDataFrame with missing values:\")\n",
    "print(\"Missing values count:\", df_missing.isnull().sum().sum())\n",
    "#print(df_missing.head(20))\n",
    "\n",
    "print(f\"\\nNumPy array with missing values:\")\n",
    "print(\"Missing values count:\", (np.isnan(financial_data_missing).sum()))\n",
    "print(\"Missing values count:\", sum(np.isnan(financial_data_missing).sum()))\n",
    "print(\"Shape:\", financial_data_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7a1ad16-6bea-4b4f-a07e-ea218e220511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINANCIAL IMPUTATION ANALYZER\n",
      "================================================================================\n",
      "analyzer = FinancialImputationAnalyzer(data_with_missing, original_data)\n",
      "results = analyzer.compare_all_methods(window=30)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FinancialImputationAnalyzer:\n",
    "    \"\"\"\n",
    "    Advanced imputation analyzer specifically designed for financial time series data.\n",
    "    Implements mean/median imputation with financial data considerations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_with_missing, original_data=None):\n",
    "        \"\"\"\n",
    "        Initialize the imputation analyzer\n",
    "        \n",
    "        Parameters:\n",
    "        - data_with_missing: DataFrame or numpy array with missing values\n",
    "        - original_data: Original complete data for evaluation (optional)\n",
    "        \"\"\"\n",
    "        self.data_missing = data_with_missing.copy() if hasattr(data_with_missing, 'copy') else data_with_missing.copy()\n",
    "        self.original_data = original_data.copy() if original_data is not None else None\n",
    "        self.imputation_results = {}\n",
    "        self.performance_metrics = {}\n",
    "        \n",
    "    def analyze_missing_pattern(self):\n",
    "        \"\"\"Analyze the pattern of missing values for financial context\"\"\"\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            missing_info = self.data_missing.isnull()\n",
    "        else:\n",
    "            missing_info = pd.DataFrame(np.isnan(self.data_missing))\n",
    "        \n",
    "        print(\"ðŸ” MISSING VALUE PATTERN ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_missing = missing_info.sum().sum()\n",
    "        total_cells = missing_info.shape[0] * missing_info.shape[1]\n",
    "        missing_pct = (total_missing / total_cells) * 100\n",
    "        \n",
    "        print(f\"Dataset shape: {missing_info.shape}\")\n",
    "        print(f\"Total missing values: {total_missing:,}\")\n",
    "        print(f\"Missing percentage: {missing_pct:.2f}%\")\n",
    "        \n",
    "        # Missing by time period (rows)\n",
    "        missing_by_row = missing_info.sum(axis=1)\n",
    "        print(f\"\\nMissing values per time period:\")\n",
    "        print(f\"  Min: {missing_by_row.min()}\")\n",
    "        print(f\"  Max: {missing_by_row.max()}\")\n",
    "        print(f\"  Mean: {missing_by_row.mean():.1f}\")\n",
    "        \n",
    "        # Missing by asset (columns)\n",
    "        missing_by_col = missing_info.sum(axis=0)\n",
    "        print(f\"\\nMissing values per asset:\")\n",
    "        print(f\"  Min: {missing_by_col.min()}\")\n",
    "        print(f\"  Max: {missing_by_col.max()}\")\n",
    "        print(f\"  Mean: {missing_by_col.mean():.1f}\")\n",
    "        \n",
    "        # Financial data specific checks\n",
    "        print(f\"\\nðŸ“Š FINANCIAL DATA QUALITY CHECKS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check for consecutive missing values (problematic for time series)\n",
    "        consecutive_missing = []\n",
    "        for col in range(missing_info.shape[1]):\n",
    "            col_missing = missing_info.iloc[:, col]\n",
    "            consecutive = 0\n",
    "            max_consecutive = 0\n",
    "            for val in col_missing:\n",
    "                if val:\n",
    "                    consecutive += 1\n",
    "                    max_consecutive = max(max_consecutive, consecutive)\n",
    "                else:\n",
    "                    consecutive = 0\n",
    "            consecutive_missing.append(max_consecutive)\n",
    "        \n",
    "        max_consecutive_overall = max(consecutive_missing)\n",
    "        print(f\"Maximum consecutive missing values: {max_consecutive_overall}\")\n",
    "        \n",
    "        if max_consecutive_overall > 5:\n",
    "            print(\"âš ï¸  WARNING: Long consecutive missing periods detected!\")\n",
    "            print(\"   Consider using interpolation instead of mean/median\")\n",
    "        else:\n",
    "            print(\"âœ… Missing pattern suitable for mean/median imputation\")\n",
    "            \n",
    "        return {\n",
    "            'total_missing': total_missing,\n",
    "            'missing_pct': missing_pct,\n",
    "            'max_consecutive': max_consecutive_overall,\n",
    "            'missing_by_row': missing_by_row,\n",
    "            'missing_by_col': missing_by_col\n",
    "        }\n",
    "    \n",
    "    def simple_mean_imputation(self):\n",
    "        \"\"\"\n",
    "        Simple mean imputation - replaces missing values with column mean\n",
    "        âš ï¸ WARNING: This distorts variance and ignores time series nature\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ“Š SIMPLE MEAN IMPUTATION\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                mean_val = imputed_data[col].mean()\n",
    "                imputed_data[col].fillna(mean_val, inplace=True)\n",
    "                print(f\"  {col}: filled with mean {mean_val:.2f}\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "                mean_val = np.nanmean(col_data)\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = mean_val\n",
    "                if col < 5:  # Print first 5 for brevity\n",
    "                    print(f\"  Asset {col+1:03d}: filled with mean {mean_val:.2f}\")\n",
    "            \n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1]-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['simple_mean'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def simple_median_imputation(self):\n",
    "        \"\"\"\n",
    "        Simple median imputation - replaces missing values with column median\n",
    "        More robust to outliers than mean\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ“Š SIMPLE MEDIAN IMPUTATION\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                median_val = imputed_data[col].median()\n",
    "                imputed_data[col].fillna(median_val, inplace=True)\n",
    "                print(f\"  {col}: filled with median {median_val:.2f}\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "                median_val = np.nanmedian(col_data)\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = median_val\n",
    "                if col < 5:  # Print first 5 for brevity\n",
    "                    print(f\"  Asset {col+1:03d}: filled with median {median_val:.2f}\")\n",
    "            \n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1]-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['simple_median'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def rolling_mean_imputation(self, window=30):\n",
    "        \"\"\"\n",
    "        Rolling mean imputation - uses local time window mean\n",
    "        Better for financial time series as it adapts to local trends\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ“Š ROLLING MEAN IMPUTATION (Window: {window})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                # Calculate rolling mean\n",
    "                rolling_mean = imputed_data[col].rolling(window=window, center=True, min_periods=1).mean()\n",
    "                # Fill missing values\n",
    "                imputed_data[col] = imputed_data[col].fillna(rolling_mean)\n",
    "                # If still missing (edge cases), use global mean\n",
    "                global_mean = imputed_data[col].mean()\n",
    "                imputed_data[col] = imputed_data[col].fillna(global_mean)\n",
    "                \n",
    "                filled_count = self.data_missing[col].isnull().sum()\n",
    "                print(f\"  {col}: filled {filled_count} values with rolling mean\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "                \n",
    "                # Create rolling mean using pandas for convenience\n",
    "                temp_series = pd.Series(col_data)\n",
    "                rolling_mean = temp_series.rolling(window=window, center=True, min_periods=1).mean()\n",
    "                \n",
    "                # Fill missing values\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = rolling_mean[mask]\n",
    "                \n",
    "                # Handle remaining NaN with global mean\n",
    "                remaining_nan = np.isnan(imputed_data[:, col])\n",
    "                if remaining_nan.any():\n",
    "                    global_mean = np.nanmean(imputed_data[:, col])\n",
    "                    imputed_data[remaining_nan, col] = global_mean\n",
    "                \n",
    "                if col < 5:\n",
    "                    filled_count = mask.sum()\n",
    "                    print(f\"  Asset {col+1:03d}: filled {filled_count} values with rolling mean\")\n",
    "            \n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1]-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['rolling_mean'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def rolling_median_imputation(self, window=30):\n",
    "        \"\"\"\n",
    "        Rolling median imputation - uses local time window median\n",
    "        Even more robust to outliers, good for volatile financial data\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ“Š ROLLING MEDIAN IMPUTATION (Window: {window})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                # Calculate rolling median\n",
    "                rolling_median = imputed_data[col].rolling(window=window, center=True, min_periods=1).median()\n",
    "                # Fill missing values\n",
    "                imputed_data[col] = imputed_data[col].fillna(rolling_median)\n",
    "                # If still missing (edge cases), use global median\n",
    "                global_median = imputed_data[col].median()\n",
    "                imputed_data[col] = imputed_data[col].fillna(global_median)\n",
    "                \n",
    "                filled_count = self.data_missing[col].isnull().sum()\n",
    "                print(f\"  {col}: filled {filled_count} values with rolling median\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "                \n",
    "                # Create rolling median using pandas for convenience\n",
    "                temp_series = pd.Series(col_data)\n",
    "                rolling_median = temp_series.rolling(window=window, center=True, min_periods=1).median()\n",
    "                \n",
    "                # Fill missing values\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = rolling_median[mask]\n",
    "                \n",
    "                # Handle remaining NaN with global median\n",
    "                remaining_nan = np.isnan(imputed_data[:, col])\n",
    "                if remaining_nan.any():\n",
    "                    global_median = np.nanmedian(imputed_data[:, col])\n",
    "                    imputed_data[remaining_nan, col] = global_median\n",
    "                \n",
    "                if col < 5:\n",
    "                    filled_count = mask.sum()\n",
    "                    print(f\"  Asset {col+1:03d}: filled {filled_count} values with rolling median\")\n",
    "            \n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1]-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['rolling_median'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def evaluate_imputation_quality(self, method_name, imputed_data):\n",
    "        \"\"\"\n",
    "        Evaluate imputation quality if original data is available - NO SKLEARN VERSION\n",
    "        \"\"\"\n",
    "        if self.original_data is None:\n",
    "            print(f\"\\nâš ï¸  No original data available for {method_name} evaluation\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nðŸ“ˆ IMPUTATION QUALITY EVALUATION: {method_name.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Force everything to be numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "                \n",
    "            if hasattr(imputed_data, 'values'):\n",
    "                imputed_np = imputed_data.values.copy()\n",
    "            else:\n",
    "                imputed_np = imputed_data.copy()\n",
    "            \n",
    "            print(f\"  Data shapes - Missing: {missing_np.shape}, Original: {original_np.shape}, Imputed: {imputed_np.shape}\")\n",
    "            \n",
    "            # Create mask\n",
    "            mask = np.isnan(missing_np)\n",
    "            total_missing = np.sum(mask)\n",
    "            print(f\"  Found {total_missing} missing values to evaluate\")\n",
    "            \n",
    "            if total_missing == 0:\n",
    "                print(\"  No missing values found!\")\n",
    "                return None\n",
    "            \n",
    "            # Extract values using simple indexing\n",
    "            orig_vals = []\n",
    "            imp_vals = []\n",
    "            \n",
    "            rows, cols = missing_np.shape\n",
    "            for i in range(rows):\n",
    "                for j in range(cols):\n",
    "                    if mask[i, j]:  # This was originally missing\n",
    "                        orig_vals.append(original_np[i, j])\n",
    "                        imp_vals.append(imputed_np[i, j])\n",
    "            \n",
    "            orig_vals = np.array(orig_vals)\n",
    "            imp_vals = np.array(imp_vals)\n",
    "            \n",
    "            print(f\"  Extracted {len(orig_vals)} value pairs\")\n",
    "            \n",
    "            # Simple metrics - MANUAL CALCULATION ONLY\n",
    "            differences = orig_vals - imp_vals\n",
    "            abs_differences = np.abs(differences)\n",
    "            \n",
    "            mae = np.mean(abs_differences)\n",
    "            mse = np.mean(differences**2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Simple percentage error (avoid division by zero)\n",
    "            nonzero_mask = orig_vals != 0\n",
    "            if np.sum(nonzero_mask) > 0:\n",
    "                pct_errors = abs_differences[nonzero_mask] / np.abs(orig_vals[nonzero_mask])\n",
    "                mape = np.mean(pct_errors) * 100\n",
    "            else:\n",
    "                mape = float('inf')\n",
    "            \n",
    "            # Simple correlation\n",
    "            if len(orig_vals) > 1:\n",
    "                corr = np.corrcoef(orig_vals, imp_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 1.0\n",
    "                \n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "            print(f\"  RMSE: {rmse:.4f}\")\n",
    "            print(f\"  MAPE: {mape:.2f}%\" if np.isfinite(mape) else \"  MAPE: âˆž\")\n",
    "            print(f\"  Correlation: {corr:.4f}\")\n",
    "            \n",
    "            # Financial interpretation\n",
    "            print(f\"\\nðŸ’° FINANCIAL INTERPRETATION:\")\n",
    "            print(f\"  Average price difference: ${mae:.2f}\")\n",
    "            print(f\"  Typical error magnitude: ${rmse:.2f}\")\n",
    "            \n",
    "            if np.isfinite(mape):\n",
    "                if mape < 5:\n",
    "                    print(\"  âœ… EXCELLENT: Very accurate imputation\")\n",
    "                elif mape < 10:\n",
    "                    print(\"  âœ… GOOD: Acceptable imputation quality\")\n",
    "                elif mape < 20:\n",
    "                    print(\"  âš ï¸  FAIR: Moderate imputation errors\")\n",
    "                else:\n",
    "                    print(\"  âŒ POOR: High imputation errors\")\n",
    "            else:\n",
    "                print(\"  âš ï¸  Note: MAPE could not be calculated due to zero values\")\n",
    "            \n",
    "            metrics = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse, \n",
    "                'MAPE': mape,\n",
    "                'Correlation': corr,\n",
    "                'N_Values': len(orig_vals)\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics[method_name] = metrics\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR in evaluation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def compare_all_methods(self, window=30):\n",
    "        \"\"\"\n",
    "        Run all imputation methods and compare results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸŽ¯ COMPREHENSIVE MEAN/MEDIAN IMPUTATION ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Analyze missing pattern first\n",
    "        self.analyze_missing_pattern()\n",
    "        \n",
    "        # Run all methods\n",
    "        methods = {\n",
    "            'Simple Mean': self.simple_mean_imputation,\n",
    "            'Simple Median': self.simple_median_imputation,\n",
    "            'Rolling Mean': lambda: self.rolling_mean_imputation(window),\n",
    "            'Rolling Median': lambda: self.rolling_median_imputation(window)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, method in methods.items():\n",
    "            print(f\"\\n{'='*20} {name.upper()} {'='*20}\")\n",
    "            imputed = method()\n",
    "            results[name] = imputed\n",
    "            \n",
    "            # Evaluate quality\n",
    "            metrics = self.evaluate_imputation_quality(name.lower().replace(' ', '_'), imputed)\n",
    "        \n",
    "        # Summary comparison\n",
    "        if self.performance_metrics:\n",
    "            self.print_method_comparison()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_method_comparison(self):\n",
    "        \"\"\"Print comparison of all methods\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ† METHOD COMPARISON SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        comparison_df = pd.DataFrame(self.performance_metrics).T\n",
    "        print(comparison_df.round(4))\n",
    "        \n",
    "        # Find best method by lowest RMSE\n",
    "        best_method = comparison_df['RMSE'].idxmin()\n",
    "        print(f\"\\nðŸ† BEST PERFORMING METHOD: {best_method.replace('_', ' ').title()}\")\n",
    "        print(f\"   RMSE: {comparison_df.loc[best_method, 'RMSE']:.4f}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\nðŸ’¡ RECOMMENDATIONS FOR FINANCIAL TIME SERIES:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if 'rolling_median' in self.performance_metrics:\n",
    "            print(\"ðŸ¥‡ BEST PRACTICE: Rolling Median\")\n",
    "            print(\"   âœ… Adapts to local market conditions\")\n",
    "            print(\"   âœ… Robust to price spikes/crashes\")\n",
    "            print(\"   âœ… Preserves time series properties\")\n",
    "        \n",
    "        print(f\"\\nðŸ¥ˆ ALTERNATIVE: Rolling Mean\")\n",
    "        print(\"   âœ… Good for stable market periods\")\n",
    "        print(\"   âš ï¸  Sensitive to outliers\")\n",
    "        \n",
    "        print(f\"\\nâš ï¸  AVOID: Simple Mean/Median\")\n",
    "        print(\"   âŒ Ignores time series structure\")\n",
    "        print(\"   âŒ Can create artificial patterns\")\n",
    "        print(\"   âŒ Distorts volatility\")\n",
    "        \n",
    "        print(f\"\\nðŸ”„ NEXT STEPS:\")\n",
    "        print(\"   1. Try interpolation methods (linear, spline)\")\n",
    "        print(\"   2. Consider forward/backward fill\")\n",
    "        print(\"   3. Test advanced methods (KNN, MICE)\")\n",
    "\n",
    "\n",
    "# READY TO USE\n",
    "print(\"=\" * 80)\n",
    "print(\"FINANCIAL IMPUTATION ANALYZER\")\n",
    "print(\"=\" * 80)\n",
    "print(\"analyzer = FinancialImputationAnalyzer(data_with_missing, original_data)\")\n",
    "print(\"results = analyzer.compare_all_methods(window=30)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9ac9df1-a872-4fa1-86ec-f41e1cf6514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ COMPREHENSIVE MEAN/MEDIAN IMPUTATION ANALYSIS\n",
      "================================================================================\n",
      "ðŸ” MISSING VALUE PATTERN ANALYSIS\n",
      "==================================================\n",
      "Dataset shape: (700, 200)\n",
      "Total missing values: 1,000\n",
      "Missing percentage: 0.71%\n",
      "\n",
      "Missing values per time period:\n",
      "  Min: 0\n",
      "  Max: 6\n",
      "  Mean: 1.4\n",
      "\n",
      "Missing values per asset:\n",
      "  Min: 0\n",
      "  Max: 12\n",
      "  Mean: 5.0\n",
      "\n",
      "ðŸ“Š FINANCIAL DATA QUALITY CHECKS:\n",
      "----------------------------------------\n",
      "Maximum consecutive missing values: 2\n",
      "âœ… Missing pattern suitable for mean/median imputation\n",
      "\n",
      "==================== SIMPLE MEAN ====================\n",
      "\n",
      "ðŸ“Š SIMPLE MEAN IMPUTATION\n",
      "========================================\n",
      "  ASSET_01: filled with mean 201.76\n",
      "  ASSET_02: filled with mean 145.55\n",
      "  ASSET_03: filled with mean 275.08\n",
      "  ASSET_04: filled with mean 287.13\n",
      "  ASSET_05: filled with mean 55.87\n",
      "  ASSET_06: filled with mean 226.24\n",
      "  ASSET_07: filled with mean 205.26\n",
      "  ASSET_08: filled with mean 239.53\n",
      "  ASSET_09: filled with mean 326.99\n",
      "  ASSET_10: filled with mean 219.54\n",
      "  ASSET_11: filled with mean 667.65\n",
      "  ASSET_12: filled with mean 71.45\n",
      "  ASSET_13: filled with mean 222.77\n",
      "  ASSET_14: filled with mean 213.62\n",
      "  ASSET_15: filled with mean 222.27\n",
      "  ASSET_16: filled with mean 355.05\n",
      "  ASSET_17: filled with mean 260.63\n",
      "  ASSET_18: filled with mean 261.07\n",
      "  ASSET_19: filled with mean 191.36\n",
      "  ASSET_20: filled with mean 348.04\n",
      "  ASSET_21: filled with mean 196.36\n",
      "  ASSET_22: filled with mean 347.54\n",
      "  ASSET_23: filled with mean 160.74\n",
      "  ASSET_24: filled with mean 426.11\n",
      "  ASSET_25: filled with mean 341.75\n",
      "  ASSET_26: filled with mean 162.52\n",
      "  ASSET_27: filled with mean 201.26\n",
      "  ASSET_28: filled with mean 725.21\n",
      "  ASSET_29: filled with mean 257.53\n",
      "  ASSET_30: filled with mean 145.59\n",
      "  ASSET_31: filled with mean 194.57\n",
      "  ASSET_32: filled with mean 75.53\n",
      "  ASSET_33: filled with mean 210.70\n",
      "  ASSET_34: filled with mean 155.46\n",
      "  ASSET_35: filled with mean 404.66\n",
      "  ASSET_36: filled with mean 499.87\n",
      "  ASSET_37: filled with mean 232.37\n",
      "  ASSET_38: filled with mean 136.37\n",
      "  ASSET_39: filled with mean 179.67\n",
      "  ASSET_40: filled with mean 109.08\n",
      "  ASSET_41: filled with mean 95.28\n",
      "  ASSET_42: filled with mean 132.01\n",
      "  ASSET_43: filled with mean 157.48\n",
      "  ASSET_44: filled with mean 137.69\n",
      "  ASSET_45: filled with mean 243.71\n",
      "  ASSET_46: filled with mean 241.25\n",
      "  ASSET_47: filled with mean 372.03\n",
      "  ASSET_48: filled with mean 154.50\n",
      "  ASSET_49: filled with mean 139.13\n",
      "  ASSET_50: filled with mean 581.70\n",
      "  ASSET_51: filled with mean 167.26\n",
      "  ASSET_52: filled with mean 299.32\n",
      "  ASSET_53: filled with mean 126.59\n",
      "  ASSET_54: filled with mean 290.01\n",
      "  ASSET_55: filled with mean 246.00\n",
      "  ASSET_56: filled with mean 331.33\n",
      "  ASSET_57: filled with mean 209.17\n",
      "  ASSET_58: filled with mean 359.06\n",
      "  ASSET_59: filled with mean 198.19\n",
      "  ASSET_60: filled with mean 194.46\n",
      "  ASSET_61: filled with mean 222.99\n",
      "  ASSET_62: filled with mean 280.99\n",
      "  ASSET_63: filled with mean 381.92\n",
      "  ASSET_64: filled with mean 93.20\n",
      "  ASSET_65: filled with mean 474.08\n",
      "  ASSET_66: filled with mean 191.03\n",
      "  ASSET_67: filled with mean 131.95\n",
      "  ASSET_68: filled with mean 135.34\n",
      "  ASSET_69: filled with mean 184.51\n",
      "  ASSET_70: filled with mean 246.11\n",
      "  ASSET_71: filled with mean 208.31\n",
      "  ASSET_72: filled with mean 193.15\n",
      "  ASSET_73: filled with mean 153.10\n",
      "  ASSET_74: filled with mean 136.53\n",
      "  ASSET_75: filled with mean 524.36\n",
      "  ASSET_76: filled with mean 105.31\n",
      "  ASSET_77: filled with mean 255.25\n",
      "  ASSET_78: filled with mean 278.13\n",
      "  ASSET_79: filled with mean 258.94\n",
      "  ASSET_80: filled with mean 354.84\n",
      "  ASSET_81: filled with mean 243.16\n",
      "  ASSET_82: filled with mean 536.93\n",
      "  ASSET_83: filled with mean 284.82\n",
      "  ASSET_84: filled with mean 406.55\n",
      "  ASSET_85: filled with mean 71.09\n",
      "  ASSET_86: filled with mean 441.81\n",
      "  ASSET_87: filled with mean 155.80\n",
      "  ASSET_88: filled with mean 285.08\n",
      "  ASSET_89: filled with mean 81.76\n",
      "  ASSET_90: filled with mean 95.30\n",
      "  ASSET_91: filled with mean 121.86\n",
      "  ASSET_92: filled with mean 364.64\n",
      "  ASSET_93: filled with mean 256.72\n",
      "  ASSET_94: filled with mean 247.79\n",
      "  ASSET_95: filled with mean 245.04\n",
      "  ASSET_96: filled with mean 144.54\n",
      "  ASSET_97: filled with mean 239.11\n",
      "  ASSET_98: filled with mean 237.79\n",
      "  ASSET_99: filled with mean 355.14\n",
      "  ASSET_100: filled with mean 329.31\n",
      "  ASSET_101: filled with mean 298.57\n",
      "  ASSET_102: filled with mean 371.40\n",
      "  ASSET_103: filled with mean 322.81\n",
      "  ASSET_104: filled with mean 346.69\n",
      "  ASSET_105: filled with mean 109.60\n",
      "  ASSET_106: filled with mean 174.39\n",
      "  ASSET_107: filled with mean 481.39\n",
      "  ASSET_108: filled with mean 273.98\n",
      "  ASSET_109: filled with mean 134.82\n",
      "  ASSET_110: filled with mean 246.89\n",
      "  ASSET_111: filled with mean 267.08\n",
      "  ASSET_112: filled with mean 118.29\n",
      "  ASSET_113: filled with mean 182.06\n",
      "  ASSET_114: filled with mean 387.41\n",
      "  ASSET_115: filled with mean 126.46\n",
      "  ASSET_116: filled with mean 183.20\n",
      "  ASSET_117: filled with mean 149.48\n",
      "  ASSET_118: filled with mean 48.23\n",
      "  ASSET_119: filled with mean 270.99\n",
      "  ASSET_120: filled with mean 137.69\n",
      "  ASSET_121: filled with mean 173.20\n",
      "  ASSET_122: filled with mean 134.76\n",
      "  ASSET_123: filled with mean 128.55\n",
      "  ASSET_124: filled with mean 238.41\n",
      "  ASSET_125: filled with mean 238.19\n",
      "  ASSET_126: filled with mean 202.53\n",
      "  ASSET_127: filled with mean 126.88\n",
      "  ASSET_128: filled with mean 257.05\n",
      "  ASSET_129: filled with mean 655.93\n",
      "  ASSET_130: filled with mean 264.57\n",
      "  ASSET_131: filled with mean 122.43\n",
      "  ASSET_132: filled with mean 341.61\n",
      "  ASSET_133: filled with mean 182.92\n",
      "  ASSET_134: filled with mean 439.53\n",
      "  ASSET_135: filled with mean 187.76\n",
      "  ASSET_136: filled with mean 176.55\n",
      "  ASSET_137: filled with mean 318.95\n",
      "  ASSET_138: filled with mean 77.84\n",
      "  ASSET_139: filled with mean 264.52\n",
      "  ASSET_140: filled with mean 147.95\n",
      "  ASSET_141: filled with mean 467.32\n",
      "  ASSET_142: filled with mean 215.92\n",
      "  ASSET_143: filled with mean 269.28\n",
      "  ASSET_144: filled with mean 99.44\n",
      "  ASSET_145: filled with mean 283.75\n",
      "  ASSET_146: filled with mean 145.33\n",
      "  ASSET_147: filled with mean 180.06\n",
      "  ASSET_148: filled with mean 210.72\n",
      "  ASSET_149: filled with mean 175.61\n",
      "  ASSET_150: filled with mean 207.58\n",
      "  ASSET_151: filled with mean 200.07\n",
      "  ASSET_152: filled with mean 154.06\n",
      "  ASSET_153: filled with mean 143.47\n",
      "  ASSET_154: filled with mean 263.42\n",
      "  ASSET_155: filled with mean 141.86\n",
      "  ASSET_156: filled with mean 117.20\n",
      "  ASSET_157: filled with mean 182.61\n",
      "  ASSET_158: filled with mean 81.01\n",
      "  ASSET_159: filled with mean 321.89\n",
      "  ASSET_160: filled with mean 95.73\n",
      "  ASSET_161: filled with mean 145.35\n",
      "  ASSET_162: filled with mean 182.12\n",
      "  ASSET_163: filled with mean 225.92\n",
      "  ASSET_164: filled with mean 245.14\n",
      "  ASSET_165: filled with mean 180.61\n",
      "  ASSET_166: filled with mean 150.89\n",
      "  ASSET_167: filled with mean 211.26\n",
      "  ASSET_168: filled with mean 95.30\n",
      "  ASSET_169: filled with mean 310.17\n",
      "  ASSET_170: filled with mean 355.35\n",
      "  ASSET_171: filled with mean 108.31\n",
      "  ASSET_172: filled with mean 642.82\n",
      "  ASSET_173: filled with mean 218.94\n",
      "  ASSET_174: filled with mean 209.31\n",
      "  ASSET_175: filled with mean 231.74\n",
      "  ASSET_176: filled with mean 573.38\n",
      "  ASSET_177: filled with mean 283.44\n",
      "  ASSET_178: filled with mean 148.19\n",
      "  ASSET_179: filled with mean 184.70\n",
      "  ASSET_180: filled with mean 156.32\n",
      "  ASSET_181: filled with mean 189.57\n",
      "  ASSET_182: filled with mean 282.42\n",
      "  ASSET_183: filled with mean 185.53\n",
      "  ASSET_184: filled with mean 608.06\n",
      "  ASSET_185: filled with mean 212.45\n",
      "  ASSET_186: filled with mean 509.49\n",
      "  ASSET_187: filled with mean 324.76\n",
      "  ASSET_188: filled with mean 482.13\n",
      "  ASSET_189: filled with mean 161.58\n",
      "  ASSET_190: filled with mean 209.19\n",
      "  ASSET_191: filled with mean 167.46\n",
      "  ASSET_192: filled with mean 167.82\n",
      "  ASSET_193: filled with mean 213.47\n",
      "  ASSET_194: filled with mean 132.31\n",
      "  ASSET_195: filled with mean 390.07\n",
      "  ASSET_196: filled with mean 352.24\n",
      "  ASSET_197: filled with mean 75.92\n",
      "  ASSET_198: filled with mean 362.60\n",
      "  ASSET_199: filled with mean 142.14\n",
      "  ASSET_200: filled with mean 533.63\n",
      "\n",
      "ðŸ“ˆ IMPUTATION QUALITY EVALUATION: SIMPLE_MEAN\n",
      "============================================================\n",
      "  Data shapes - Missing: (700, 200), Original: (700, 200), Imputed: (700, 200)\n",
      "  Found 1000 missing values to evaluate\n",
      "  Extracted 1000 value pairs\n",
      "  MAE: 74.7459\n",
      "  RMSE: 123.8212\n",
      "  MAPE: 32.76%\n",
      "  Correlation: 0.7378\n",
      "\n",
      "ðŸ’° FINANCIAL INTERPRETATION:\n",
      "  Average price difference: $74.75\n",
      "  Typical error magnitude: $123.82\n",
      "  âŒ POOR: High imputation errors\n",
      "\n",
      "==================== SIMPLE MEDIAN ====================\n",
      "\n",
      "ðŸ“Š SIMPLE MEDIAN IMPUTATION\n",
      "========================================\n",
      "  ASSET_01: filled with median 178.79\n",
      "  ASSET_02: filled with median 127.70\n",
      "  ASSET_03: filled with median 280.95\n",
      "  ASSET_04: filled with median 275.36\n",
      "  ASSET_05: filled with median 50.16\n",
      "  ASSET_06: filled with median 209.03\n",
      "  ASSET_07: filled with median 139.87\n",
      "  ASSET_08: filled with median 235.79\n",
      "  ASSET_09: filled with median 350.02\n",
      "  ASSET_10: filled with median 217.75\n",
      "  ASSET_11: filled with median 509.98\n",
      "  ASSET_12: filled with median 64.82\n",
      "  ASSET_13: filled with median 189.95\n",
      "  ASSET_14: filled with median 231.22\n",
      "  ASSET_15: filled with median 207.09\n",
      "  ASSET_16: filled with median 344.55\n",
      "  ASSET_17: filled with median 174.57\n",
      "  ASSET_18: filled with median 265.41\n",
      "  ASSET_19: filled with median 163.34\n",
      "  ASSET_20: filled with median 320.49\n",
      "  ASSET_21: filled with median 189.69\n",
      "  ASSET_22: filled with median 315.96\n",
      "  ASSET_23: filled with median 159.72\n",
      "  ASSET_24: filled with median 239.97\n",
      "  ASSET_25: filled with median 318.34\n",
      "  ASSET_26: filled with median 151.61\n",
      "  ASSET_27: filled with median 211.90\n",
      "  ASSET_28: filled with median 480.25\n",
      "  ASSET_29: filled with median 226.26\n",
      "  ASSET_30: filled with median 138.50\n",
      "  ASSET_31: filled with median 208.47\n",
      "  ASSET_32: filled with median 68.25\n",
      "  ASSET_33: filled with median 198.54\n",
      "  ASSET_34: filled with median 162.10\n",
      "  ASSET_35: filled with median 386.49\n",
      "  ASSET_36: filled with median 563.32\n",
      "  ASSET_37: filled with median 240.42\n",
      "  ASSET_38: filled with median 137.18\n",
      "  ASSET_39: filled with median 176.23\n",
      "  ASSET_40: filled with median 87.44\n",
      "  ASSET_41: filled with median 96.75\n",
      "  ASSET_42: filled with median 126.35\n",
      "  ASSET_43: filled with median 157.41\n",
      "  ASSET_44: filled with median 118.03\n",
      "  ASSET_45: filled with median 275.16\n",
      "  ASSET_46: filled with median 231.78\n",
      "  ASSET_47: filled with median 327.44\n",
      "  ASSET_48: filled with median 158.10\n",
      "  ASSET_49: filled with median 117.51\n",
      "  ASSET_50: filled with median 623.96\n",
      "  ASSET_51: filled with median 168.99\n",
      "  ASSET_52: filled with median 267.56\n",
      "  ASSET_53: filled with median 117.78\n",
      "  ASSET_54: filled with median 294.24\n",
      "  ASSET_55: filled with median 254.61\n",
      "  ASSET_56: filled with median 314.39\n",
      "  ASSET_57: filled with median 172.16\n",
      "  ASSET_58: filled with median 349.24\n",
      "  ASSET_59: filled with median 189.04\n",
      "  ASSET_60: filled with median 192.62\n",
      "  ASSET_61: filled with median 176.72\n",
      "  ASSET_62: filled with median 266.38\n",
      "  ASSET_63: filled with median 383.98\n",
      "  ASSET_64: filled with median 82.47\n",
      "  ASSET_65: filled with median 470.59\n",
      "  ASSET_66: filled with median 183.37\n",
      "  ASSET_67: filled with median 109.66\n",
      "  ASSET_68: filled with median 130.82\n",
      "  ASSET_69: filled with median 163.95\n",
      "  ASSET_70: filled with median 253.40\n",
      "  ASSET_71: filled with median 190.62\n",
      "  ASSET_72: filled with median 149.55\n",
      "  ASSET_73: filled with median 165.24\n",
      "  ASSET_74: filled with median 137.83\n",
      "  ASSET_75: filled with median 549.10\n",
      "  ASSET_76: filled with median 105.14\n",
      "  ASSET_77: filled with median 224.24\n",
      "  ASSET_78: filled with median 311.84\n",
      "  ASSET_79: filled with median 225.48\n",
      "  ASSET_80: filled with median 258.04\n",
      "  ASSET_81: filled with median 241.44\n",
      "  ASSET_82: filled with median 397.99\n",
      "  ASSET_83: filled with median 277.04\n",
      "  ASSET_84: filled with median 338.35\n",
      "  ASSET_85: filled with median 71.28\n",
      "  ASSET_86: filled with median 388.56\n",
      "  ASSET_87: filled with median 148.88\n",
      "  ASSET_88: filled with median 300.86\n",
      "  ASSET_89: filled with median 75.31\n",
      "  ASSET_90: filled with median 91.16\n",
      "  ASSET_91: filled with median 98.96\n",
      "  ASSET_92: filled with median 367.72\n",
      "  ASSET_93: filled with median 241.58\n",
      "  ASSET_94: filled with median 253.67\n",
      "  ASSET_95: filled with median 251.66\n",
      "  ASSET_96: filled with median 145.75\n",
      "  ASSET_97: filled with median 250.63\n",
      "  ASSET_98: filled with median 220.88\n",
      "  ASSET_99: filled with median 249.30\n",
      "  ASSET_100: filled with median 316.02\n",
      "  ASSET_101: filled with median 303.75\n",
      "  ASSET_102: filled with median 355.83\n",
      "  ASSET_103: filled with median 306.99\n",
      "  ASSET_104: filled with median 244.46\n",
      "  ASSET_105: filled with median 108.21\n",
      "  ASSET_106: filled with median 167.24\n",
      "  ASSET_107: filled with median 479.85\n",
      "  ASSET_108: filled with median 284.05\n",
      "  ASSET_109: filled with median 126.31\n",
      "  ASSET_110: filled with median 262.62\n",
      "  ASSET_111: filled with median 271.74\n",
      "  ASSET_112: filled with median 102.21\n",
      "  ASSET_113: filled with median 169.82\n",
      "  ASSET_114: filled with median 332.86\n",
      "  ASSET_115: filled with median 133.70\n",
      "  ASSET_116: filled with median 164.46\n",
      "  ASSET_117: filled with median 133.73\n",
      "  ASSET_118: filled with median 49.38\n",
      "  ASSET_119: filled with median 257.55\n",
      "  ASSET_120: filled with median 138.61\n",
      "  ASSET_121: filled with median 173.84\n",
      "  ASSET_122: filled with median 129.53\n",
      "  ASSET_123: filled with median 124.97\n",
      "  ASSET_124: filled with median 243.74\n",
      "  ASSET_125: filled with median 205.95\n",
      "  ASSET_126: filled with median 220.13\n",
      "  ASSET_127: filled with median 143.28\n",
      "  ASSET_128: filled with median 249.62\n",
      "  ASSET_129: filled with median 733.29\n",
      "  ASSET_130: filled with median 220.78\n",
      "  ASSET_131: filled with median 136.85\n",
      "  ASSET_132: filled with median 356.90\n",
      "  ASSET_133: filled with median 174.74\n",
      "  ASSET_134: filled with median 521.94\n",
      "  ASSET_135: filled with median 189.82\n",
      "  ASSET_136: filled with median 193.37\n",
      "  ASSET_137: filled with median 312.45\n",
      "  ASSET_138: filled with median 77.84\n",
      "  ASSET_139: filled with median 286.96\n",
      "  ASSET_140: filled with median 146.83\n",
      "  ASSET_141: filled with median 495.47\n",
      "  ASSET_142: filled with median 191.45\n",
      "  ASSET_143: filled with median 282.90\n",
      "  ASSET_144: filled with median 91.08\n",
      "  ASSET_145: filled with median 230.88\n",
      "  ASSET_146: filled with median 143.53\n",
      "  ASSET_147: filled with median 134.88\n",
      "  ASSET_148: filled with median 206.45\n",
      "  ASSET_149: filled with median 151.16\n",
      "  ASSET_150: filled with median 184.85\n",
      "  ASSET_151: filled with median 238.51\n",
      "  ASSET_152: filled with median 153.45\n",
      "  ASSET_153: filled with median 125.33\n",
      "  ASSET_154: filled with median 262.90\n",
      "  ASSET_155: filled with median 130.39\n",
      "  ASSET_156: filled with median 96.38\n",
      "  ASSET_157: filled with median 185.86\n",
      "  ASSET_158: filled with median 80.31\n",
      "  ASSET_159: filled with median 323.19\n",
      "  ASSET_160: filled with median 88.01\n",
      "  ASSET_161: filled with median 146.92\n",
      "  ASSET_162: filled with median 151.26\n",
      "  ASSET_163: filled with median 218.45\n",
      "  ASSET_164: filled with median 225.95\n",
      "  ASSET_165: filled with median 187.26\n",
      "  ASSET_166: filled with median 139.36\n",
      "  ASSET_167: filled with median 188.72\n",
      "  ASSET_168: filled with median 87.66\n",
      "  ASSET_169: filled with median 210.51\n",
      "  ASSET_170: filled with median 317.64\n",
      "  ASSET_171: filled with median 102.91\n",
      "  ASSET_172: filled with median 735.89\n",
      "  ASSET_173: filled with median 202.72\n",
      "  ASSET_174: filled with median 208.75\n",
      "  ASSET_175: filled with median 161.06\n",
      "  ASSET_176: filled with median 648.03\n",
      "  ASSET_177: filled with median 266.12\n",
      "  ASSET_178: filled with median 139.85\n",
      "  ASSET_179: filled with median 155.39\n",
      "  ASSET_180: filled with median 110.53\n",
      "  ASSET_181: filled with median 181.12\n",
      "  ASSET_182: filled with median 262.39\n",
      "  ASSET_183: filled with median 178.33\n",
      "  ASSET_184: filled with median 598.87\n",
      "  ASSET_185: filled with median 205.22\n",
      "  ASSET_186: filled with median 310.49\n",
      "  ASSET_187: filled with median 290.27\n",
      "  ASSET_188: filled with median 451.23\n",
      "  ASSET_189: filled with median 160.30\n",
      "  ASSET_190: filled with median 206.41\n",
      "  ASSET_191: filled with median 157.81\n",
      "  ASSET_192: filled with median 163.17\n",
      "  ASSET_193: filled with median 173.85\n",
      "  ASSET_194: filled with median 130.93\n",
      "  ASSET_195: filled with median 377.91\n",
      "  ASSET_196: filled with median 386.28\n",
      "  ASSET_197: filled with median 81.98\n",
      "  ASSET_198: filled with median 367.80\n",
      "  ASSET_199: filled with median 131.76\n",
      "  ASSET_200: filled with median 525.74\n",
      "\n",
      "ðŸ“ˆ IMPUTATION QUALITY EVALUATION: SIMPLE_MEDIAN\n",
      "============================================================\n",
      "  Data shapes - Missing: (700, 200), Original: (700, 200), Imputed: (700, 200)\n",
      "  Found 1000 missing values to evaluate\n",
      "  Extracted 1000 value pairs\n",
      "  MAE: 72.1954\n",
      "  RMSE: 130.8157\n",
      "  MAPE: 29.29%\n",
      "  Correlation: 0.7056\n",
      "\n",
      "ðŸ’° FINANCIAL INTERPRETATION:\n",
      "  Average price difference: $72.20\n",
      "  Typical error magnitude: $130.82\n",
      "  âŒ POOR: High imputation errors\n",
      "\n",
      "==================== ROLLING MEAN ====================\n",
      "\n",
      "ðŸ“Š ROLLING MEAN IMPUTATION (Window: 30)\n",
      "==================================================\n",
      "  ASSET_01: filled 3 values with rolling mean\n",
      "  ASSET_02: filled 5 values with rolling mean\n",
      "  ASSET_03: filled 4 values with rolling mean\n",
      "  ASSET_04: filled 5 values with rolling mean\n",
      "  ASSET_05: filled 9 values with rolling mean\n",
      "  ASSET_06: filled 6 values with rolling mean\n",
      "  ASSET_07: filled 5 values with rolling mean\n",
      "  ASSET_08: filled 4 values with rolling mean\n",
      "  ASSET_09: filled 7 values with rolling mean\n",
      "  ASSET_10: filled 6 values with rolling mean\n",
      "  ASSET_11: filled 5 values with rolling mean\n",
      "  ASSET_12: filled 3 values with rolling mean\n",
      "  ASSET_13: filled 4 values with rolling mean\n",
      "  ASSET_14: filled 2 values with rolling mean\n",
      "  ASSET_15: filled 3 values with rolling mean\n",
      "  ASSET_16: filled 2 values with rolling mean\n",
      "  ASSET_17: filled 3 values with rolling mean\n",
      "  ASSET_18: filled 6 values with rolling mean\n",
      "  ASSET_19: filled 3 values with rolling mean\n",
      "  ASSET_20: filled 5 values with rolling mean\n",
      "  ASSET_21: filled 4 values with rolling mean\n",
      "  ASSET_22: filled 1 values with rolling mean\n",
      "  ASSET_23: filled 4 values with rolling mean\n",
      "  ASSET_24: filled 4 values with rolling mean\n",
      "  ASSET_25: filled 3 values with rolling mean\n",
      "  ASSET_26: filled 5 values with rolling mean\n",
      "  ASSET_27: filled 4 values with rolling mean\n",
      "  ASSET_28: filled 5 values with rolling mean\n",
      "  ASSET_29: filled 3 values with rolling mean\n",
      "  ASSET_30: filled 7 values with rolling mean\n",
      "  ASSET_31: filled 7 values with rolling mean\n",
      "  ASSET_32: filled 5 values with rolling mean\n",
      "  ASSET_33: filled 4 values with rolling mean\n",
      "  ASSET_34: filled 7 values with rolling mean\n",
      "  ASSET_35: filled 3 values with rolling mean\n",
      "  ASSET_36: filled 7 values with rolling mean\n",
      "  ASSET_37: filled 4 values with rolling mean\n",
      "  ASSET_38: filled 4 values with rolling mean\n",
      "  ASSET_39: filled 6 values with rolling mean\n",
      "  ASSET_40: filled 7 values with rolling mean\n",
      "  ASSET_41: filled 2 values with rolling mean\n",
      "  ASSET_42: filled 9 values with rolling mean\n",
      "  ASSET_43: filled 8 values with rolling mean\n",
      "  ASSET_44: filled 5 values with rolling mean\n",
      "  ASSET_45: filled 5 values with rolling mean\n",
      "  ASSET_46: filled 3 values with rolling mean\n",
      "  ASSET_47: filled 2 values with rolling mean\n",
      "  ASSET_48: filled 7 values with rolling mean\n",
      "  ASSET_49: filled 3 values with rolling mean\n",
      "  ASSET_50: filled 6 values with rolling mean\n",
      "  ASSET_51: filled 5 values with rolling mean\n",
      "  ASSET_52: filled 6 values with rolling mean\n",
      "  ASSET_53: filled 2 values with rolling mean\n",
      "  ASSET_54: filled 6 values with rolling mean\n",
      "  ASSET_55: filled 7 values with rolling mean\n",
      "  ASSET_56: filled 12 values with rolling mean\n",
      "  ASSET_57: filled 6 values with rolling mean\n",
      "  ASSET_58: filled 5 values with rolling mean\n",
      "  ASSET_59: filled 3 values with rolling mean\n",
      "  ASSET_60: filled 4 values with rolling mean\n",
      "  ASSET_61: filled 2 values with rolling mean\n",
      "  ASSET_62: filled 4 values with rolling mean\n",
      "  ASSET_63: filled 6 values with rolling mean\n",
      "  ASSET_64: filled 4 values with rolling mean\n",
      "  ASSET_65: filled 2 values with rolling mean\n",
      "  ASSET_66: filled 3 values with rolling mean\n",
      "  ASSET_67: filled 4 values with rolling mean\n",
      "  ASSET_68: filled 5 values with rolling mean\n",
      "  ASSET_69: filled 6 values with rolling mean\n",
      "  ASSET_70: filled 2 values with rolling mean\n",
      "  ASSET_71: filled 2 values with rolling mean\n",
      "  ASSET_72: filled 5 values with rolling mean\n",
      "  ASSET_73: filled 6 values with rolling mean\n",
      "  ASSET_74: filled 0 values with rolling mean\n",
      "  ASSET_75: filled 1 values with rolling mean\n",
      "  ASSET_76: filled 4 values with rolling mean\n",
      "  ASSET_77: filled 9 values with rolling mean\n",
      "  ASSET_78: filled 7 values with rolling mean\n",
      "  ASSET_79: filled 5 values with rolling mean\n",
      "  ASSET_80: filled 1 values with rolling mean\n",
      "  ASSET_81: filled 8 values with rolling mean\n",
      "  ASSET_82: filled 7 values with rolling mean\n",
      "  ASSET_83: filled 5 values with rolling mean\n",
      "  ASSET_84: filled 5 values with rolling mean\n",
      "  ASSET_85: filled 8 values with rolling mean\n",
      "  ASSET_86: filled 6 values with rolling mean\n",
      "  ASSET_87: filled 6 values with rolling mean\n",
      "  ASSET_88: filled 8 values with rolling mean\n",
      "  ASSET_89: filled 8 values with rolling mean\n",
      "  ASSET_90: filled 2 values with rolling mean\n",
      "  ASSET_91: filled 7 values with rolling mean\n",
      "  ASSET_92: filled 3 values with rolling mean\n",
      "  ASSET_93: filled 4 values with rolling mean\n",
      "  ASSET_94: filled 7 values with rolling mean\n",
      "  ASSET_95: filled 3 values with rolling mean\n",
      "  ASSET_96: filled 5 values with rolling mean\n",
      "  ASSET_97: filled 4 values with rolling mean\n",
      "  ASSET_98: filled 5 values with rolling mean\n",
      "  ASSET_99: filled 4 values with rolling mean\n",
      "  ASSET_100: filled 4 values with rolling mean\n",
      "  ASSET_101: filled 2 values with rolling mean\n",
      "  ASSET_102: filled 7 values with rolling mean\n",
      "  ASSET_103: filled 4 values with rolling mean\n",
      "  ASSET_104: filled 9 values with rolling mean\n",
      "  ASSET_105: filled 7 values with rolling mean\n",
      "  ASSET_106: filled 5 values with rolling mean\n",
      "  ASSET_107: filled 4 values with rolling mean\n",
      "  ASSET_108: filled 4 values with rolling mean\n",
      "  ASSET_109: filled 6 values with rolling mean\n",
      "  ASSET_110: filled 6 values with rolling mean\n",
      "  ASSET_111: filled 6 values with rolling mean\n",
      "  ASSET_112: filled 3 values with rolling mean\n",
      "  ASSET_113: filled 2 values with rolling mean\n",
      "  ASSET_114: filled 9 values with rolling mean\n",
      "  ASSET_115: filled 3 values with rolling mean\n",
      "  ASSET_116: filled 3 values with rolling mean\n",
      "  ASSET_117: filled 5 values with rolling mean\n",
      "  ASSET_118: filled 7 values with rolling mean\n",
      "  ASSET_119: filled 5 values with rolling mean\n",
      "  ASSET_120: filled 7 values with rolling mean\n",
      "  ASSET_121: filled 8 values with rolling mean\n",
      "  ASSET_122: filled 7 values with rolling mean\n",
      "  ASSET_123: filled 6 values with rolling mean\n",
      "  ASSET_124: filled 5 values with rolling mean\n",
      "  ASSET_125: filled 7 values with rolling mean\n",
      "  ASSET_126: filled 7 values with rolling mean\n",
      "  ASSET_127: filled 4 values with rolling mean\n",
      "  ASSET_128: filled 2 values with rolling mean\n",
      "  ASSET_129: filled 6 values with rolling mean\n",
      "  ASSET_130: filled 8 values with rolling mean\n",
      "  ASSET_131: filled 3 values with rolling mean\n",
      "  ASSET_132: filled 4 values with rolling mean\n",
      "  ASSET_133: filled 10 values with rolling mean\n",
      "  ASSET_134: filled 7 values with rolling mean\n",
      "  ASSET_135: filled 5 values with rolling mean\n",
      "  ASSET_136: filled 8 values with rolling mean\n",
      "  ASSET_137: filled 3 values with rolling mean\n",
      "  ASSET_138: filled 2 values with rolling mean\n",
      "  ASSET_139: filled 2 values with rolling mean\n",
      "  ASSET_140: filled 2 values with rolling mean\n",
      "  ASSET_141: filled 9 values with rolling mean\n",
      "  ASSET_142: filled 7 values with rolling mean\n",
      "  ASSET_143: filled 5 values with rolling mean\n",
      "  ASSET_144: filled 6 values with rolling mean\n",
      "  ASSET_145: filled 2 values with rolling mean\n",
      "  ASSET_146: filled 5 values with rolling mean\n",
      "  ASSET_147: filled 5 values with rolling mean\n",
      "  ASSET_148: filled 5 values with rolling mean\n",
      "  ASSET_149: filled 11 values with rolling mean\n",
      "  ASSET_150: filled 11 values with rolling mean\n",
      "  ASSET_151: filled 5 values with rolling mean\n",
      "  ASSET_152: filled 8 values with rolling mean\n",
      "  ASSET_153: filled 2 values with rolling mean\n",
      "  ASSET_154: filled 6 values with rolling mean\n",
      "  ASSET_155: filled 5 values with rolling mean\n",
      "  ASSET_156: filled 6 values with rolling mean\n",
      "  ASSET_157: filled 4 values with rolling mean\n",
      "  ASSET_158: filled 5 values with rolling mean\n",
      "  ASSET_159: filled 2 values with rolling mean\n",
      "  ASSET_160: filled 5 values with rolling mean\n",
      "  ASSET_161: filled 6 values with rolling mean\n",
      "  ASSET_162: filled 4 values with rolling mean\n",
      "  ASSET_163: filled 4 values with rolling mean\n",
      "  ASSET_164: filled 5 values with rolling mean\n",
      "  ASSET_165: filled 3 values with rolling mean\n",
      "  ASSET_166: filled 4 values with rolling mean\n",
      "  ASSET_167: filled 3 values with rolling mean\n",
      "  ASSET_168: filled 3 values with rolling mean\n",
      "  ASSET_169: filled 6 values with rolling mean\n",
      "  ASSET_170: filled 4 values with rolling mean\n",
      "  ASSET_171: filled 4 values with rolling mean\n",
      "  ASSET_172: filled 2 values with rolling mean\n",
      "  ASSET_173: filled 3 values with rolling mean\n",
      "  ASSET_174: filled 5 values with rolling mean\n",
      "  ASSET_175: filled 6 values with rolling mean\n",
      "  ASSET_176: filled 3 values with rolling mean\n",
      "  ASSET_177: filled 6 values with rolling mean\n",
      "  ASSET_178: filled 3 values with rolling mean\n",
      "  ASSET_179: filled 3 values with rolling mean\n",
      "  ASSET_180: filled 6 values with rolling mean\n",
      "  ASSET_181: filled 8 values with rolling mean\n",
      "  ASSET_182: filled 3 values with rolling mean\n",
      "  ASSET_183: filled 7 values with rolling mean\n",
      "  ASSET_184: filled 6 values with rolling mean\n",
      "  ASSET_185: filled 4 values with rolling mean\n",
      "  ASSET_186: filled 5 values with rolling mean\n",
      "  ASSET_187: filled 8 values with rolling mean\n",
      "  ASSET_188: filled 9 values with rolling mean\n",
      "  ASSET_189: filled 12 values with rolling mean\n",
      "  ASSET_190: filled 3 values with rolling mean\n",
      "  ASSET_191: filled 1 values with rolling mean\n",
      "  ASSET_192: filled 9 values with rolling mean\n",
      "  ASSET_193: filled 10 values with rolling mean\n",
      "  ASSET_194: filled 2 values with rolling mean\n",
      "  ASSET_195: filled 7 values with rolling mean\n",
      "  ASSET_196: filled 6 values with rolling mean\n",
      "  ASSET_197: filled 2 values with rolling mean\n",
      "  ASSET_198: filled 5 values with rolling mean\n",
      "  ASSET_199: filled 1 values with rolling mean\n",
      "  ASSET_200: filled 8 values with rolling mean\n",
      "\n",
      "ðŸ“ˆ IMPUTATION QUALITY EVALUATION: ROLLING_MEAN\n",
      "============================================================\n",
      "  Data shapes - Missing: (700, 200), Original: (700, 200), Imputed: (700, 200)\n",
      "  Found 1000 missing values to evaluate\n",
      "  Extracted 1000 value pairs\n",
      "  MAE: 6.7238\n",
      "  RMSE: 10.2029\n",
      "  MAPE: 2.78%\n",
      "  Correlation: 0.9985\n",
      "\n",
      "ðŸ’° FINANCIAL INTERPRETATION:\n",
      "  Average price difference: $6.72\n",
      "  Typical error magnitude: $10.20\n",
      "  âœ… EXCELLENT: Very accurate imputation\n",
      "\n",
      "==================== ROLLING MEDIAN ====================\n",
      "\n",
      "ðŸ“Š ROLLING MEDIAN IMPUTATION (Window: 30)\n",
      "==================================================\n",
      "  ASSET_01: filled 3 values with rolling median\n",
      "  ASSET_02: filled 5 values with rolling median\n",
      "  ASSET_03: filled 4 values with rolling median\n",
      "  ASSET_04: filled 5 values with rolling median\n",
      "  ASSET_05: filled 9 values with rolling median\n",
      "  ASSET_06: filled 6 values with rolling median\n",
      "  ASSET_07: filled 5 values with rolling median\n",
      "  ASSET_08: filled 4 values with rolling median\n",
      "  ASSET_09: filled 7 values with rolling median\n",
      "  ASSET_10: filled 6 values with rolling median\n",
      "  ASSET_11: filled 5 values with rolling median\n",
      "  ASSET_12: filled 3 values with rolling median\n",
      "  ASSET_13: filled 4 values with rolling median\n",
      "  ASSET_14: filled 2 values with rolling median\n",
      "  ASSET_15: filled 3 values with rolling median\n",
      "  ASSET_16: filled 2 values with rolling median\n",
      "  ASSET_17: filled 3 values with rolling median\n",
      "  ASSET_18: filled 6 values with rolling median\n",
      "  ASSET_19: filled 3 values with rolling median\n",
      "  ASSET_20: filled 5 values with rolling median\n",
      "  ASSET_21: filled 4 values with rolling median\n",
      "  ASSET_22: filled 1 values with rolling median\n",
      "  ASSET_23: filled 4 values with rolling median\n",
      "  ASSET_24: filled 4 values with rolling median\n",
      "  ASSET_25: filled 3 values with rolling median\n",
      "  ASSET_26: filled 5 values with rolling median\n",
      "  ASSET_27: filled 4 values with rolling median\n",
      "  ASSET_28: filled 5 values with rolling median\n",
      "  ASSET_29: filled 3 values with rolling median\n",
      "  ASSET_30: filled 7 values with rolling median\n",
      "  ASSET_31: filled 7 values with rolling median\n",
      "  ASSET_32: filled 5 values with rolling median\n",
      "  ASSET_33: filled 4 values with rolling median\n",
      "  ASSET_34: filled 7 values with rolling median\n",
      "  ASSET_35: filled 3 values with rolling median\n",
      "  ASSET_36: filled 7 values with rolling median\n",
      "  ASSET_37: filled 4 values with rolling median\n",
      "  ASSET_38: filled 4 values with rolling median\n",
      "  ASSET_39: filled 6 values with rolling median\n",
      "  ASSET_40: filled 7 values with rolling median\n",
      "  ASSET_41: filled 2 values with rolling median\n",
      "  ASSET_42: filled 9 values with rolling median\n",
      "  ASSET_43: filled 8 values with rolling median\n",
      "  ASSET_44: filled 5 values with rolling median\n",
      "  ASSET_45: filled 5 values with rolling median\n",
      "  ASSET_46: filled 3 values with rolling median\n",
      "  ASSET_47: filled 2 values with rolling median\n",
      "  ASSET_48: filled 7 values with rolling median\n",
      "  ASSET_49: filled 3 values with rolling median\n",
      "  ASSET_50: filled 6 values with rolling median\n",
      "  ASSET_51: filled 5 values with rolling median\n",
      "  ASSET_52: filled 6 values with rolling median\n",
      "  ASSET_53: filled 2 values with rolling median\n",
      "  ASSET_54: filled 6 values with rolling median\n",
      "  ASSET_55: filled 7 values with rolling median\n",
      "  ASSET_56: filled 12 values with rolling median\n",
      "  ASSET_57: filled 6 values with rolling median\n",
      "  ASSET_58: filled 5 values with rolling median\n",
      "  ASSET_59: filled 3 values with rolling median\n",
      "  ASSET_60: filled 4 values with rolling median\n",
      "  ASSET_61: filled 2 values with rolling median\n",
      "  ASSET_62: filled 4 values with rolling median\n",
      "  ASSET_63: filled 6 values with rolling median\n",
      "  ASSET_64: filled 4 values with rolling median\n",
      "  ASSET_65: filled 2 values with rolling median\n",
      "  ASSET_66: filled 3 values with rolling median\n",
      "  ASSET_67: filled 4 values with rolling median\n",
      "  ASSET_68: filled 5 values with rolling median\n",
      "  ASSET_69: filled 6 values with rolling median\n",
      "  ASSET_70: filled 2 values with rolling median\n",
      "  ASSET_71: filled 2 values with rolling median\n",
      "  ASSET_72: filled 5 values with rolling median\n",
      "  ASSET_73: filled 6 values with rolling median\n",
      "  ASSET_74: filled 0 values with rolling median\n",
      "  ASSET_75: filled 1 values with rolling median\n",
      "  ASSET_76: filled 4 values with rolling median\n",
      "  ASSET_77: filled 9 values with rolling median\n",
      "  ASSET_78: filled 7 values with rolling median\n",
      "  ASSET_79: filled 5 values with rolling median\n",
      "  ASSET_80: filled 1 values with rolling median\n",
      "  ASSET_81: filled 8 values with rolling median\n",
      "  ASSET_82: filled 7 values with rolling median\n",
      "  ASSET_83: filled 5 values with rolling median\n",
      "  ASSET_84: filled 5 values with rolling median\n",
      "  ASSET_85: filled 8 values with rolling median\n",
      "  ASSET_86: filled 6 values with rolling median\n",
      "  ASSET_87: filled 6 values with rolling median\n",
      "  ASSET_88: filled 8 values with rolling median\n",
      "  ASSET_89: filled 8 values with rolling median\n",
      "  ASSET_90: filled 2 values with rolling median\n",
      "  ASSET_91: filled 7 values with rolling median\n",
      "  ASSET_92: filled 3 values with rolling median\n",
      "  ASSET_93: filled 4 values with rolling median\n",
      "  ASSET_94: filled 7 values with rolling median\n",
      "  ASSET_95: filled 3 values with rolling median\n",
      "  ASSET_96: filled 5 values with rolling median\n",
      "  ASSET_97: filled 4 values with rolling median\n",
      "  ASSET_98: filled 5 values with rolling median\n",
      "  ASSET_99: filled 4 values with rolling median\n",
      "  ASSET_100: filled 4 values with rolling median\n",
      "  ASSET_101: filled 2 values with rolling median\n",
      "  ASSET_102: filled 7 values with rolling median\n",
      "  ASSET_103: filled 4 values with rolling median\n",
      "  ASSET_104: filled 9 values with rolling median\n",
      "  ASSET_105: filled 7 values with rolling median\n",
      "  ASSET_106: filled 5 values with rolling median\n",
      "  ASSET_107: filled 4 values with rolling median\n",
      "  ASSET_108: filled 4 values with rolling median\n",
      "  ASSET_109: filled 6 values with rolling median\n",
      "  ASSET_110: filled 6 values with rolling median\n",
      "  ASSET_111: filled 6 values with rolling median\n",
      "  ASSET_112: filled 3 values with rolling median\n",
      "  ASSET_113: filled 2 values with rolling median\n",
      "  ASSET_114: filled 9 values with rolling median\n",
      "  ASSET_115: filled 3 values with rolling median\n",
      "  ASSET_116: filled 3 values with rolling median\n",
      "  ASSET_117: filled 5 values with rolling median\n",
      "  ASSET_118: filled 7 values with rolling median\n",
      "  ASSET_119: filled 5 values with rolling median\n",
      "  ASSET_120: filled 7 values with rolling median\n",
      "  ASSET_121: filled 8 values with rolling median\n",
      "  ASSET_122: filled 7 values with rolling median\n",
      "  ASSET_123: filled 6 values with rolling median\n",
      "  ASSET_124: filled 5 values with rolling median\n",
      "  ASSET_125: filled 7 values with rolling median\n",
      "  ASSET_126: filled 7 values with rolling median\n",
      "  ASSET_127: filled 4 values with rolling median\n",
      "  ASSET_128: filled 2 values with rolling median\n",
      "  ASSET_129: filled 6 values with rolling median\n",
      "  ASSET_130: filled 8 values with rolling median\n",
      "  ASSET_131: filled 3 values with rolling median\n",
      "  ASSET_132: filled 4 values with rolling median\n",
      "  ASSET_133: filled 10 values with rolling median\n",
      "  ASSET_134: filled 7 values with rolling median\n",
      "  ASSET_135: filled 5 values with rolling median\n",
      "  ASSET_136: filled 8 values with rolling median\n",
      "  ASSET_137: filled 3 values with rolling median\n",
      "  ASSET_138: filled 2 values with rolling median\n",
      "  ASSET_139: filled 2 values with rolling median\n",
      "  ASSET_140: filled 2 values with rolling median\n",
      "  ASSET_141: filled 9 values with rolling median\n",
      "  ASSET_142: filled 7 values with rolling median\n",
      "  ASSET_143: filled 5 values with rolling median\n",
      "  ASSET_144: filled 6 values with rolling median\n",
      "  ASSET_145: filled 2 values with rolling median\n",
      "  ASSET_146: filled 5 values with rolling median\n",
      "  ASSET_147: filled 5 values with rolling median\n",
      "  ASSET_148: filled 5 values with rolling median\n",
      "  ASSET_149: filled 11 values with rolling median\n",
      "  ASSET_150: filled 11 values with rolling median\n",
      "  ASSET_151: filled 5 values with rolling median\n",
      "  ASSET_152: filled 8 values with rolling median\n",
      "  ASSET_153: filled 2 values with rolling median\n",
      "  ASSET_154: filled 6 values with rolling median\n",
      "  ASSET_155: filled 5 values with rolling median\n",
      "  ASSET_156: filled 6 values with rolling median\n",
      "  ASSET_157: filled 4 values with rolling median\n",
      "  ASSET_158: filled 5 values with rolling median\n",
      "  ASSET_159: filled 2 values with rolling median\n",
      "  ASSET_160: filled 5 values with rolling median\n",
      "  ASSET_161: filled 6 values with rolling median\n",
      "  ASSET_162: filled 4 values with rolling median\n",
      "  ASSET_163: filled 4 values with rolling median\n",
      "  ASSET_164: filled 5 values with rolling median\n",
      "  ASSET_165: filled 3 values with rolling median\n",
      "  ASSET_166: filled 4 values with rolling median\n",
      "  ASSET_167: filled 3 values with rolling median\n",
      "  ASSET_168: filled 3 values with rolling median\n",
      "  ASSET_169: filled 6 values with rolling median\n",
      "  ASSET_170: filled 4 values with rolling median\n",
      "  ASSET_171: filled 4 values with rolling median\n",
      "  ASSET_172: filled 2 values with rolling median\n",
      "  ASSET_173: filled 3 values with rolling median\n",
      "  ASSET_174: filled 5 values with rolling median\n",
      "  ASSET_175: filled 6 values with rolling median\n",
      "  ASSET_176: filled 3 values with rolling median\n",
      "  ASSET_177: filled 6 values with rolling median\n",
      "  ASSET_178: filled 3 values with rolling median\n",
      "  ASSET_179: filled 3 values with rolling median\n",
      "  ASSET_180: filled 6 values with rolling median\n",
      "  ASSET_181: filled 8 values with rolling median\n",
      "  ASSET_182: filled 3 values with rolling median\n",
      "  ASSET_183: filled 7 values with rolling median\n",
      "  ASSET_184: filled 6 values with rolling median\n",
      "  ASSET_185: filled 4 values with rolling median\n",
      "  ASSET_186: filled 5 values with rolling median\n",
      "  ASSET_187: filled 8 values with rolling median\n",
      "  ASSET_188: filled 9 values with rolling median\n",
      "  ASSET_189: filled 12 values with rolling median\n",
      "  ASSET_190: filled 3 values with rolling median\n",
      "  ASSET_191: filled 1 values with rolling median\n",
      "  ASSET_192: filled 9 values with rolling median\n",
      "  ASSET_193: filled 10 values with rolling median\n",
      "  ASSET_194: filled 2 values with rolling median\n",
      "  ASSET_195: filled 7 values with rolling median\n",
      "  ASSET_196: filled 6 values with rolling median\n",
      "  ASSET_197: filled 2 values with rolling median\n",
      "  ASSET_198: filled 5 values with rolling median\n",
      "  ASSET_199: filled 1 values with rolling median\n",
      "  ASSET_200: filled 8 values with rolling median\n",
      "\n",
      "ðŸ“ˆ IMPUTATION QUALITY EVALUATION: ROLLING_MEDIAN\n",
      "============================================================\n",
      "  Data shapes - Missing: (700, 200), Original: (700, 200), Imputed: (700, 200)\n",
      "  Found 1000 missing values to evaluate\n",
      "  Extracted 1000 value pairs\n",
      "  MAE: 5.3549\n",
      "  RMSE: 8.8933\n",
      "  MAPE: 2.19%\n",
      "  Correlation: 0.9989\n",
      "\n",
      "ðŸ’° FINANCIAL INTERPRETATION:\n",
      "  Average price difference: $5.35\n",
      "  Typical error magnitude: $8.89\n",
      "  âœ… EXCELLENT: Very accurate imputation\n",
      "\n",
      "================================================================================\n",
      "ðŸ† METHOD COMPARISON SUMMARY\n",
      "================================================================================\n",
      "                    MAE      RMSE     MAPE  Correlation  N_Values\n",
      "simple_mean     74.7459  123.8212  32.7649       0.7378    1000.0\n",
      "simple_median   72.1954  130.8157  29.2926       0.7056    1000.0\n",
      "rolling_mean     6.7238   10.2029   2.7761       0.9985    1000.0\n",
      "rolling_median   5.3549    8.8933   2.1942       0.9989    1000.0\n",
      "\n",
      "ðŸ† BEST PERFORMING METHOD: Rolling Median\n",
      "   RMSE: 8.8933\n",
      "\n",
      "ðŸ’¡ RECOMMENDATIONS FOR FINANCIAL TIME SERIES:\n",
      "--------------------------------------------------\n",
      "ðŸ¥‡ BEST PRACTICE: Rolling Median\n",
      "   âœ… Adapts to local market conditions\n",
      "   âœ… Robust to price spikes/crashes\n",
      "   âœ… Preserves time series properties\n",
      "\n",
      "ðŸ¥ˆ ALTERNATIVE: Rolling Mean\n",
      "   âœ… Good for stable market periods\n",
      "   âš ï¸  Sensitive to outliers\n",
      "\n",
      "âš ï¸  AVOID: Simple Mean/Median\n",
      "   âŒ Ignores time series structure\n",
      "   âŒ Can create artificial patterns\n",
      "   âŒ Distorts volatility\n",
      "\n",
      "ðŸ”„ NEXT STEPS:\n",
      "   1. Try interpolation methods (linear, spline)\n",
      "   2. Consider forward/backward fill\n",
      "   3. Test advanced methods (KNN, MICE)\n",
      "            ASSET_01  ASSET_02  ASSET_03  ASSET_04  ASSET_05  ASSET_06  \\\n",
      "date                                                                     \n",
      "2024-01-01     95.10     66.54     97.23    139.55     77.47    154.16   \n",
      "2024-01-02     95.97     66.90     96.84    138.36     77.08    152.61   \n",
      "2024-01-03     95.20     65.96     97.85    138.24     76.84    152.71   \n",
      "2024-01-04     96.28     65.65     97.62    140.46     77.83    153.68   \n",
      "2024-01-05     97.92     65.25     96.69    140.20     77.65    152.64   \n",
      "2024-01-06     97.09     64.97     94.59    143.59     77.74    152.93   \n",
      "2024-01-07    108.48     64.06     96.09    142.73     77.26    152.08   \n",
      "2024-01-08    107.99     64.97     97.12    143.62     79.09    152.82   \n",
      "2024-01-09    105.88     65.73     98.52    141.01     74.47    152.81   \n",
      "2024-01-10    105.28     64.79     97.80    142.74     74.39    156.77   \n",
      "\n",
      "            ASSET_07  ASSET_08  ASSET_09  ASSET_10  ...  ASSET_191  ASSET_192  \\\n",
      "date                                                ...                         \n",
      "2024-01-01    92.680    111.98    144.21    187.20  ...     134.22     105.71   \n",
      "2024-01-02    92.970    111.21    140.78    181.79  ...     133.08     109.95   \n",
      "2024-01-03    92.090    111.73    140.23    180.57  ...     134.40     109.56   \n",
      "2024-01-04    90.660    110.11    137.40    180.99  ...     134.65     108.31   \n",
      "2024-01-05    91.965    110.72    134.15    186.70  ...     134.82     111.58   \n",
      "2024-01-06    87.180    112.33    132.93    185.16  ...     135.57     111.03   \n",
      "2024-01-07    87.360    110.76    134.13    182.42  ...     137.45     111.32   \n",
      "2024-01-08    87.200    110.20    134.98    186.96  ...     130.95     114.27   \n",
      "2024-01-09    87.520    110.28    133.57    189.21  ...     130.69     114.56   \n",
      "2024-01-10    91.360    109.86    134.76    187.25  ...     130.91     114.15   \n",
      "\n",
      "            ASSET_193  ASSET_194  ASSET_195  ASSET_196  ASSET_197  ASSET_198  \\\n",
      "date                                                                           \n",
      "2024-01-01     156.42      75.52     141.44     194.76      55.51     190.42   \n",
      "2024-01-02     157.41      75.10     140.66     195.82      56.74     187.70   \n",
      "2024-01-03     152.90      75.55     141.27     201.22      57.04     188.78   \n",
      "2024-01-04     152.11      74.65     142.09     201.45      57.86     190.06   \n",
      "2024-01-05     153.30      74.91     142.84     199.08      58.67     188.93   \n",
      "2024-01-06     154.73      75.13     141.43     199.26      58.00     182.50   \n",
      "2024-01-07     151.40      74.18     142.32     199.92      57.44     180.51   \n",
      "2024-01-08     150.27      74.02     144.32     200.68      58.05     181.05   \n",
      "2024-01-09     150.69      74.34     145.37     198.91      56.03     181.21   \n",
      "2024-01-10     149.88      73.13     143.53     197.09      64.85     187.17   \n",
      "\n",
      "            ASSET_199  ASSET_200  \n",
      "date                              \n",
      "2024-01-01     137.44     193.72  \n",
      "2024-01-02     135.51     189.04  \n",
      "2024-01-03     135.89     192.41  \n",
      "2024-01-04     135.13     193.76  \n",
      "2024-01-05     135.94     190.89  \n",
      "2024-01-06     139.48     190.22  \n",
      "2024-01-07     138.93     189.25  \n",
      "2024-01-08     139.65     188.73  \n",
      "2024-01-09     135.15     186.63  \n",
      "2024-01-10     135.81     184.77  \n",
      "\n",
      "[10 rows x 200 columns]\n",
      "Locations where values were imputed:\n",
      "Total missing positions: ASSET_01     3\n",
      "ASSET_02     5\n",
      "ASSET_03     4\n",
      "ASSET_04     5\n",
      "ASSET_05     9\n",
      "            ..\n",
      "ASSET_196    6\n",
      "ASSET_197    2\n",
      "ASSET_198    5\n",
      "ASSET_199    1\n",
      "ASSET_200    8\n",
      "Length: 200, dtype: int64\n",
      "\n",
      "Sample of imputed values (original NaN -> new value):\n",
      "Position (0, 169): NaN -> 119.21\n",
      "Position (2, 37): NaN -> 165.83\n",
      "Position (3, 81): NaN -> 199.42\n",
      "Position (4, 6): NaN -> 91.97\n",
      "Position (4, 86): NaN -> 185.00\n",
      "Position (4, 189): NaN -> 183.13\n",
      "Position (6, 118): NaN -> 155.22\n",
      "Position (7, 1): NaN -> 64.97\n",
      "Position (9, 107): NaN -> 134.73\n",
      "Position (10, 83): NaN -> 110.02\n"
     ]
    }
   ],
   "source": [
    "analyzer = FinancialImputationAnalyzer(financial_data_missing, df)\n",
    "results = analyzer.compare_all_methods(window=30)\n",
    "print(pd.DataFrame(results['Rolling Median']).head(10))\n",
    "# Show where imputation occurred\n",
    "missing_mask = np.isnan(financial_data_missing)\n",
    "print(\"Locations where values were imputed:\")\n",
    "print(f\"Total missing positions: {np.sum(missing_mask)}\")\n",
    "\n",
    "# Show some specific imputed values\n",
    "imputed_rolling_median = results['Rolling Median']\n",
    "print(\"\\nSample of imputed values (original NaN -> new value):\")\n",
    "for i in range(10):  # Show first 10 missing positions\n",
    "    row, col = np.where(missing_mask)\n",
    "    if i < len(row):\n",
    "        print(f\"Position ({row[i]}, {col[i]}): NaN -> {imputed_rolling_median.iloc[row[i], col[i]]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7def8108-3bcf-41ff-9daf-66dd8f0fa4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FORWARD/BACKWARD FILL ANALYZER READY!\n",
      "============================================================\n",
      "Simple and effective for financial time series gaps\n",
      "\n",
      "USAGE:\n",
      "analyzer = FinancialForwardBackwardFill(data_with_missing, original_data)\n",
      "results = analyzer.compare_all_methods()\n"
     ]
    }
   ],
   "source": [
    "class FinancialForwardBackwardFill:\n",
    "    \"\"\"\n",
    "    Forward/Backward Fill imputation for financial time series data.\n",
    "    LOCF = Last Observation Carried Forward\n",
    "    NOCB = Next Observation Carried Backward\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_with_missing, original_data=None):\n",
    "        \"\"\"\n",
    "        Initialize the forward/backward fill analyzer\n",
    "        \n",
    "        Parameters:\n",
    "        - data_with_missing: DataFrame or numpy array with missing values\n",
    "        - original_data: Original complete data for evaluation (optional)\n",
    "        \"\"\"\n",
    "        self.data_missing = data_with_missing.copy() if hasattr(data_with_missing, 'copy') else data_with_missing.copy()\n",
    "        self.original_data = original_data.copy() if original_data is not None else None\n",
    "        self.imputation_results = {}\n",
    "        self.performance_metrics = {}\n",
    "        \n",
    "    def analyze_missing_pattern(self):\n",
    "        \"\"\"Analyze missing pattern - critical for forward/backward fill\"\"\"\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            missing_info = self.data_missing.isnull()\n",
    "        else:\n",
    "            missing_info = pd.DataFrame(np.isnan(self.data_missing))\n",
    "        \n",
    "        print(\"Forward/Backward Fill - Missing Pattern Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        total_missing = missing_info.sum().sum()\n",
    "        total_cells = missing_info.shape[0] * missing_info.shape[1]\n",
    "        missing_pct = (total_missing / total_cells) * 100\n",
    "        \n",
    "        print(f\"Dataset shape: {missing_info.shape}\")\n",
    "        print(f\"Total missing values: {total_missing:,}\")\n",
    "        print(f\"Missing percentage: {missing_pct:.2f}%\")\n",
    "        \n",
    "        # Check for edge cases (first/last row missing)\n",
    "        first_row_missing = missing_info.iloc[0, :].sum()\n",
    "        last_row_missing = missing_info.iloc[-1, :].sum()\n",
    "        \n",
    "        print(f\"\\nEdge case analysis:\")\n",
    "        print(f\"  First row missing values: {first_row_missing}\")\n",
    "        print(f\"  Last row missing values: {last_row_missing}\")\n",
    "        \n",
    "        if first_row_missing > 0:\n",
    "            print(\"  WARNING: First row has missing values - forward fill will fail here\")\n",
    "        if last_row_missing > 0:\n",
    "            print(\"  WARNING: Last row has missing values - backward fill will fail here\")\n",
    "            \n",
    "        # Analyze consecutive gaps\n",
    "        max_consecutive_gaps = []\n",
    "        for col in range(missing_info.shape[1]):\n",
    "            col_missing = missing_info.iloc[:, col]\n",
    "            consecutive = 0\n",
    "            max_consecutive = 0\n",
    "            for val in col_missing:\n",
    "                if val:\n",
    "                    consecutive += 1\n",
    "                    max_consecutive = max(max_consecutive, consecutive)\n",
    "                else:\n",
    "                    consecutive = 0\n",
    "            max_consecutive_gaps.append(max_consecutive)\n",
    "        \n",
    "        overall_max_gap = max(max_consecutive_gaps) if max_consecutive_gaps else 0\n",
    "        print(f\"  Maximum consecutive gap: {overall_max_gap} periods\")\n",
    "        \n",
    "        if overall_max_gap > 10:\n",
    "            print(\"  WARNING: Long consecutive gaps detected!\")\n",
    "            print(\"  Consider interpolation for gaps > 10 periods\")\n",
    "        else:\n",
    "            print(\"  GOOD: Gap length suitable for forward/backward fill\")\n",
    "            \n",
    "        return {\n",
    "            'total_missing': total_missing,\n",
    "            'missing_pct': missing_pct,\n",
    "            'first_row_missing': first_row_missing,\n",
    "            'last_row_missing': last_row_missing,\n",
    "            'max_gap': overall_max_gap\n",
    "        }\n",
    "    \n",
    "    def forward_fill_locf(self):\n",
    "        \"\"\"\n",
    "        Forward Fill (LOCF) - Last Observation Carried Forward\n",
    "        Uses previous valid value to fill missing data\n",
    "        \"\"\"\n",
    "        print(\"\\nFORWARD FILL (LOCF) - Last Observation Carried Forward\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            \n",
    "            # Forward fill each column\n",
    "            for col in imputed_data.columns:\n",
    "                before_count = imputed_data[col].isnull().sum()\n",
    "                imputed_data[col] = imputed_data[col].fillna(method='ffill')\n",
    "                after_count = imputed_data[col].isnull().sum()\n",
    "                filled_count = before_count - after_count\n",
    "                print(f\"  {col}: filled {filled_count} values via forward fill\")\n",
    "                \n",
    "                if after_count > 0:\n",
    "                    print(f\"    WARNING: {after_count} values still missing (no prior value available)\")\n",
    "        \n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            rows, cols = imputed_data.shape\n",
    "            \n",
    "            for col in range(cols):\n",
    "                filled_count = 0\n",
    "                remaining_missing = 0\n",
    "                \n",
    "                # Forward fill column by column\n",
    "                for row in range(1, rows):  # Start from row 1\n",
    "                    if np.isnan(imputed_data[row, col]):\n",
    "                        if not np.isnan(imputed_data[row-1, col]):\n",
    "                            # Fill with previous value\n",
    "                            imputed_data[row, col] = imputed_data[row-1, col]\n",
    "                            filled_count += 1\n",
    "                        else:\n",
    "                            remaining_missing += 1\n",
    "                \n",
    "                if col < 5:  # Print first 5 for brevity\n",
    "                    print(f\"  Asset {col+1:03d}: filled {filled_count} values\")\n",
    "                    if remaining_missing > 0:\n",
    "                        print(f\"    WARNING: {remaining_missing} values still missing\")\n",
    "            \n",
    "            if cols > 5:\n",
    "                print(f\"  ... processed {cols-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['forward_fill'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def backward_fill_nocb(self):\n",
    "        \"\"\"\n",
    "        Backward Fill (NOCB) - Next Observation Carried Backward\n",
    "        Uses next valid value to fill missing data\n",
    "        \"\"\"\n",
    "        print(\"\\nBACKWARD FILL (NOCB) - Next Observation Carried Backward\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            \n",
    "            # Backward fill each column\n",
    "            for col in imputed_data.columns:\n",
    "                before_count = imputed_data[col].isnull().sum()\n",
    "                imputed_data[col] = imputed_data[col].fillna(method='bfill')\n",
    "                after_count = imputed_data[col].isnull().sum()\n",
    "                filled_count = before_count - after_count\n",
    "                print(f\"  {col}: filled {filled_count} values via backward fill\")\n",
    "                \n",
    "                if after_count > 0:\n",
    "                    print(f\"    WARNING: {after_count} values still missing (no future value available)\")\n",
    "        \n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            rows, cols = imputed_data.shape\n",
    "            \n",
    "            for col in range(cols):\n",
    "                filled_count = 0\n",
    "                remaining_missing = 0\n",
    "                \n",
    "                # Backward fill column by column (go backwards)\n",
    "                for row in range(rows-2, -1, -1):  # Start from second-to-last row, go to row 0\n",
    "                    if np.isnan(imputed_data[row, col]):\n",
    "                        if not np.isnan(imputed_data[row+1, col]):\n",
    "                            # Fill with next value\n",
    "                            imputed_data[row, col] = imputed_data[row+1, col]\n",
    "                            filled_count += 1\n",
    "                        else:\n",
    "                            remaining_missing += 1\n",
    "                \n",
    "                if col < 5:  # Print first 5 for brevity\n",
    "                    print(f\"  Asset {col+1:03d}: filled {filled_count} values\")\n",
    "                    if remaining_missing > 0:\n",
    "                        print(f\"    WARNING: {remaining_missing} values still missing\")\n",
    "            \n",
    "            if cols > 5:\n",
    "                print(f\"  ... processed {cols-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['backward_fill'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def combined_fill(self):\n",
    "        \"\"\"\n",
    "        Combined Forward-Backward Fill\n",
    "        1. First apply forward fill\n",
    "        2. Then apply backward fill to remaining gaps\n",
    "        \"\"\"\n",
    "        print(\"\\nCOMBINED FORWARD-BACKWARD FILL\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Start with forward fill\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            \n",
    "            # Step 1: Forward fill\n",
    "            imputed_data = imputed_data.fillna(method='ffill')\n",
    "            # Step 2: Backward fill remaining\n",
    "            imputed_data = imputed_data.fillna(method='bfill')\n",
    "            \n",
    "            # Count what was filled\n",
    "            original_missing = self.data_missing.isnull().sum().sum()\n",
    "            final_missing = imputed_data.isnull().sum().sum()\n",
    "            filled_count = original_missing - final_missing\n",
    "            \n",
    "            print(f\"  Total values filled: {filled_count}\")\n",
    "            print(f\"  Remaining missing: {final_missing}\")\n",
    "        \n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            rows, cols = imputed_data.shape\n",
    "            \n",
    "            # Step 1: Forward fill\n",
    "            for col in range(cols):\n",
    "                for row in range(1, rows):\n",
    "                    if np.isnan(imputed_data[row, col]) and not np.isnan(imputed_data[row-1, col]):\n",
    "                        imputed_data[row, col] = imputed_data[row-1, col]\n",
    "            \n",
    "            # Step 2: Backward fill remaining\n",
    "            for col in range(cols):\n",
    "                for row in range(rows-2, -1, -1):\n",
    "                    if np.isnan(imputed_data[row, col]) and not np.isnan(imputed_data[row+1, col]):\n",
    "                        imputed_data[row, col] = imputed_data[row+1, col]\n",
    "            \n",
    "            original_missing = np.sum(np.isnan(self.data_missing))\n",
    "            final_missing = np.sum(np.isnan(imputed_data))\n",
    "            filled_count = original_missing - final_missing\n",
    "            \n",
    "            print(f\"  Total values filled: {filled_count}\")\n",
    "            print(f\"  Remaining missing: {final_missing}\")\n",
    "        \n",
    "        self.imputation_results['combined_fill'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def evaluate_imputation_quality(self, method_name, imputed_data):\n",
    "        \"\"\"\n",
    "        Evaluate fill quality - same as before but optimized for time series\n",
    "        \"\"\"\n",
    "        if self.original_data is None:\n",
    "            print(f\"\\nNo original data available for {method_name} evaluation\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nIMPUTATION QUALITY EVALUATION: {method_name.upper()}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "                \n",
    "            if hasattr(imputed_data, 'values'):\n",
    "                imputed_np = imputed_data.values.copy()\n",
    "            else:\n",
    "                imputed_np = imputed_data.copy()\n",
    "            \n",
    "            # Find missing positions\n",
    "            mask = np.isnan(missing_np)\n",
    "            total_missing = np.sum(mask)\n",
    "            \n",
    "            if total_missing == 0:\n",
    "                print(\"  No missing values found!\")\n",
    "                return None\n",
    "            \n",
    "            # Extract original and imputed values\n",
    "            orig_vals = []\n",
    "            imp_vals = []\n",
    "            \n",
    "            rows, cols = missing_np.shape\n",
    "            for i in range(rows):\n",
    "                for j in range(cols):\n",
    "                    if mask[i, j]:\n",
    "                        orig_vals.append(original_np[i, j])\n",
    "                        imp_vals.append(imputed_np[i, j])\n",
    "            \n",
    "            orig_vals = np.array(orig_vals)\n",
    "            imp_vals = np.array(imp_vals)\n",
    "            \n",
    "            print(f\"  Evaluated {len(orig_vals)} imputed values\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            differences = orig_vals - imp_vals\n",
    "            abs_differences = np.abs(differences)\n",
    "            \n",
    "            mae = np.mean(abs_differences)\n",
    "            rmse = np.sqrt(np.mean(differences**2))\n",
    "            \n",
    "            # MAPE calculation\n",
    "            nonzero_mask = orig_vals != 0\n",
    "            if np.sum(nonzero_mask) > 0:\n",
    "                mape = np.mean(abs_differences[nonzero_mask] / np.abs(orig_vals[nonzero_mask])) * 100\n",
    "            else:\n",
    "                mape = float('inf')\n",
    "            \n",
    "            # Correlation\n",
    "            if len(orig_vals) > 1:\n",
    "                corr = np.corrcoef(orig_vals, imp_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 1.0\n",
    "            \n",
    "            print(f\"  MAE: ${mae:.2f}\")\n",
    "            print(f\"  RMSE: ${rmse:.2f}\")\n",
    "            print(f\"  MAPE: {mape:.1f}%\" if np.isfinite(mape) else \"  MAPE: âˆž\")\n",
    "            print(f\"  Correlation: {corr:.3f}\")\n",
    "            \n",
    "            # Time series specific evaluation\n",
    "            print(f\"\\n  TIME SERIES EVALUATION:\")\n",
    "            \n",
    "            # Check for trend preservation\n",
    "            orig_trend = np.mean(np.diff(orig_vals))\n",
    "            imp_trend = np.mean(np.diff(imp_vals))\n",
    "            trend_error = abs(orig_trend - imp_trend)\n",
    "            \n",
    "            print(f\"  Original trend: {orig_trend:.3f}/period\")\n",
    "            print(f\"  Imputed trend: {imp_trend:.3f}/period\") \n",
    "            print(f\"  Trend preservation error: {trend_error:.3f}\")\n",
    "            \n",
    "            # Financial interpretation\n",
    "            if mape < 2:\n",
    "                print(\"  EXCELLENT: Very accurate for financial data\")\n",
    "            elif mape < 5:\n",
    "                print(\"  GOOD: Acceptable for most financial analysis\")\n",
    "            elif mape < 10:\n",
    "                print(\"  FAIR: Use with caution for risk calculations\")\n",
    "            else:\n",
    "                print(\"  POOR: High errors - consider other methods\")\n",
    "            \n",
    "            metrics = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'Correlation': corr,\n",
    "                'Trend_Error': trend_error,\n",
    "                'N_Values': len(orig_vals)\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics[method_name] = metrics\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compare_all_methods(self):\n",
    "        \"\"\"\n",
    "        Compare Forward Fill, Backward Fill, and Combined approaches\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FORWARD/BACKWARD FILL COMPREHENSIVE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Analyze missing pattern\n",
    "        pattern_info = self.analyze_missing_pattern()\n",
    "        \n",
    "        # Run all methods\n",
    "        print(f\"\\n{'='*25} FORWARD FILL {'='*25}\")\n",
    "        ff_result = self.forward_fill_locf()\n",
    "        self.evaluate_imputation_quality('forward_fill', ff_result)\n",
    "        \n",
    "        print(f\"\\n{'='*25} BACKWARD FILL {'='*24}\")\n",
    "        bf_result = self.backward_fill_nocb()\n",
    "        self.evaluate_imputation_quality('backward_fill', bf_result)\n",
    "        \n",
    "        print(f\"\\n{'='*23} COMBINED FILL {'='*23}\")\n",
    "        combined_result = self.combined_fill()\n",
    "        self.evaluate_imputation_quality('combined_fill', combined_result)\n",
    "        \n",
    "        # Summary comparison\n",
    "        if self.performance_metrics:\n",
    "            self.print_comparison()\n",
    "        \n",
    "        return {\n",
    "            'Forward Fill': ff_result,\n",
    "            'Backward Fill': bf_result,\n",
    "            'Combined Fill': combined_result\n",
    "        }\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print method comparison\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"METHOD COMPARISON SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if self.performance_metrics:\n",
    "            comparison_df = pd.DataFrame(self.performance_metrics).T\n",
    "            print(comparison_df.round(3))\n",
    "            \n",
    "            # Find best method\n",
    "            best_method = comparison_df['RMSE'].idxmin()\n",
    "            print(f\"\\nBEST PERFORMING: {best_method.replace('_', ' ').title()}\")\n",
    "        \n",
    "        print(f\"\\nFINANCIAL TIME SERIES RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 45)\n",
    "        print(\"BEST CHOICE: Combined Fill\")\n",
    "        print(\"  - Fills most gaps possible\")\n",
    "        print(\"  - Uses both past and future information\")\n",
    "        print(\"  - Minimal remaining missing values\")\n",
    "        \n",
    "        print(f\"\\nFORWARD FILL (LOCF):\")\n",
    "        print(\"  - Conservative approach\")\n",
    "        print(\"  - Only uses past information\") \n",
    "        print(\"  - Good for real-time applications\")\n",
    "        \n",
    "        print(f\"\\nBACKWARD FILL (NOCB):\")\n",
    "        print(\"  - Uses future information\")\n",
    "        print(\"  - Good for filling beginning gaps\")\n",
    "        print(\"  - Less realistic for trading strategies\")\n",
    "\n",
    "\n",
    "# READY TO USE\n",
    "print(\"=\"*60)\n",
    "print(\"FORWARD/BACKWARD FILL ANALYZER READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Simple and effective for financial time series gaps\")\n",
    "print(\"\\nUSAGE:\")\n",
    "print(\"analyzer = FinancialForwardBackwardFill(data_with_missing, original_data)\")\n",
    "print(\"results = analyzer.compare_all_methods()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dadc515a-d8a4-4ed8-b9e4-4665a26592f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FORWARD/BACKWARD FILL COMPREHENSIVE ANALYSIS\n",
      "================================================================================\n",
      "Forward/Backward Fill - Missing Pattern Analysis\n",
      "============================================================\n",
      "Dataset shape: (700, 200)\n",
      "Total missing values: 1,000\n",
      "Missing percentage: 0.71%\n",
      "\n",
      "Edge case analysis:\n",
      "  First row missing values: 1\n",
      "  Last row missing values: 0\n",
      "  WARNING: First row has missing values - forward fill will fail here\n",
      "  Maximum consecutive gap: 2 periods\n",
      "  GOOD: Gap length suitable for forward/backward fill\n",
      "\n",
      "========================= FORWARD FILL =========================\n",
      "\n",
      "FORWARD FILL (LOCF) - Last Observation Carried Forward\n",
      "============================================================\n",
      "  ASSET_01: filled 3 values via forward fill\n",
      "  ASSET_02: filled 5 values via forward fill\n",
      "  ASSET_03: filled 4 values via forward fill\n",
      "  ASSET_04: filled 5 values via forward fill\n",
      "  ASSET_05: filled 9 values via forward fill\n",
      "  ASSET_06: filled 6 values via forward fill\n",
      "  ASSET_07: filled 5 values via forward fill\n",
      "  ASSET_08: filled 4 values via forward fill\n",
      "  ASSET_09: filled 7 values via forward fill\n",
      "  ASSET_10: filled 6 values via forward fill\n",
      "  ASSET_11: filled 5 values via forward fill\n",
      "  ASSET_12: filled 3 values via forward fill\n",
      "  ASSET_13: filled 4 values via forward fill\n",
      "  ASSET_14: filled 2 values via forward fill\n",
      "  ASSET_15: filled 3 values via forward fill\n",
      "  ASSET_16: filled 2 values via forward fill\n",
      "  ASSET_17: filled 3 values via forward fill\n",
      "  ASSET_18: filled 6 values via forward fill\n",
      "  ASSET_19: filled 3 values via forward fill\n",
      "  ASSET_20: filled 5 values via forward fill\n",
      "  ASSET_21: filled 4 values via forward fill\n",
      "  ASSET_22: filled 1 values via forward fill\n",
      "  ASSET_23: filled 4 values via forward fill\n",
      "  ASSET_24: filled 4 values via forward fill\n",
      "  ASSET_25: filled 3 values via forward fill\n",
      "  ASSET_26: filled 5 values via forward fill\n",
      "  ASSET_27: filled 4 values via forward fill\n",
      "  ASSET_28: filled 5 values via forward fill\n",
      "  ASSET_29: filled 3 values via forward fill\n",
      "  ASSET_30: filled 7 values via forward fill\n",
      "  ASSET_31: filled 7 values via forward fill\n",
      "  ASSET_32: filled 5 values via forward fill\n",
      "  ASSET_33: filled 4 values via forward fill\n",
      "  ASSET_34: filled 7 values via forward fill\n",
      "  ASSET_35: filled 3 values via forward fill\n",
      "  ASSET_36: filled 7 values via forward fill\n",
      "  ASSET_37: filled 4 values via forward fill\n",
      "  ASSET_38: filled 4 values via forward fill\n",
      "  ASSET_39: filled 6 values via forward fill\n",
      "  ASSET_40: filled 7 values via forward fill\n",
      "  ASSET_41: filled 2 values via forward fill\n",
      "  ASSET_42: filled 9 values via forward fill\n",
      "  ASSET_43: filled 8 values via forward fill\n",
      "  ASSET_44: filled 5 values via forward fill\n",
      "  ASSET_45: filled 5 values via forward fill\n",
      "  ASSET_46: filled 3 values via forward fill\n",
      "  ASSET_47: filled 2 values via forward fill\n",
      "  ASSET_48: filled 7 values via forward fill\n",
      "  ASSET_49: filled 3 values via forward fill\n",
      "  ASSET_50: filled 6 values via forward fill\n",
      "  ASSET_51: filled 5 values via forward fill\n",
      "  ASSET_52: filled 6 values via forward fill\n",
      "  ASSET_53: filled 2 values via forward fill\n",
      "  ASSET_54: filled 6 values via forward fill\n",
      "  ASSET_55: filled 7 values via forward fill\n",
      "  ASSET_56: filled 12 values via forward fill\n",
      "  ASSET_57: filled 6 values via forward fill\n",
      "  ASSET_58: filled 5 values via forward fill\n",
      "  ASSET_59: filled 3 values via forward fill\n",
      "  ASSET_60: filled 4 values via forward fill\n",
      "  ASSET_61: filled 2 values via forward fill\n",
      "  ASSET_62: filled 4 values via forward fill\n",
      "  ASSET_63: filled 6 values via forward fill\n",
      "  ASSET_64: filled 4 values via forward fill\n",
      "  ASSET_65: filled 2 values via forward fill\n",
      "  ASSET_66: filled 3 values via forward fill\n",
      "  ASSET_67: filled 4 values via forward fill\n",
      "  ASSET_68: filled 5 values via forward fill\n",
      "  ASSET_69: filled 6 values via forward fill\n",
      "  ASSET_70: filled 2 values via forward fill\n",
      "  ASSET_71: filled 2 values via forward fill\n",
      "  ASSET_72: filled 5 values via forward fill\n",
      "  ASSET_73: filled 6 values via forward fill\n",
      "  ASSET_74: filled 0 values via forward fill\n",
      "  ASSET_75: filled 1 values via forward fill\n",
      "  ASSET_76: filled 4 values via forward fill\n",
      "  ASSET_77: filled 9 values via forward fill\n",
      "  ASSET_78: filled 7 values via forward fill\n",
      "  ASSET_79: filled 5 values via forward fill\n",
      "  ASSET_80: filled 1 values via forward fill\n",
      "  ASSET_81: filled 8 values via forward fill\n",
      "  ASSET_82: filled 7 values via forward fill\n",
      "  ASSET_83: filled 5 values via forward fill\n",
      "  ASSET_84: filled 5 values via forward fill\n",
      "  ASSET_85: filled 8 values via forward fill\n",
      "  ASSET_86: filled 6 values via forward fill\n",
      "  ASSET_87: filled 6 values via forward fill\n",
      "  ASSET_88: filled 8 values via forward fill\n",
      "  ASSET_89: filled 8 values via forward fill\n",
      "  ASSET_90: filled 2 values via forward fill\n",
      "  ASSET_91: filled 7 values via forward fill\n",
      "  ASSET_92: filled 3 values via forward fill\n",
      "  ASSET_93: filled 4 values via forward fill\n",
      "  ASSET_94: filled 7 values via forward fill\n",
      "  ASSET_95: filled 3 values via forward fill\n",
      "  ASSET_96: filled 5 values via forward fill\n",
      "  ASSET_97: filled 4 values via forward fill\n",
      "  ASSET_98: filled 5 values via forward fill\n",
      "  ASSET_99: filled 4 values via forward fill\n",
      "  ASSET_100: filled 4 values via forward fill\n",
      "  ASSET_101: filled 2 values via forward fill\n",
      "  ASSET_102: filled 7 values via forward fill\n",
      "  ASSET_103: filled 4 values via forward fill\n",
      "  ASSET_104: filled 9 values via forward fill\n",
      "  ASSET_105: filled 7 values via forward fill\n",
      "  ASSET_106: filled 5 values via forward fill\n",
      "  ASSET_107: filled 4 values via forward fill\n",
      "  ASSET_108: filled 4 values via forward fill\n",
      "  ASSET_109: filled 6 values via forward fill\n",
      "  ASSET_110: filled 6 values via forward fill\n",
      "  ASSET_111: filled 6 values via forward fill\n",
      "  ASSET_112: filled 3 values via forward fill\n",
      "  ASSET_113: filled 2 values via forward fill\n",
      "  ASSET_114: filled 9 values via forward fill\n",
      "  ASSET_115: filled 3 values via forward fill\n",
      "  ASSET_116: filled 3 values via forward fill\n",
      "  ASSET_117: filled 5 values via forward fill\n",
      "  ASSET_118: filled 7 values via forward fill\n",
      "  ASSET_119: filled 5 values via forward fill\n",
      "  ASSET_120: filled 7 values via forward fill\n",
      "  ASSET_121: filled 8 values via forward fill\n",
      "  ASSET_122: filled 7 values via forward fill\n",
      "  ASSET_123: filled 6 values via forward fill\n",
      "  ASSET_124: filled 5 values via forward fill\n",
      "  ASSET_125: filled 7 values via forward fill\n",
      "  ASSET_126: filled 7 values via forward fill\n",
      "  ASSET_127: filled 4 values via forward fill\n",
      "  ASSET_128: filled 2 values via forward fill\n",
      "  ASSET_129: filled 6 values via forward fill\n",
      "  ASSET_130: filled 8 values via forward fill\n",
      "  ASSET_131: filled 3 values via forward fill\n",
      "  ASSET_132: filled 4 values via forward fill\n",
      "  ASSET_133: filled 10 values via forward fill\n",
      "  ASSET_134: filled 7 values via forward fill\n",
      "  ASSET_135: filled 5 values via forward fill\n",
      "  ASSET_136: filled 8 values via forward fill\n",
      "  ASSET_137: filled 3 values via forward fill\n",
      "  ASSET_138: filled 2 values via forward fill\n",
      "  ASSET_139: filled 2 values via forward fill\n",
      "  ASSET_140: filled 2 values via forward fill\n",
      "  ASSET_141: filled 9 values via forward fill\n",
      "  ASSET_142: filled 7 values via forward fill\n",
      "  ASSET_143: filled 5 values via forward fill\n",
      "  ASSET_144: filled 6 values via forward fill\n",
      "  ASSET_145: filled 2 values via forward fill\n",
      "  ASSET_146: filled 5 values via forward fill\n",
      "  ASSET_147: filled 5 values via forward fill\n",
      "  ASSET_148: filled 5 values via forward fill\n",
      "  ASSET_149: filled 11 values via forward fill\n",
      "  ASSET_150: filled 11 values via forward fill\n",
      "  ASSET_151: filled 5 values via forward fill\n",
      "  ASSET_152: filled 8 values via forward fill\n",
      "  ASSET_153: filled 2 values via forward fill\n",
      "  ASSET_154: filled 6 values via forward fill\n",
      "  ASSET_155: filled 5 values via forward fill\n",
      "  ASSET_156: filled 6 values via forward fill\n",
      "  ASSET_157: filled 4 values via forward fill\n",
      "  ASSET_158: filled 5 values via forward fill\n",
      "  ASSET_159: filled 2 values via forward fill\n",
      "  ASSET_160: filled 5 values via forward fill\n",
      "  ASSET_161: filled 6 values via forward fill\n",
      "  ASSET_162: filled 4 values via forward fill\n",
      "  ASSET_163: filled 4 values via forward fill\n",
      "  ASSET_164: filled 5 values via forward fill\n",
      "  ASSET_165: filled 3 values via forward fill\n",
      "  ASSET_166: filled 4 values via forward fill\n",
      "  ASSET_167: filled 3 values via forward fill\n",
      "  ASSET_168: filled 3 values via forward fill\n",
      "  ASSET_169: filled 6 values via forward fill\n",
      "  ASSET_170: filled 3 values via forward fill\n",
      "    WARNING: 1 values still missing (no prior value available)\n",
      "  ASSET_171: filled 4 values via forward fill\n",
      "  ASSET_172: filled 2 values via forward fill\n",
      "  ASSET_173: filled 3 values via forward fill\n",
      "  ASSET_174: filled 5 values via forward fill\n",
      "  ASSET_175: filled 6 values via forward fill\n",
      "  ASSET_176: filled 3 values via forward fill\n",
      "  ASSET_177: filled 6 values via forward fill\n",
      "  ASSET_178: filled 3 values via forward fill\n",
      "  ASSET_179: filled 3 values via forward fill\n",
      "  ASSET_180: filled 6 values via forward fill\n",
      "  ASSET_181: filled 8 values via forward fill\n",
      "  ASSET_182: filled 3 values via forward fill\n",
      "  ASSET_183: filled 7 values via forward fill\n",
      "  ASSET_184: filled 6 values via forward fill\n",
      "  ASSET_185: filled 4 values via forward fill\n",
      "  ASSET_186: filled 5 values via forward fill\n",
      "  ASSET_187: filled 8 values via forward fill\n",
      "  ASSET_188: filled 9 values via forward fill\n",
      "  ASSET_189: filled 12 values via forward fill\n",
      "  ASSET_190: filled 3 values via forward fill\n",
      "  ASSET_191: filled 1 values via forward fill\n",
      "  ASSET_192: filled 9 values via forward fill\n",
      "  ASSET_193: filled 10 values via forward fill\n",
      "  ASSET_194: filled 2 values via forward fill\n",
      "  ASSET_195: filled 7 values via forward fill\n",
      "  ASSET_196: filled 6 values via forward fill\n",
      "  ASSET_197: filled 2 values via forward fill\n",
      "  ASSET_198: filled 5 values via forward fill\n",
      "  ASSET_199: filled 1 values via forward fill\n",
      "  ASSET_200: filled 8 values via forward fill\n",
      "\n",
      "IMPUTATION QUALITY EVALUATION: FORWARD_FILL\n",
      "==================================================\n",
      "  Evaluated 1000 imputed values\n",
      "  MAE: $nan\n",
      "  RMSE: $nan\n",
      "  MAPE: âˆž\n",
      "  Correlation: nan\n",
      "\n",
      "  TIME SERIES EVALUATION:\n",
      "  Original trend: -0.022/period\n",
      "  Imputed trend: nan/period\n",
      "  Trend preservation error: nan\n",
      "  POOR: High errors - consider other methods\n",
      "\n",
      "========================= BACKWARD FILL ========================\n",
      "\n",
      "BACKWARD FILL (NOCB) - Next Observation Carried Backward\n",
      "============================================================\n",
      "  ASSET_01: filled 3 values via backward fill\n",
      "  ASSET_02: filled 5 values via backward fill\n",
      "  ASSET_03: filled 4 values via backward fill\n",
      "  ASSET_04: filled 5 values via backward fill\n",
      "  ASSET_05: filled 9 values via backward fill\n",
      "  ASSET_06: filled 6 values via backward fill\n",
      "  ASSET_07: filled 5 values via backward fill\n",
      "  ASSET_08: filled 4 values via backward fill\n",
      "  ASSET_09: filled 7 values via backward fill\n",
      "  ASSET_10: filled 6 values via backward fill\n",
      "  ASSET_11: filled 5 values via backward fill\n",
      "  ASSET_12: filled 3 values via backward fill\n",
      "  ASSET_13: filled 4 values via backward fill\n",
      "  ASSET_14: filled 2 values via backward fill\n",
      "  ASSET_15: filled 3 values via backward fill\n",
      "  ASSET_16: filled 2 values via backward fill\n",
      "  ASSET_17: filled 3 values via backward fill\n",
      "  ASSET_18: filled 6 values via backward fill\n",
      "  ASSET_19: filled 3 values via backward fill\n",
      "  ASSET_20: filled 5 values via backward fill\n",
      "  ASSET_21: filled 4 values via backward fill\n",
      "  ASSET_22: filled 1 values via backward fill\n",
      "  ASSET_23: filled 4 values via backward fill\n",
      "  ASSET_24: filled 4 values via backward fill\n",
      "  ASSET_25: filled 3 values via backward fill\n",
      "  ASSET_26: filled 5 values via backward fill\n",
      "  ASSET_27: filled 4 values via backward fill\n",
      "  ASSET_28: filled 5 values via backward fill\n",
      "  ASSET_29: filled 3 values via backward fill\n",
      "  ASSET_30: filled 7 values via backward fill\n",
      "  ASSET_31: filled 7 values via backward fill\n",
      "  ASSET_32: filled 5 values via backward fill\n",
      "  ASSET_33: filled 4 values via backward fill\n",
      "  ASSET_34: filled 7 values via backward fill\n",
      "  ASSET_35: filled 3 values via backward fill\n",
      "  ASSET_36: filled 7 values via backward fill\n",
      "  ASSET_37: filled 4 values via backward fill\n",
      "  ASSET_38: filled 4 values via backward fill\n",
      "  ASSET_39: filled 6 values via backward fill\n",
      "  ASSET_40: filled 7 values via backward fill\n",
      "  ASSET_41: filled 2 values via backward fill\n",
      "  ASSET_42: filled 9 values via backward fill\n",
      "  ASSET_43: filled 8 values via backward fill\n",
      "  ASSET_44: filled 5 values via backward fill\n",
      "  ASSET_45: filled 5 values via backward fill\n",
      "  ASSET_46: filled 3 values via backward fill\n",
      "  ASSET_47: filled 2 values via backward fill\n",
      "  ASSET_48: filled 7 values via backward fill\n",
      "  ASSET_49: filled 3 values via backward fill\n",
      "  ASSET_50: filled 6 values via backward fill\n",
      "  ASSET_51: filled 5 values via backward fill\n",
      "  ASSET_52: filled 6 values via backward fill\n",
      "  ASSET_53: filled 2 values via backward fill\n",
      "  ASSET_54: filled 6 values via backward fill\n",
      "  ASSET_55: filled 7 values via backward fill\n",
      "  ASSET_56: filled 12 values via backward fill\n",
      "  ASSET_57: filled 6 values via backward fill\n",
      "  ASSET_58: filled 5 values via backward fill\n",
      "  ASSET_59: filled 3 values via backward fill\n",
      "  ASSET_60: filled 4 values via backward fill\n",
      "  ASSET_61: filled 2 values via backward fill\n",
      "  ASSET_62: filled 4 values via backward fill\n",
      "  ASSET_63: filled 6 values via backward fill\n",
      "  ASSET_64: filled 4 values via backward fill\n",
      "  ASSET_65: filled 2 values via backward fill\n",
      "  ASSET_66: filled 3 values via backward fill\n",
      "  ASSET_67: filled 4 values via backward fill\n",
      "  ASSET_68: filled 5 values via backward fill\n",
      "  ASSET_69: filled 6 values via backward fill\n",
      "  ASSET_70: filled 2 values via backward fill\n",
      "  ASSET_71: filled 2 values via backward fill\n",
      "  ASSET_72: filled 5 values via backward fill\n",
      "  ASSET_73: filled 6 values via backward fill\n",
      "  ASSET_74: filled 0 values via backward fill\n",
      "  ASSET_75: filled 1 values via backward fill\n",
      "  ASSET_76: filled 4 values via backward fill\n",
      "  ASSET_77: filled 9 values via backward fill\n",
      "  ASSET_78: filled 7 values via backward fill\n",
      "  ASSET_79: filled 5 values via backward fill\n",
      "  ASSET_80: filled 1 values via backward fill\n",
      "  ASSET_81: filled 8 values via backward fill\n",
      "  ASSET_82: filled 7 values via backward fill\n",
      "  ASSET_83: filled 5 values via backward fill\n",
      "  ASSET_84: filled 5 values via backward fill\n",
      "  ASSET_85: filled 8 values via backward fill\n",
      "  ASSET_86: filled 6 values via backward fill\n",
      "  ASSET_87: filled 6 values via backward fill\n",
      "  ASSET_88: filled 8 values via backward fill\n",
      "  ASSET_89: filled 8 values via backward fill\n",
      "  ASSET_90: filled 2 values via backward fill\n",
      "  ASSET_91: filled 7 values via backward fill\n",
      "  ASSET_92: filled 3 values via backward fill\n",
      "  ASSET_93: filled 4 values via backward fill\n",
      "  ASSET_94: filled 7 values via backward fill\n",
      "  ASSET_95: filled 3 values via backward fill\n",
      "  ASSET_96: filled 5 values via backward fill\n",
      "  ASSET_97: filled 4 values via backward fill\n",
      "  ASSET_98: filled 5 values via backward fill\n",
      "  ASSET_99: filled 4 values via backward fill\n",
      "  ASSET_100: filled 4 values via backward fill\n",
      "  ASSET_101: filled 2 values via backward fill\n",
      "  ASSET_102: filled 7 values via backward fill\n",
      "  ASSET_103: filled 4 values via backward fill\n",
      "  ASSET_104: filled 9 values via backward fill\n",
      "  ASSET_105: filled 7 values via backward fill\n",
      "  ASSET_106: filled 5 values via backward fill\n",
      "  ASSET_107: filled 4 values via backward fill\n",
      "  ASSET_108: filled 4 values via backward fill\n",
      "  ASSET_109: filled 6 values via backward fill\n",
      "  ASSET_110: filled 6 values via backward fill\n",
      "  ASSET_111: filled 6 values via backward fill\n",
      "  ASSET_112: filled 3 values via backward fill\n",
      "  ASSET_113: filled 2 values via backward fill\n",
      "  ASSET_114: filled 9 values via backward fill\n",
      "  ASSET_115: filled 3 values via backward fill\n",
      "  ASSET_116: filled 3 values via backward fill\n",
      "  ASSET_117: filled 5 values via backward fill\n",
      "  ASSET_118: filled 7 values via backward fill\n",
      "  ASSET_119: filled 5 values via backward fill\n",
      "  ASSET_120: filled 7 values via backward fill\n",
      "  ASSET_121: filled 8 values via backward fill\n",
      "  ASSET_122: filled 7 values via backward fill\n",
      "  ASSET_123: filled 6 values via backward fill\n",
      "  ASSET_124: filled 5 values via backward fill\n",
      "  ASSET_125: filled 7 values via backward fill\n",
      "  ASSET_126: filled 7 values via backward fill\n",
      "  ASSET_127: filled 4 values via backward fill\n",
      "  ASSET_128: filled 2 values via backward fill\n",
      "  ASSET_129: filled 6 values via backward fill\n",
      "  ASSET_130: filled 8 values via backward fill\n",
      "  ASSET_131: filled 3 values via backward fill\n",
      "  ASSET_132: filled 4 values via backward fill\n",
      "  ASSET_133: filled 10 values via backward fill\n",
      "  ASSET_134: filled 7 values via backward fill\n",
      "  ASSET_135: filled 5 values via backward fill\n",
      "  ASSET_136: filled 8 values via backward fill\n",
      "  ASSET_137: filled 3 values via backward fill\n",
      "  ASSET_138: filled 2 values via backward fill\n",
      "  ASSET_139: filled 2 values via backward fill\n",
      "  ASSET_140: filled 2 values via backward fill\n",
      "  ASSET_141: filled 9 values via backward fill\n",
      "  ASSET_142: filled 7 values via backward fill\n",
      "  ASSET_143: filled 5 values via backward fill\n",
      "  ASSET_144: filled 6 values via backward fill\n",
      "  ASSET_145: filled 2 values via backward fill\n",
      "  ASSET_146: filled 5 values via backward fill\n",
      "  ASSET_147: filled 5 values via backward fill\n",
      "  ASSET_148: filled 5 values via backward fill\n",
      "  ASSET_149: filled 11 values via backward fill\n",
      "  ASSET_150: filled 11 values via backward fill\n",
      "  ASSET_151: filled 5 values via backward fill\n",
      "  ASSET_152: filled 8 values via backward fill\n",
      "  ASSET_153: filled 2 values via backward fill\n",
      "  ASSET_154: filled 6 values via backward fill\n",
      "  ASSET_155: filled 5 values via backward fill\n",
      "  ASSET_156: filled 6 values via backward fill\n",
      "  ASSET_157: filled 4 values via backward fill\n",
      "  ASSET_158: filled 5 values via backward fill\n",
      "  ASSET_159: filled 2 values via backward fill\n",
      "  ASSET_160: filled 5 values via backward fill\n",
      "  ASSET_161: filled 6 values via backward fill\n",
      "  ASSET_162: filled 4 values via backward fill\n",
      "  ASSET_163: filled 4 values via backward fill\n",
      "  ASSET_164: filled 5 values via backward fill\n",
      "  ASSET_165: filled 3 values via backward fill\n",
      "  ASSET_166: filled 4 values via backward fill\n",
      "  ASSET_167: filled 3 values via backward fill\n",
      "  ASSET_168: filled 3 values via backward fill\n",
      "  ASSET_169: filled 6 values via backward fill\n",
      "  ASSET_170: filled 4 values via backward fill\n",
      "  ASSET_171: filled 4 values via backward fill\n",
      "  ASSET_172: filled 2 values via backward fill\n",
      "  ASSET_173: filled 3 values via backward fill\n",
      "  ASSET_174: filled 5 values via backward fill\n",
      "  ASSET_175: filled 6 values via backward fill\n",
      "  ASSET_176: filled 3 values via backward fill\n",
      "  ASSET_177: filled 6 values via backward fill\n",
      "  ASSET_178: filled 3 values via backward fill\n",
      "  ASSET_179: filled 3 values via backward fill\n",
      "  ASSET_180: filled 6 values via backward fill\n",
      "  ASSET_181: filled 8 values via backward fill\n",
      "  ASSET_182: filled 3 values via backward fill\n",
      "  ASSET_183: filled 7 values via backward fill\n",
      "  ASSET_184: filled 6 values via backward fill\n",
      "  ASSET_185: filled 4 values via backward fill\n",
      "  ASSET_186: filled 5 values via backward fill\n",
      "  ASSET_187: filled 8 values via backward fill\n",
      "  ASSET_188: filled 9 values via backward fill\n",
      "  ASSET_189: filled 12 values via backward fill\n",
      "  ASSET_190: filled 3 values via backward fill\n",
      "  ASSET_191: filled 1 values via backward fill\n",
      "  ASSET_192: filled 9 values via backward fill\n",
      "  ASSET_193: filled 10 values via backward fill\n",
      "  ASSET_194: filled 2 values via backward fill\n",
      "  ASSET_195: filled 7 values via backward fill\n",
      "  ASSET_196: filled 6 values via backward fill\n",
      "  ASSET_197: filled 2 values via backward fill\n",
      "  ASSET_198: filled 5 values via backward fill\n",
      "  ASSET_199: filled 1 values via backward fill\n",
      "  ASSET_200: filled 8 values via backward fill\n",
      "\n",
      "IMPUTATION QUALITY EVALUATION: BACKWARD_FILL\n",
      "==================================================\n",
      "  Evaluated 1000 imputed values\n",
      "  MAE: $3.56\n",
      "  RMSE: $6.79\n",
      "  MAPE: 1.5%\n",
      "  Correlation: 0.999\n",
      "\n",
      "  TIME SERIES EVALUATION:\n",
      "  Original trend: -0.022/period\n",
      "  Imputed trend: -0.019/period\n",
      "  Trend preservation error: 0.003\n",
      "  EXCELLENT: Very accurate for financial data\n",
      "\n",
      "======================= COMBINED FILL =======================\n",
      "\n",
      "COMBINED FORWARD-BACKWARD FILL\n",
      "==================================================\n",
      "  Total values filled: 1000\n",
      "  Remaining missing: 0\n",
      "\n",
      "IMPUTATION QUALITY EVALUATION: COMBINED_FILL\n",
      "==================================================\n",
      "  Evaluated 1000 imputed values\n",
      "  MAE: $3.70\n",
      "  RMSE: $8.94\n",
      "  MAPE: 1.5%\n",
      "  Correlation: 0.999\n",
      "\n",
      "  TIME SERIES EVALUATION:\n",
      "  Original trend: -0.022/period\n",
      "  Imputed trend: -0.017/period\n",
      "  Trend preservation error: 0.005\n",
      "  EXCELLENT: Very accurate for financial data\n",
      "\n",
      "============================================================\n",
      "METHOD COMPARISON SUMMARY\n",
      "============================================================\n",
      "                 MAE   RMSE   MAPE  Correlation  Trend_Error  N_Values\n",
      "forward_fill     NaN    NaN    NaN          NaN          NaN    1000.0\n",
      "backward_fill  3.556  6.785  1.461        0.999        0.003    1000.0\n",
      "combined_fill  3.695  8.939  1.468        0.999        0.005    1000.0\n",
      "\n",
      "BEST PERFORMING: Backward Fill\n",
      "\n",
      "FINANCIAL TIME SERIES RECOMMENDATIONS:\n",
      "---------------------------------------------\n",
      "BEST CHOICE: Combined Fill\n",
      "  - Fills most gaps possible\n",
      "  - Uses both past and future information\n",
      "  - Minimal remaining missing values\n",
      "\n",
      "FORWARD FILL (LOCF):\n",
      "  - Conservative approach\n",
      "  - Only uses past information\n",
      "  - Good for real-time applications\n",
      "\n",
      "BACKWARD FILL (NOCB):\n",
      "  - Uses future information\n",
      "  - Good for filling beginning gaps\n",
      "  - Less realistic for trading strategies\n",
      "            ASSET_01  ASSET_02  ASSET_03  ASSET_04  ASSET_05  ASSET_06  \\\n",
      "date                                                                     \n",
      "2024-01-01     95.10     66.54     97.23    139.55     77.47    154.16   \n",
      "2024-01-02     95.97     66.90     96.84    138.36     77.08    152.61   \n",
      "2024-01-03     95.20     65.96     97.85    138.24     76.84    152.71   \n",
      "2024-01-04     96.28     65.65     97.62    140.46     77.83    153.68   \n",
      "2024-01-05     97.92     65.25     96.69    140.20     77.65    152.64   \n",
      "2024-01-06     97.09     64.97     94.59    143.59     77.74    152.93   \n",
      "2024-01-07    108.48     64.06     96.09    142.73     77.26    152.08   \n",
      "2024-01-08    107.99     64.06     97.12    143.62     79.09    152.82   \n",
      "2024-01-09    105.88     65.73     98.52    141.01     74.47    152.81   \n",
      "2024-01-10    105.28     64.79     97.80    142.74     74.39    156.77   \n",
      "\n",
      "            ASSET_07  ASSET_08  ASSET_09  ASSET_10  ...  ASSET_191  ASSET_192  \\\n",
      "date                                                ...                         \n",
      "2024-01-01     92.68    111.98    144.21    187.20  ...     134.22     105.71   \n",
      "2024-01-02     92.97    111.21    140.78    181.79  ...     133.08     109.95   \n",
      "2024-01-03     92.09    111.73    140.23    180.57  ...     134.40     109.56   \n",
      "2024-01-04     90.66    110.11    137.40    180.99  ...     134.65     108.31   \n",
      "2024-01-05     90.66    110.72    134.15    186.70  ...     134.82     111.58   \n",
      "2024-01-06     87.18    112.33    132.93    185.16  ...     135.57     111.03   \n",
      "2024-01-07     87.36    110.76    134.13    182.42  ...     137.45     111.32   \n",
      "2024-01-08     87.20    110.20    134.98    186.96  ...     130.95     114.27   \n",
      "2024-01-09     87.52    110.28    133.57    189.21  ...     130.69     114.56   \n",
      "2024-01-10     91.36    109.86    134.76    187.25  ...     130.91     114.15   \n",
      "\n",
      "            ASSET_193  ASSET_194  ASSET_195  ASSET_196  ASSET_197  ASSET_198  \\\n",
      "date                                                                           \n",
      "2024-01-01     156.42      75.52     141.44     194.76      55.51     190.42   \n",
      "2024-01-02     157.41      75.10     140.66     195.82      56.74     187.70   \n",
      "2024-01-03     152.90      75.55     141.27     201.22      57.04     188.78   \n",
      "2024-01-04     152.11      74.65     142.09     201.45      57.86     190.06   \n",
      "2024-01-05     153.30      74.91     142.84     199.08      58.67     188.93   \n",
      "2024-01-06     154.73      75.13     141.43     199.26      58.00     182.50   \n",
      "2024-01-07     151.40      74.18     142.32     199.92      57.44     180.51   \n",
      "2024-01-08     150.27      74.02     144.32     200.68      58.05     181.05   \n",
      "2024-01-09     150.69      74.34     145.37     198.91      56.03     181.21   \n",
      "2024-01-10     149.88      73.13     143.53     197.09      64.85     187.17   \n",
      "\n",
      "            ASSET_199  ASSET_200  \n",
      "date                              \n",
      "2024-01-01     137.44     193.72  \n",
      "2024-01-02     135.51     189.04  \n",
      "2024-01-03     135.89     192.41  \n",
      "2024-01-04     135.13     193.76  \n",
      "2024-01-05     135.94     190.89  \n",
      "2024-01-06     139.48     190.22  \n",
      "2024-01-07     138.93     189.25  \n",
      "2024-01-08     139.65     188.73  \n",
      "2024-01-09     135.15     186.63  \n",
      "2024-01-10     135.81     184.77  \n",
      "\n",
      "[10 rows x 200 columns]\n",
      "\n",
      "Sample of imputed values (original NaN -> new value):\n",
      "Position (0, 169): NaN -> 113.73\n",
      "Position (2, 37): NaN -> 164.85\n",
      "Position (3, 81): NaN -> 199.42\n",
      "Position (4, 6): NaN -> 87.18\n",
      "Position (4, 86): NaN -> 185.48\n",
      "Position (4, 189): NaN -> 157.37\n",
      "Position (6, 118): NaN -> 156.21\n",
      "Position (7, 1): NaN -> 65.73\n",
      "Position (9, 107): NaN -> 133.38\n",
      "Position (10, 83): NaN -> 109.66\n"
     ]
    }
   ],
   "source": [
    "# Initialize with your 700x200 dataset  \n",
    "analyzer = FinancialForwardBackwardFill(financial_data_missing, df)\n",
    "\n",
    "# Run comprehensive analysis\n",
    "results = analyzer.compare_all_methods()\n",
    "\n",
    "# Access specific methods\n",
    "forward_filled = results['Forward Fill']\n",
    "combined_filled = results['Combined Fill']  \n",
    "print(pd.DataFrame(results['Combined Fill']).head(10))\n",
    "\n",
    "# Show some specific imputed values\n",
    "backward_filled = results['Backward Fill']\n",
    "print(\"\\nSample of imputed values (original NaN -> new value):\")\n",
    "for i in range(10):  # Show first 10 missing positions\n",
    "    row, col = np.where(missing_mask)\n",
    "    if i < len(row):\n",
    "        print(f\"Position ({row[i]}, {col[i]}): NaN -> {backward_filled.iloc[row[i], col[i]]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37840911-e869-4c38-90cd-754c3e5954b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SKLEARN K-NN IMPUTATION READY!\n",
      "==================================================\n",
      "Simple sklearn-based KNN imputation\n",
      "\n",
      "USAGE:\n",
      "analyzer = SimpleSklearnKNNImputation(data_with_missing, original_data)\n",
      "results = analyzer.compare_all_methods(n_neighbors=5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SimpleSklearnKNNImputation:\n",
    "    \"\"\"\n",
    "    Simple K-NN imputation using sklearn KNNImputer.\n",
    "    Clean, straightforward implementation for financial time series data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_with_missing, original_data=None):\n",
    "        \"\"\"\n",
    "        Initialize sklearn KNN imputation\n",
    "        \n",
    "        Parameters:\n",
    "        - data_with_missing: DataFrame or numpy array with missing values\n",
    "        - original_data: Original complete data for evaluation (optional)\n",
    "        \"\"\"\n",
    "        self.data_missing = data_with_missing.copy() if hasattr(data_with_missing, 'copy') else data_with_missing.copy()\n",
    "        self.original_data = original_data.copy() if original_data is not None else None\n",
    "        self.imputation_results = {}\n",
    "        self.performance_metrics = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def analyze_missing_pattern(self):\n",
    "        \"\"\"Quick missing value analysis\"\"\"\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            missing_info = self.data_missing.isnull()\n",
    "            data_shape = self.data_missing.shape\n",
    "        else:\n",
    "            missing_info = pd.DataFrame(np.isnan(self.data_missing))\n",
    "            data_shape = self.data_missing.shape\n",
    "        \n",
    "        total_missing = missing_info.sum().sum()\n",
    "        missing_pct = (total_missing / (data_shape[0] * data_shape[1])) * 100\n",
    "        \n",
    "        print(\"K-NN IMPUTATION - MISSING DATA ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Dataset shape: {data_shape}\")\n",
    "        print(f\"Total missing values: {total_missing:,}\")\n",
    "        print(f\"Missing percentage: {missing_pct:.2f}%\")\n",
    "        \n",
    "        return {'total_missing': total_missing, 'missing_pct': missing_pct}\n",
    "    \n",
    "    def knn_imputation_basic(self, n_neighbors=5):\n",
    "        \"\"\"\n",
    "        Basic sklearn KNN imputation\n",
    "        \"\"\"\n",
    "        print(f\"\\nBASIC SKLEARN K-NN IMPUTATION (K={n_neighbors})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        \n",
    "        # Apply KNN imputation\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        \n",
    "        self.imputation_results['knn_basic'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def knn_imputation_scaled(self, n_neighbors=5):\n",
    "        \"\"\"\n",
    "        KNN imputation with feature scaling (better for financial data)\n",
    "        \"\"\"\n",
    "        print(f\"\\nSCALED SKLEARN K-NN IMPUTATION (K={n_neighbors})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        print(\"Applying StandardScaler for better distance calculations...\")\n",
    "        \n",
    "        # Scale the data first (handles NaN automatically)\n",
    "        scaler = StandardScaler()  # Z-score normalization : (~N(0,1))\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "        \n",
    "        # Apply KNN imputation on scaled data\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        imputed_scaled = imputer.fit_transform(data_scaled)\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        imputed_data = scaler.inverse_transform(imputed_scaled)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        \n",
    "        self.imputation_results['knn_scaled'] = imputed_data\n",
    "        self.scalers['knn_scaled'] = scaler\n",
    "        return imputed_data\n",
    "    \n",
    "    def knn_imputation_different_k(self, k_values=[3, 5, 7, 10]):\n",
    "        \"\"\"\n",
    "        Test different K values to find optimal\n",
    "        \"\"\"\n",
    "        print(f\"\\nTESTING DIFFERENT K VALUES: {k_values}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        for k in k_values:\n",
    "            print(f\"\\nTesting K={k}...\")\n",
    "            \n",
    "            # Apply KNN imputation\n",
    "            imputer = KNNImputer(n_neighbors=k)\n",
    "            imputed_data = imputer.fit_transform(data)\n",
    "            \n",
    "            # Quick evaluation if original data available\n",
    "            if self.original_data is not None:\n",
    "                metrics = self.quick_evaluate(imputed_data, f'k_{k}')\n",
    "                results[f'K={k}'] = {\n",
    "                    'data': imputed_data,\n",
    "                    'rmse': metrics['RMSE'] if metrics else None\n",
    "                }\n",
    "                print(f\"  RMSE: {metrics['RMSE']:.3f}\" if metrics else \"  No evaluation data\")\n",
    "            else:\n",
    "                results[f'K={k}'] = {'data': imputed_data, 'rmse': None}\n",
    "                print(f\"  Imputation completed\")\n",
    "        \n",
    "        # Find best K if evaluation possible\n",
    "        if self.original_data is not None:\n",
    "            best_k = min(k_values, key=lambda k: results[f'K={k}']['rmse'] if results[f'K={k}']['rmse'] else float('inf'))\n",
    "            print(f\"\\nBest K value: {best_k}\")\n",
    "            self.imputation_results['knn_best_k'] = results[f'K={best_k}']['data']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def quick_evaluate(self, imputed_data, method_name):\n",
    "        \"\"\"Quick evaluation for K comparison\"\"\"\n",
    "        if self.original_data is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "            \n",
    "            # Find missing positions and extract values\n",
    "            mask = np.isnan(missing_np)\n",
    "            orig_vals = original_np[mask]\n",
    "            imp_vals = imputed_data[mask]\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(np.mean((orig_vals - imp_vals) ** 2))\n",
    "            \n",
    "            return {'RMSE': rmse}\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def evaluate_imputation_quality(self, method_name, imputed_data):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of KNN imputation quality\n",
    "        \"\"\"\n",
    "        if self.original_data is None:\n",
    "            print(f\"\\nNo original data available for {method_name} evaluation\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nSKLEARN K-NN EVALUATION: {method_name.upper()}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "            \n",
    "            # Find missing positions\n",
    "            mask = np.isnan(missing_np)\n",
    "            total_missing = np.sum(mask)\n",
    "            \n",
    "            # Extract values\n",
    "            orig_vals = original_np[mask]\n",
    "            imp_vals = imputed_data[mask]\n",
    "            \n",
    "            print(f\"Evaluated {len(orig_vals)} imputed values\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = np.mean(np.abs(orig_vals - imp_vals))\n",
    "            rmse = np.sqrt(np.mean((orig_vals - imp_vals) ** 2))\n",
    "            \n",
    "            # MAPE calculation\n",
    "            nonzero_mask = orig_vals != 0\n",
    "            if np.sum(nonzero_mask) > 0:\n",
    "                mape = np.mean(np.abs(orig_vals - imp_vals)[nonzero_mask] / np.abs(orig_vals[nonzero_mask])) * 100\n",
    "            else:\n",
    "                mape = float('inf')\n",
    "            \n",
    "            # Correlation\n",
    "            if len(orig_vals) > 1:\n",
    "                corr = np.corrcoef(orig_vals, imp_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 1.0\n",
    "            \n",
    "            print(f\"MAE: ${mae:.2f}\")\n",
    "            print(f\"RMSE: ${rmse:.2f}\")\n",
    "            print(f\"MAPE: {mape:.1f}%\" if np.isfinite(mape) else \"MAPE: âˆž\")\n",
    "            print(f\"Correlation: {corr:.3f}\")\n",
    "            \n",
    "            # Financial interpretation\n",
    "            if mape < 3:\n",
    "                print(\"EXCELLENT: Very accurate K-NN imputation\")\n",
    "            elif mape < 7:\n",
    "                print(\"GOOD: Acceptable K-NN performance\")\n",
    "            elif mape < 15:\n",
    "                print(\"FAIR: Consider different K or scaling\")\n",
    "            else:\n",
    "                print(\"POOR: K-NN may not suit this data\")\n",
    "            \n",
    "            metrics = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'Correlation': corr,\n",
    "                'N_Values': len(orig_vals)\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics[method_name] = metrics\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compare_all_methods(self, n_neighbors=5):\n",
    "        \"\"\"\n",
    "        Compare different sklearn KNN approaches\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SKLEARN K-NN IMPUTATION ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Analyze missing pattern\n",
    "        self.analyze_missing_pattern()\n",
    "        \n",
    "        # Test basic KNN\n",
    "        print(f\"\\n{'='*15} BASIC K-NN {'='*15}\")\n",
    "        basic_result = self.knn_imputation_basic(n_neighbors)\n",
    "        self.evaluate_imputation_quality('knn_basic', basic_result)\n",
    "        \n",
    "        # Test scaled KNN\n",
    "        print(f\"\\n{'='*15} SCALED K-NN {'='*14}\")\n",
    "        scaled_result = self.knn_imputation_scaled(n_neighbors)\n",
    "        self.evaluate_imputation_quality('knn_scaled', scaled_result)\n",
    "        \n",
    "        # Test different K values\n",
    "        print(f\"\\n{'='*12} K VALUE TESTING {'='*12}\")\n",
    "        k_results = self.knn_imputation_different_k([3, 5, 7, 10])\n",
    "        \n",
    "        # Summary comparison\n",
    "        if self.performance_metrics:\n",
    "            self.print_comparison()\n",
    "        \n",
    "        return {\n",
    "            'Basic KNN': basic_result,\n",
    "            'Scaled KNN': scaled_result,\n",
    "            'K Results': k_results\n",
    "        }\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print method comparison summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SKLEARN K-NN COMPARISON SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if self.performance_metrics:\n",
    "            comparison_df = pd.DataFrame(self.performance_metrics).T\n",
    "            print(comparison_df.round(3))\n",
    "            \n",
    "            # Find best method\n",
    "            best_method = comparison_df['RMSE'].idxmin()\n",
    "            print(f\"\\nBest performing: {best_method.replace('_', ' ').title()}\")\n",
    "        \n",
    "        print(f\"\\nRECOMMENDATIONS:\")\n",
    "        print(\"- Scaled K-NN usually performs best for financial data\")\n",
    "        print(\"- K=5 is good default, but aslo test  other ranges\")\n",
    "        print(\"- Feature scaling important when assets have different price ranges\")\n",
    "        print(\"- sklearn KNNImputer is robust and well-tested\")\n",
    "\n",
    "\n",
    "# READY TO USE\n",
    "print(\"=\"*50)\n",
    "print(\"SKLEARN K-NN IMPUTATION READY!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Simple sklearn-based KNN imputation\")\n",
    "print(\"\\nUSAGE:\")\n",
    "print(\"analyzer = SimpleSklearnKNNImputation(data_with_missing, original_data)\")\n",
    "print(\"results = analyzer.compare_all_methods(n_neighbors=5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee2ad2c-e94a-4f87-b529-152b4e3995fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SKLEARN K-NN IMPUTATION ANALYSIS\n",
      "============================================================\n",
      "K-NN IMPUTATION - MISSING DATA ANALYSIS\n",
      "==================================================\n",
      "Dataset shape: (700, 200)\n",
      "Total missing values: 1,000\n",
      "Missing percentage: 0.71%\n",
      "\n",
      "=============== BASIC K-NN ===============\n",
      "\n",
      "BASIC SKLEARN K-NN IMPUTATION (K=5)\n",
      "==================================================\n",
      "Processing 700x200 dataset...\n",
      "Successfully imputed: 1000 values\n",
      "Remaining missing: 0\n",
      "\n",
      "SKLEARN K-NN EVALUATION: KNN_BASIC\n",
      "==================================================\n",
      "Evaluated 1000 imputed values\n",
      "MAE: $3.37\n",
      "RMSE: $6.74\n",
      "MAPE: 1.4%\n",
      "Correlation: 0.999\n",
      "EXCELLENT: Very accurate K-NN imputation\n",
      "\n",
      "=============== SCALED K-NN ==============\n",
      "\n",
      "SCALED SKLEARN K-NN IMPUTATION (K=5)\n",
      "==================================================\n",
      "Processing 700x200 dataset...\n",
      "Applying StandardScaler for better distance calculations...\n",
      "Successfully imputed: 1000 values\n",
      "Remaining missing: 0\n",
      "\n",
      "SKLEARN K-NN EVALUATION: KNN_SCALED\n",
      "==================================================\n",
      "Evaluated 1000 imputed values\n",
      "MAE: $3.47\n",
      "RMSE: $6.87\n",
      "MAPE: 1.4%\n",
      "Correlation: 0.999\n",
      "EXCELLENT: Very accurate K-NN imputation\n",
      "\n",
      "============ K VALUE TESTING ============\n",
      "\n",
      "TESTING DIFFERENT K VALUES: [3, 5, 7, 10]\n",
      "==================================================\n",
      "\n",
      "Testing K=3...\n",
      "  RMSE: 6.669\n",
      "\n",
      "Testing K=5...\n",
      "  RMSE: 6.737\n",
      "\n",
      "Testing K=7...\n",
      "  RMSE: 7.283\n",
      "\n",
      "Testing K=10...\n",
      "  RMSE: 7.787\n",
      "\n",
      "Best K value: 3\n",
      "\n",
      "==================================================\n",
      "SKLEARN K-NN COMPARISON SUMMARY\n",
      "==================================================\n",
      "              MAE   RMSE   MAPE  Correlation  N_Values\n",
      "knn_basic   3.368  6.737  1.387        0.999    1000.0\n",
      "knn_scaled  3.471  6.865  1.421        0.999    1000.0\n",
      "\n",
      "Best performing: Knn Basic\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "- Scaled K-NN usually performs best for financial data\n",
      "- K=5 is good default, but aslo test  other ranges\n",
      "- Feature scaling important when assets have different price ranges\n",
      "- sklearn KNNImputer is robust and well-tested\n",
      "      0       1      2       3      4       5       6       7       8    \\\n",
      "0   95.10  66.540  97.23  139.55  77.47  154.16  92.680  111.98  144.21   \n",
      "1   95.97  66.900  96.84  138.36  77.08  152.61  92.970  111.21  140.78   \n",
      "2   95.20  65.960  97.85  138.24  76.84  152.71  92.090  111.73  140.23   \n",
      "3   96.28  65.650  97.62  140.46  77.83  153.68  90.660  110.11  137.40   \n",
      "4   97.92  65.250  96.69  140.20  77.65  152.64  90.052  110.72  134.15   \n",
      "5   97.09  64.970  94.59  143.59  77.74  152.93  87.180  112.33  132.93   \n",
      "6  108.48  64.060  96.09  142.73  77.26  152.08  87.360  110.76  134.13   \n",
      "7  107.99  64.766  97.12  143.62  79.09  152.82  87.200  110.20  134.98   \n",
      "8  105.88  65.730  98.52  141.01  74.47  152.81  87.520  110.28  133.57   \n",
      "9  105.28  64.790  97.80  142.74  74.39  156.77  91.360  109.86  134.76   \n",
      "\n",
      "      9    ...     190     191     192    193     194     195    196     197  \\\n",
      "0  187.20  ...  134.22  105.71  156.42  75.52  141.44  194.76  55.51  190.42   \n",
      "1  181.79  ...  133.08  109.95  157.41  75.10  140.66  195.82  56.74  187.70   \n",
      "2  180.57  ...  134.40  109.56  152.90  75.55  141.27  201.22  57.04  188.78   \n",
      "3  180.99  ...  134.65  108.31  152.11  74.65  142.09  201.45  57.86  190.06   \n",
      "4  186.70  ...  134.82  111.58  153.30  74.91  142.84  199.08  58.67  188.93   \n",
      "5  185.16  ...  135.57  111.03  154.73  75.13  141.43  199.26  58.00  182.50   \n",
      "6  182.42  ...  137.45  111.32  151.40  74.18  142.32  199.92  57.44  180.51   \n",
      "7  186.96  ...  130.95  114.27  150.27  74.02  144.32  200.68  58.05  181.05   \n",
      "8  189.21  ...  130.69  114.56  150.69  74.34  145.37  198.91  56.03  181.21   \n",
      "9  187.25  ...  130.91  114.15  149.88  73.13  143.53  197.09  64.85  187.17   \n",
      "\n",
      "      198     199  \n",
      "0  137.44  193.72  \n",
      "1  135.51  189.04  \n",
      "2  135.89  192.41  \n",
      "3  135.13  193.76  \n",
      "4  135.94  190.89  \n",
      "5  139.48  190.22  \n",
      "6  138.93  189.25  \n",
      "7  139.65  188.73  \n",
      "8  135.15  186.63  \n",
      "9  135.81  184.77  \n",
      "\n",
      "[10 rows x 200 columns]\n",
      "\n",
      "Sample of imputed values (original NaN -> new value):\n",
      "Position (0, 169): NaN -> 121.56\n",
      "Position (2, 37): NaN -> 165.07\n",
      "Position (3, 81): NaN -> 197.81\n",
      "Position (4, 6): NaN -> 90.05\n",
      "Position (4, 86): NaN -> 185.51\n",
      "Position (4, 189): NaN -> 162.80\n",
      "Position (6, 118): NaN -> 154.83\n",
      "Position (7, 1): NaN -> 64.77\n",
      "Position (9, 107): NaN -> 132.94\n",
      "Position (10, 83): NaN -> 108.06\n"
     ]
    }
   ],
   "source": [
    "# Initialize with your 600x200 dataset\n",
    "analyzer = SimpleSklearnKNNImputation(financial_data_missing, df)\n",
    "\n",
    "# Run comprehensive analysis\n",
    "results = analyzer.compare_all_methods(n_neighbors = 5)\n",
    "\n",
    "# Access specific results\n",
    "best_imputed = results['Scaled KNN']  # Usually performs best\n",
    "print(pd.DataFrame(best_imputed).head(10))\n",
    "\n",
    "print(\"\\nSample of imputed values (original NaN -> new value):\")\n",
    "for i in range(10):  # Show first 10 missing positions\n",
    "    row, col = np.where(missing_mask)\n",
    "    if i < len(row):\n",
    "        print(f\"Position ({row[i]}, {col[i]}): NaN -> {best_imputed[row[i], col[i]]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e309d13-24c9-4f31-be0b-d26129ee0a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SKLEARN MICE IMPUTATION READY!\n",
      "==================================================\n",
      "Multiple Imputation by Chained Equations with sklearn\n",
      "\n",
      "USAGE:\n",
      "analyzer = SimpleSklearnMICEImputation(data_with_missing, original_data)\n",
      "results = analyzer.compare_all_mice_methods(max_iter=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # Required for IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SimpleSklearnMICEImputation:\n",
    "    \"\"\"\n",
    "    Simple MICE (Multiple Imputation by Chained Equations) using sklearn IterativeImputer.\n",
    "    Clean, straightforward implementation for financial time series data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_with_missing, original_data=None):\n",
    "        \"\"\"\n",
    "        Initialize MICE imputation\n",
    "        \n",
    "        Parameters:\n",
    "        - data_with_missing: DataFrame or numpy array with missing values\n",
    "        - original_data: Original complete data for evaluation (optional)\n",
    "        \"\"\"\n",
    "        self.data_missing = data_with_missing.copy() if hasattr(data_with_missing, 'copy') else data_with_missing.copy()\n",
    "        self.original_data = original_data.copy() if original_data is not None else None\n",
    "        self.imputation_results = {}\n",
    "        self.performance_metrics = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def analyze_missing_pattern(self):\n",
    "        \"\"\"Quick missing value analysis for MICE\"\"\"\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            missing_info = self.data_missing.isnull()\n",
    "            data_shape = self.data_missing.shape\n",
    "        else:\n",
    "            missing_info = pd.DataFrame(np.isnan(self.data_missing))\n",
    "            data_shape = self.data_missing.shape\n",
    "        \n",
    "        total_missing = missing_info.sum().sum()\n",
    "        missing_pct = (total_missing / (data_shape[0] * data_shape[1])) * 100\n",
    "        \n",
    "        print(\"MICE IMPUTATION - MISSING DATA ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Dataset shape: {data_shape}\")\n",
    "        print(f\"Total missing values: {total_missing:,}\")\n",
    "        print(f\"Missing percentage: {missing_pct:.2f}%\")\n",
    "        \n",
    "        if missing_pct > 50:\n",
    "            print(\"WARNING: High missing percentage may affect MICE performance\")\n",
    "        elif missing_pct > 20:\n",
    "            print(\"CAUTION: Moderate missing percentage - MICE will work but may be slow\")\n",
    "        else:\n",
    "            print(\"GOOD: Missing percentage suitable for MICE imputation\")\n",
    "        \n",
    "        return {'total_missing': total_missing, 'missing_pct': missing_pct}\n",
    "    \n",
    "    def mice_imputation_basic(self, max_iter=10, random_state=42):\n",
    "        \"\"\"\n",
    "        Basic MICE imputation with BayesianRidge estimator\n",
    "        \"\"\"\n",
    "        print(f\"\\nBASIC MICE IMPUTATION (max_iter={max_iter})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        print(\"Using BayesianRidge estimator (default)\")\n",
    "        \n",
    "        # Apply MICE imputation\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=BayesianRidge(),\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        print(f\"Convergence after {max_iter} iterations\")\n",
    "        \n",
    "        self.imputation_results['mice_basic'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def mice_imputation_linear(self, max_iter=10, random_state=42):\n",
    "        \"\"\"\n",
    "        MICE imputation with LinearRegression estimator\n",
    "        \"\"\"\n",
    "        print(f\"\\nMICE WITH LINEAR REGRESSION (max_iter={max_iter})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        print(\"Using LinearRegression estimator (faster)\")\n",
    "        \n",
    "        # Apply MICE imputation\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=LinearRegression(),\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        print(f\"Linear regression converged after {max_iter} iterations\")\n",
    "        \n",
    "        self.imputation_results['mice_linear'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def mice_imputation_rf(self, max_iter=5, random_state=42):\n",
    "        \"\"\"\n",
    "        MICE imputation with RandomForest estimator (more robust but slower)\n",
    "        \"\"\"\n",
    "        print(f\"\\nMICE WITH RANDOM FOREST (max_iter={max_iter})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        print(\"Using RandomForest estimator (robust, handles non-linearity)\")\n",
    "        \n",
    "        # Apply MICE imputation with RandomForest\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=RandomForestRegressor(n_estimators=10, random_state=random_state),\n",
    "            max_iter=max_iter,  # Fewer iterations for RF as it's slower\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        print(f\"RandomForest MICE completed\")\n",
    "        \n",
    "        self.imputation_results['mice_rf'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def mice_imputation_scaled(self, max_iter=10, random_state=42):\n",
    "        \"\"\"\n",
    "        MICE imputation with feature scaling (better for financial data)\n",
    "        \"\"\"\n",
    "        print(f\"\\nSCALED MICE IMPUTATION (max_iter={max_iter})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        print(\"Applying StandardScaler before MICE...\")\n",
    "        \n",
    "        # Scale the data first (handles NaN automatically)\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "        \n",
    "        # Apply MICE imputation on scaled data\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=BayesianRidge(),\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        imputed_scaled = imputer.fit_transform(data_scaled)\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        imputed_data = scaler.inverse_transform(imputed_scaled)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        print(f\"Scaled MICE completed and inverse transformed\")\n",
    "        \n",
    "        self.imputation_results['mice_scaled'] = imputed_data\n",
    "        self.scalers['mice_scaled'] = scaler\n",
    "        return imputed_data\n",
    "    \n",
    "    def mice_test_iterations(self, iter_values=[5, 10, 15, 20]):\n",
    "        \"\"\"\n",
    "        Test different iteration counts to find optimal convergence\n",
    "        \"\"\"\n",
    "        print(f\"\\nTESTING DIFFERENT ITERATION COUNTS: {iter_values}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        for max_iter in iter_values:\n",
    "            print(f\"\\nTesting max_iter={max_iter}...\")\n",
    "            \n",
    "            # Apply MICE imputation\n",
    "            imputer = IterativeImputer(\n",
    "                estimator=BayesianRidge(),\n",
    "                max_iter=max_iter,\n",
    "                random_state=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            imputed_data = imputer.fit_transform(data)\n",
    "            \n",
    "            # Quick evaluation if original data available\n",
    "            if self.original_data is not None:\n",
    "                metrics = self.quick_evaluate(imputed_data, f'iter_{max_iter}')\n",
    "                results[f'iter_{max_iter}'] = {\n",
    "                    'data': imputed_data,\n",
    "                    'rmse': metrics['RMSE'] if metrics else None,\n",
    "                    'iterations': max_iter\n",
    "                }\n",
    "                print(f\"  RMSE: {metrics['RMSE']:.3f}\" if metrics else \"  No evaluation data\")\n",
    "            else:\n",
    "                results[f'iter_{max_iter}'] = {'data': imputed_data, 'rmse': None, 'iterations': max_iter}\n",
    "                print(f\"  MICE completed in {max_iter} iterations\")\n",
    "        \n",
    "        # Find best iteration count if evaluation possible\n",
    "        if self.original_data is not None:\n",
    "            best_iter = min(iter_values, key=lambda i: results[f'iter_{i}']['rmse'] if results[f'iter_{i}']['rmse'] else float('inf'))\n",
    "            print(f\"\\nBest iteration count: {best_iter}\")\n",
    "            self.imputation_results['mice_best_iter'] = results[f'iter_{best_iter}']['data']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def quick_evaluate(self, imputed_data, method_name):\n",
    "        \"\"\"Quick evaluation for iteration/method comparison\"\"\"\n",
    "        if self.original_data is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "            \n",
    "            # Find missing positions and extract values\n",
    "            mask = np.isnan(missing_np)\n",
    "            orig_vals = original_np[mask]\n",
    "            imp_vals = imputed_data[mask]\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(np.mean((orig_vals - imp_vals) ** 2))\n",
    "            \n",
    "            return {'RMSE': rmse}\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def evaluate_imputation_quality(self, method_name, imputed_data):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of MICE imputation quality\n",
    "        \"\"\"\n",
    "        if self.original_data is None:\n",
    "            print(f\"\\nNo original data available for {method_name} evaluation\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nMICE EVALUATION: {method_name.upper()}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "            \n",
    "            # Find missing positions\n",
    "            mask = np.isnan(missing_np)\n",
    "            total_missing = np.sum(mask)\n",
    "            \n",
    "            # Extract values\n",
    "            orig_vals = original_np[mask]\n",
    "            imp_vals = imputed_data[mask]\n",
    "            \n",
    "            print(f\"Evaluated {len(orig_vals)} imputed values\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = np.mean(np.abs(orig_vals - imp_vals))\n",
    "            rmse = np.sqrt(np.mean((orig_vals - imp_vals) ** 2))\n",
    "            \n",
    "            # MAPE calculation\n",
    "            nonzero_mask = orig_vals != 0\n",
    "            if np.sum(nonzero_mask) > 0:\n",
    "                mape = np.mean(np.abs(orig_vals - imp_vals)[nonzero_mask] / np.abs(orig_vals[nonzero_mask])) * 100\n",
    "            else:\n",
    "                mape = float('inf')\n",
    "            \n",
    "            # Correlation\n",
    "            if len(orig_vals) > 1:\n",
    "                corr = np.corrcoef(orig_vals, imp_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 1.0\n",
    "            \n",
    "            # MICE-specific: Variance preservation\n",
    "            orig_var = np.var(orig_vals)\n",
    "            imp_var = np.var(imp_vals)\n",
    "            var_ratio = imp_var / orig_var if orig_var > 0 else 1.0\n",
    "            \n",
    "            print(f\"MAE: ${mae:.2f}\")\n",
    "            print(f\"RMSE: ${rmse:.2f}\")\n",
    "            print(f\"MAPE: {mape:.1f}%\" if np.isfinite(mape) else \"MAPE: âˆž\")\n",
    "            print(f\"Correlation: {corr:.3f}\")\n",
    "            print(f\"Variance ratio: {var_ratio:.3f} (1.0 = perfect)\")\n",
    "            \n",
    "            # Financial interpretation\n",
    "            if mape < 2:\n",
    "                print(\"EXCELLENT: MICE captured multivariate relationships very well\")\n",
    "            elif mape < 5:\n",
    "                print(\"GOOD: MICE found meaningful patterns between variables\")\n",
    "            elif mape < 10:\n",
    "                print(\"FAIR: Some multivariate structure captured\")\n",
    "            else:\n",
    "                print(\"POOR: Consider different estimator or more iterations\")\n",
    "            \n",
    "            metrics = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'Correlation': corr,\n",
    "                'Variance_Ratio': var_ratio,\n",
    "                'N_Values': len(orig_vals)\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics[method_name] = metrics\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compare_all_mice_methods(self, max_iter=10):\n",
    "        \"\"\"\n",
    "        Compare different MICE approaches\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MICE IMPUTATION COMPREHENSIVE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Analyze missing pattern\n",
    "        self.analyze_missing_pattern()\n",
    "        \n",
    "        # Test basic MICE\n",
    "        print(f\"\\n{'='*15} BASIC MICE {'='*15}\")\n",
    "        basic_result = self.mice_imputation_basic(max_iter)\n",
    "        self.evaluate_imputation_quality('mice_basic', basic_result)\n",
    "        \n",
    "        # Test linear MICE (faster)\n",
    "        print(f\"\\n{'='*14} LINEAR MICE {'='*14}\")\n",
    "        linear_result = self.mice_imputation_linear(max_iter)\n",
    "        self.evaluate_imputation_quality('mice_linear', linear_result)\n",
    "        \n",
    "        # Test scaled MICE\n",
    "        print(f\"\\n{'='*14} SCALED MICE {'='*14}\")\n",
    "        scaled_result = self.mice_imputation_scaled(max_iter)\n",
    "        self.evaluate_imputation_quality('mice_scaled', scaled_result)\n",
    "        \n",
    "        # Test RandomForest MICE (fewer iterations as it's slower)\n",
    "        print(f\"\\n{'='*12} RANDOM FOREST MICE {'='*12}\")\n",
    "        rf_result = self.mice_imputation_rf(max_iter=5)  # Fewer iterations for RF\n",
    "        self.evaluate_imputation_quality('mice_rf', rf_result)\n",
    "        \n",
    "        # Test different iteration counts\n",
    "        print(f\"\\n{'='*10} ITERATION TESTING {'='*10}\")\n",
    "        iter_results = self.mice_test_iterations([5, 10, 15])\n",
    "        \n",
    "        # Summary comparison\n",
    "        if self.performance_metrics:\n",
    "            self.print_comparison()\n",
    "        \n",
    "        return {\n",
    "            'Basic MICE': basic_result,\n",
    "            'Linear MICE': linear_result,\n",
    "            'Scaled MICE': scaled_result,\n",
    "            'RandomForest MICE': rf_result,\n",
    "            'Iteration Results': iter_results\n",
    "        }\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print MICE method comparison summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MICE COMPARISON SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if self.performance_metrics:\n",
    "            comparison_df = pd.DataFrame(self.performance_metrics).T\n",
    "            print(comparison_df.round(3))\n",
    "            \n",
    "            # Find best method\n",
    "            best_method = comparison_df['RMSE'].idxmin()\n",
    "            print(f\"\\nBest performing: {best_method.replace('_', ' ').title()}\")\n",
    "        \n",
    "        print(f\"\\nMICE RECOMMENDATIONS:\")\n",
    "        print(\"- Basic MICE (BayesianRidge): Good balance of speed and accuracy\")\n",
    "        print(\"- Linear MICE: Fastest for large datasets\")\n",
    "        print(\"- Scaled MICE: Best for financial data with different scales\")\n",
    "        print(\"- RandomForest MICE: Best for non-linear relationships\")\n",
    "        print(\"- 10-15 iterations usually sufficient for convergence\")\n",
    "        \n",
    "        print(f\"\\nADVANTAGES OF MICE:\")\n",
    "        print(\"âœ… Captures relationships between variables\")\n",
    "        print(\"âœ… Preserves variance and distributions\")\n",
    "        print(\"âœ… Handles different variable types\")\n",
    "        print(\"âœ… Provides uncertainty estimates\")\n",
    "\n",
    "\n",
    "# READY TO USE\n",
    "print(\"=\"*50)\n",
    "print(\"SKLEARN MICE IMPUTATION READY!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Multiple Imputation by Chained Equations with sklearn\")\n",
    "print(\"\\nUSAGE:\")\n",
    "print(\"analyzer = SimpleSklearnMICEImputation(data_with_missing, original_data)\")\n",
    "print(\"results = analyzer.compare_all_mice_methods(max_iter=10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ccb7e0a-6484-4abb-a94f-6246c8cd7eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MICE IMPUTATION COMPREHENSIVE ANALYSIS\n",
      "============================================================\n",
      "MICE IMPUTATION - MISSING DATA ANALYSIS\n",
      "==================================================\n",
      "Dataset shape: (700, 200)\n",
      "Total missing values: 1,000\n",
      "Missing percentage: 0.71%\n",
      "GOOD: Missing percentage suitable for MICE imputation\n",
      "\n",
      "=============== BASIC MICE ===============\n",
      "\n",
      "BASIC MICE IMPUTATION (max_iter=10)\n",
      "==================================================\n",
      "Processing 700x200 dataset...\n",
      "Using BayesianRidge estimator (default)\n",
      "Successfully imputed: 1000 values\n",
      "Remaining missing: 0\n",
      "Convergence after 10 iterations\n",
      "\n",
      "MICE EVALUATION: MICE_BASIC\n",
      "==================================================\n",
      "Evaluated 1000 imputed values\n",
      "MAE: $5.13\n",
      "RMSE: $8.76\n",
      "MAPE: 2.2%\n",
      "Correlation: 0.999\n",
      "Variance ratio: 0.993 (1.0 = perfect)\n",
      "GOOD: MICE found meaningful patterns between variables\n",
      "\n",
      "============== LINEAR MICE ==============\n",
      "\n",
      "MICE WITH LINEAR REGRESSION (max_iter=10)\n",
      "==================================================\n",
      "Processing 700x200 dataset...\n",
      "Using LinearRegression estimator (faster)\n",
      "Successfully imputed: 1000 values\n",
      "Remaining missing: 0\n",
      "Linear regression converged after 10 iterations\n",
      "\n",
      "MICE EVALUATION: MICE_LINEAR\n",
      "==================================================\n",
      "Evaluated 1000 imputed values\n",
      "MAE: $5.40\n",
      "RMSE: $8.98\n",
      "MAPE: 2.4%\n",
      "Correlation: 0.999\n",
      "Variance ratio: 0.996 (1.0 = perfect)\n",
      "GOOD: MICE found meaningful patterns between variables\n",
      "\n",
      "============== SCALED MICE ==============\n",
      "\n",
      "SCALED MICE IMPUTATION (max_iter=10)\n",
      "==================================================\n",
      "Processing 700x200 dataset...\n",
      "Applying StandardScaler before MICE...\n",
      "Successfully imputed: 1000 values\n",
      "Remaining missing: 0\n",
      "Scaled MICE completed and inverse transformed\n",
      "\n",
      "MICE EVALUATION: MICE_SCALED\n",
      "==================================================\n",
      "Evaluated 1000 imputed values\n",
      "MAE: $5.15\n",
      "RMSE: $8.51\n",
      "MAPE: 2.3%\n",
      "Correlation: 0.999\n",
      "Variance ratio: 0.992 (1.0 = perfect)\n",
      "GOOD: MICE found meaningful patterns between variables\n",
      "\n",
      "============ RANDOM FOREST MICE ============\n",
      "\n",
      "MICE WITH RANDOM FOREST (max_iter=5)\n",
      "==================================================\n",
      "Processing 700x200 dataset...\n",
      "Using RandomForest estimator (robust, handles non-linearity)\n",
      "Successfully imputed: 1000 values\n",
      "Remaining missing: 0\n",
      "RandomForest MICE completed\n",
      "\n",
      "MICE EVALUATION: MICE_RF\n",
      "==================================================\n",
      "Evaluated 1000 imputed values\n",
      "MAE: $4.24\n",
      "RMSE: $7.79\n",
      "MAPE: 1.8%\n",
      "Correlation: 0.999\n",
      "Variance ratio: 0.991 (1.0 = perfect)\n",
      "EXCELLENT: MICE captured multivariate relationships very well\n",
      "\n",
      "========== ITERATION TESTING ==========\n",
      "\n",
      "TESTING DIFFERENT ITERATION COUNTS: [5, 10, 15]\n",
      "============================================================\n",
      "\n",
      "Testing max_iter=5...\n",
      "  RMSE: 8.762\n",
      "\n",
      "Testing max_iter=10...\n",
      "  RMSE: 8.762\n",
      "\n",
      "Testing max_iter=15...\n",
      "  RMSE: 8.762\n",
      "\n",
      "Best iteration count: 5\n",
      "\n",
      "==================================================\n",
      "MICE COMPARISON SUMMARY\n",
      "==================================================\n",
      "               MAE   RMSE   MAPE  Correlation  Variance_Ratio  N_Values\n",
      "mice_basic   5.133  8.762  2.232        0.999           0.993    1000.0\n",
      "mice_linear  5.398  8.975  2.370        0.999           0.996    1000.0\n",
      "mice_scaled  5.153  8.510  2.255        0.999           0.992    1000.0\n",
      "mice_rf      4.236  7.794  1.754        0.999           0.991    1000.0\n",
      "\n",
      "Best performing: Mice Rf\n",
      "\n",
      "MICE RECOMMENDATIONS:\n",
      "- Basic MICE (BayesianRidge): Good balance of speed and accuracy\n",
      "- Linear MICE: Fastest for large datasets\n",
      "- Scaled MICE: Best for financial data with different scales\n",
      "- RandomForest MICE: Best for non-linear relationships\n",
      "- 10-15 iterations usually sufficient for convergence\n",
      "\n",
      "ADVANTAGES OF MICE:\n",
      "âœ… Captures relationships between variables\n",
      "âœ… Preserves variance and distributions\n",
      "âœ… Handles different variable types\n",
      "âœ… Provides uncertainty estimates\n",
      "      0          1      2       3      4       5          6       7       8    \\\n",
      "0   95.10  66.540000  97.23  139.55  77.47  154.16  92.680000  111.98  144.21   \n",
      "1   95.97  66.900000  96.84  138.36  77.08  152.61  92.970000  111.21  140.78   \n",
      "2   95.20  65.960000  97.85  138.24  76.84  152.71  92.090000  111.73  140.23   \n",
      "3   96.28  65.650000  97.62  140.46  77.83  153.68  90.660000  110.11  137.40   \n",
      "4   97.92  65.250000  96.69  140.20  77.65  152.64  85.347483  110.72  134.15   \n",
      "5   97.09  64.970000  94.59  143.59  77.74  152.93  87.180000  112.33  132.93   \n",
      "6  108.48  64.060000  96.09  142.73  77.26  152.08  87.360000  110.76  134.13   \n",
      "7  107.99  64.821606  97.12  143.62  79.09  152.82  87.200000  110.20  134.98   \n",
      "8  105.88  65.730000  98.52  141.01  74.47  152.81  87.520000  110.28  133.57   \n",
      "9  105.28  64.790000  97.80  142.74  74.39  156.77  91.360000  109.86  134.76   \n",
      "\n",
      "      9    ...     190     191     192    193     194     195    196     197  \\\n",
      "0  187.20  ...  134.22  105.71  156.42  75.52  141.44  194.76  55.51  190.42   \n",
      "1  181.79  ...  133.08  109.95  157.41  75.10  140.66  195.82  56.74  187.70   \n",
      "2  180.57  ...  134.40  109.56  152.90  75.55  141.27  201.22  57.04  188.78   \n",
      "3  180.99  ...  134.65  108.31  152.11  74.65  142.09  201.45  57.86  190.06   \n",
      "4  186.70  ...  134.82  111.58  153.30  74.91  142.84  199.08  58.67  188.93   \n",
      "5  185.16  ...  135.57  111.03  154.73  75.13  141.43  199.26  58.00  182.50   \n",
      "6  182.42  ...  137.45  111.32  151.40  74.18  142.32  199.92  57.44  180.51   \n",
      "7  186.96  ...  130.95  114.27  150.27  74.02  144.32  200.68  58.05  181.05   \n",
      "8  189.21  ...  130.69  114.56  150.69  74.34  145.37  198.91  56.03  181.21   \n",
      "9  187.25  ...  130.91  114.15  149.88  73.13  143.53  197.09  64.85  187.17   \n",
      "\n",
      "      198     199  \n",
      "0  137.44  193.72  \n",
      "1  135.51  189.04  \n",
      "2  135.89  192.41  \n",
      "3  135.13  193.76  \n",
      "4  135.94  190.89  \n",
      "5  139.48  190.22  \n",
      "6  138.93  189.25  \n",
      "7  139.65  188.73  \n",
      "8  135.15  186.63  \n",
      "9  135.81  184.77  \n",
      "\n",
      "[10 rows x 200 columns]\n",
      "\n",
      "Sample of imputed values (original NaN -> new value):\n",
      "Position (0, 169): NaN -> 122.05\n",
      "Position (2, 37): NaN -> 163.99\n",
      "Position (3, 81): NaN -> 204.19\n",
      "Position (4, 6): NaN -> 85.35\n",
      "Position (4, 86): NaN -> 187.84\n",
      "Position (4, 189): NaN -> 169.86\n",
      "Position (6, 118): NaN -> 156.80\n",
      "Position (7, 1): NaN -> 64.82\n",
      "Position (9, 107): NaN -> 132.38\n",
      "Position (10, 83): NaN -> 122.52\n"
     ]
    }
   ],
   "source": [
    "analyzer = SimpleSklearnMICEImputation(financial_data_missing, df)\n",
    "\n",
    "# Run comprehensive analysis\n",
    "results = analyzer.compare_all_mice_methods(max_iter=10)\n",
    "\n",
    "# Access specific methods\n",
    "scaled_mice = results['Scaled MICE']  # Usually best for financial data\n",
    "print(pd.DataFrame(scaled_mice).head(10))\n",
    "\n",
    "print(\"\\nSample of imputed values from Scaled MICE (original NaN -> new value):\")\n",
    "for i in range(10):  # Show first 10 missing positions\n",
    "    row, col = np.where(missing_mask)\n",
    "    if i < len(row):\n",
    "        print(f\"Position ({row[i]}, {col[i]}): NaN -> {scaled_mice[row[i], col[i]]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
