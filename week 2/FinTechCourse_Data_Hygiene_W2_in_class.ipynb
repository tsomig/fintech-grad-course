{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a4fd4-1331-4c2f-8baa-64ee57d08850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabc6b8-b49c-4a77-91df-020d1808bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_financial_timeseries(rows, cols, start_price=100):\n",
    "    \"\"\"\n",
    "    Generate realistic financial time series data with the following characteristics:\n",
    "    - Geometric Brownian Motion for price evolution\n",
    "    - Realistic volatility (15-25% annualized)\n",
    "    - Occasional jumps and mean reversion\n",
    "    - Correlation between some assets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters for realistic financial data\n",
    "    dt = 1/252  # Daily time step (252 trading days per year)\n",
    "    mu = 0.08   # Annual drift (8% expected return)\n",
    "    \n",
    "    # Initialize the price matrix\n",
    "    prices = np.zeros((rows, cols))\n",
    "    \n",
    "    # Set different starting prices for each asset (columns)\n",
    "    start_prices = np.random.uniform(50, 200, cols)\n",
    "    prices[0, :] = start_prices\n",
    "    \n",
    "    for t in range(1, rows):\n",
    "        for asset in range(cols):\n",
    "            # Individual asset volatility (15-25% annualized)\n",
    "            sigma = np.random.uniform(0.15, 0.25)\n",
    "            \n",
    "            # Add some correlation with previous asset (except first one)\n",
    "            if asset > 0:\n",
    "                correlation_factor = 0.3 * np.random.randn()\n",
    "                correlated_shock = correlation_factor * (prices[t-1, asset-1] / prices[t-2, asset-1] - 1) if t > 1 else 0\n",
    "            else:\n",
    "                correlated_shock = 0\n",
    "            \n",
    "            # Geometric Brownian Motion with occasional jumps\n",
    "            random_shock = np.random.randn()\n",
    "            \n",
    "            # Add occasional jump (5% probability)\n",
    "            if np.random.random() < 0.05:\n",
    "                jump = np.random.uniform(-0.1, 0.15)  # Jump between -10% and +15%\n",
    "            else:\n",
    "                jump = 0\n",
    "            \n",
    "            # Price evolution: S(t+1) = S(t) * exp((mu - 0.5*sigma^2)*dt + sigma*sqrt(dt)*Z + jump)\n",
    "            log_return = (mu - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * random_shock + jump + correlated_shock\n",
    "            \n",
    "            prices[t, asset] = prices[t-1, asset] * np.exp(log_return)\n",
    "    \n",
    "    # Round to 2 decimal places for realistic price formatting\n",
    "    prices = np.round(prices, 2)\n",
    "    \n",
    "    return prices\n",
    "\n",
    "rows = 700\n",
    "col = 200\n",
    "# Generate the dataset\n",
    "financial_data = generate_financial_timeseries(rows, col)\n",
    "\n",
    "# Create DataFrame with proper labeling\n",
    "dates = [datetime(2024, 1, 1) + timedelta(days=i) for i in range(rows)]\n",
    "asset_names = [f\"ASSET_{i+1:02d}\" for i in range(col)]\n",
    "\n",
    "df = pd.DataFrame(financial_data, \n",
    "                 index=pd.to_datetime(dates),\n",
    "                 columns=asset_names)\n",
    "\n",
    "print(\"Financial Time Series Dataset\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Date Range: {df.index[0].strftime('%Y-%m-%d')} to {df.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"Price Range: ${df.values.min():.2f} - ${df.values.max():.2f}\")\n",
    "print(\"\\nDataset Preview:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72940e4-71c4-4f21-9f28-5c0f601e2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE the DATASET ons the server\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import getpass\n",
    "\n",
    "password = getpass.getpass(\"Database password: \")\n",
    "engine = create_engine(f\"postgresql://postgres:{password}@localhost:5432/fintech_db\")\n",
    "# Reset index to make dates a proper column\n",
    "df_to_save = df.reset_index()\n",
    "df_to_save = df_to_save.rename(columns={'index': 'date'})\n",
    "\n",
    "# Save to PostgreSQL server\n",
    "df_to_save.to_sql('asset_prices', engine, if_exists='replace', index=False)\n",
    "engine.dispose()\n",
    "print(\"✅ Financial time series data saved to server!\")\n",
    "print(f\"📊 Saved: {len(df_to_save)} rows × {len(df_to_save.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727adadc-5e86-48b3-951d-26614bda6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD the DATASET from the server\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import getpass\n",
    "\n",
    "password = getpass.getpass(\"Database password: \")\n",
    "engine = create_engine(f\"postgresql://postgres:{password}@localhost:5432/fintech_db\")\n",
    "\n",
    "# Load from PostgreSQL server\n",
    "df_loaded = pd.read_sql(\"SELECT * FROM asset_prices\", engine)\n",
    "\n",
    "# Convert date column back to datetime and set as index\n",
    "df_loaded['date'] = pd.to_datetime(df_loaded['date'])\n",
    "df_loaded = df_loaded.set_index('date')\n",
    "\n",
    "df = df_loaded # rename\n",
    "engine.dispose()\n",
    "\n",
    "print(\"✅ Financial time series data loaded from server!\")\n",
    "print(f\"📊 Loaded: {df.shape}\")\n",
    "print(f\"📅 Date range: {df.index[0]} to {df.index[-1]}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4a4e9-f947-4ceb-a905-57d603c8154b",
   "metadata": {},
   "source": [
    "\n",
    "'''Strategies for Handling Missing Numeric Values'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95667a-c787-4d79-b316-5a090d57701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISSING VALUES FUNCTION = create them\n",
    "def introduce_missing_values(data, n_missing, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Work with a copy to preserve original data\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data_with_missing = data.copy()\n",
    "        rows, cols = data.shape\n",
    "    else:\n",
    "        data_with_missing = data.copy().astype(float)  # Convert to float to allow NaN\n",
    "        rows, cols = data.shape\n",
    "    \n",
    "    # Generate random positions for missing values\n",
    "    total_positions = rows * cols\n",
    "    missing_indices = np.random.choice(total_positions, size=n_missing, replace=False)\n",
    "    \n",
    "    # Convert flat indices to (row, col) positions\n",
    "    missing_positions = [(idx // cols, idx % cols) for idx in missing_indices]\n",
    "    \n",
    "    # Introduce missing values\n",
    "    for row, col in missing_positions:\n",
    "        if isinstance(data_with_missing, pd.DataFrame):\n",
    "            data_with_missing.iloc[row, col] = np.nan\n",
    "        else:\n",
    "            data_with_missing[row, col] = np.nan\n",
    "    \n",
    "    return data_with_missing, missing_positions\n",
    "\n",
    "# Apply missing values to both DataFrame and numpy array\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTRODUCING 1000 RANDOM MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For DataFrame\n",
    "df_missing, missing_pos_df = introduce_missing_values(df, n_missing = 1000)\n",
    "\n",
    "# For numpy array\n",
    "financial_data_missing, missing_pos_array = introduce_missing_values(df, n_missing = 1000)\n",
    "\n",
    "print(f\"\\nMissing values introduced at positions (row, col):\")\n",
    "for i, (row, col) in enumerate(missing_pos_df, 1):\n",
    "    asset_name = df.columns[col] if col < len(df.columns) else f\"Asset_{col}\"\n",
    "    date_str = df.index[row].strftime('%Y-%m-%d') if row < len(df.index) else f\"Day_{row}\"\n",
    "    print(f\"{i:2d}. Position ({row:2d}, {col:2d}) - {asset_name} on {date_str}\")\n",
    "\n",
    "print(f\"\\nDataFrame with missing values:\")\n",
    "print(\"Missing values count:\", df_missing.isnull().sum().sum())\n",
    "#print(df_missing.head(20))\n",
    "\n",
    "print(f\"\\nNumPy array with missing values:\")\n",
    "print(\"Missing values count:\", (np.isnan(financial_data_missing).sum()))\n",
    "print(\"Missing values count:\", sum(np.isnan(financial_data_missing).sum()))\n",
    "print(\"Shape:\", financial_data_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1ad16-6bea-4b4f-a07e-ea218e220511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FinancialImputationAnalyzer:\n",
    "    \"\"\"\n",
    "    Advanced imputation analyzer specifically designed for financial time series data.\n",
    "    Implements mean/median imputation with financial data considerations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_with_missing, original_data=None):\n",
    "        \"\"\"\n",
    "        Initialize the imputation analyzer\n",
    "        \n",
    "        Parameters:\n",
    "        - data_with_missing: DataFrame or numpy array with missing values\n",
    "        - original_data: Original complete data for evaluation (optional)\n",
    "        \"\"\"\n",
    "        self.data_missing = data_with_missing.copy() if hasattr(data_with_missing, 'copy') else data_with_missing.copy()\n",
    "        self.original_data = original_data.copy() if original_data is not None else None\n",
    "        self.imputation_results = {}\n",
    "        self.performance_metrics = {}\n",
    "        \n",
    "    def analyze_missing_pattern(self):\n",
    "        \"\"\"Analyze the pattern of missing values for financial context\"\"\"\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            missing_info = self.data_missing.isnull()\n",
    "        else:\n",
    "            missing_info = pd.DataFrame(np.isnan(self.data_missing))\n",
    "        \n",
    "        print(\"🔍 MISSING VALUE PATTERN ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_missing = missing_info.sum().sum()\n",
    "        total_cells = missing_info.shape[0] * missing_info.shape[1]\n",
    "        missing_pct = (total_missing / total_cells) * 100\n",
    "        \n",
    "        print(f\"Dataset shape: {missing_info.shape}\")\n",
    "        print(f\"Total missing values: {total_missing:,}\")\n",
    "        print(f\"Missing percentage: {missing_pct:.2f}%\")\n",
    "        \n",
    "        # Missing by time period (rows)\n",
    "        missing_by_row = missing_info.sum(axis=1)\n",
    "        print(f\"\\nMissing values per time period:\")\n",
    "        print(f\"  Min: {missing_by_row.min()}\")\n",
    "        print(f\"  Max: {missing_by_row.max()}\")\n",
    "        print(f\"  Mean: {missing_by_row.mean():.1f}\")\n",
    "        \n",
    "        # Missing by asset (columns)\n",
    "        missing_by_col = missing_info.sum(axis=0)\n",
    "        print(f\"\\nMissing values per asset:\")\n",
    "        print(f\"  Min: {missing_by_col.min()}\")\n",
    "        print(f\"  Max: {missing_by_col.max()}\")\n",
    "        print(f\"  Mean: {missing_by_col.mean():.1f}\")\n",
    "        \n",
    "        # Financial data specific checks\n",
    "        print(f\"\\n📊 FINANCIAL DATA QUALITY CHECKS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check for consecutive missing values (problematic for time series)\n",
    "        consecutive_missing = []\n",
    "        for col in range(missing_info.shape[1]):\n",
    "            col_missing = missing_info.iloc[:, col]\n",
    "            consecutive = 0\n",
    "            max_consecutive = 0\n",
    "            for val in col_missing:\n",
    "                if val:\n",
    "                    consecutive += 1\n",
    "                    max_consecutive = max(max_consecutive, consecutive)\n",
    "                else:\n",
    "                    consecutive = 0\n",
    "            consecutive_missing.append(max_consecutive)\n",
    "        \n",
    "        max_consecutive_overall = max(consecutive_missing)\n",
    "        print(f\"Maximum consecutive missing values: {max_consecutive_overall}\")\n",
    "        \n",
    "        if max_consecutive_overall > 5:\n",
    "            print(\"⚠️  WARNING: Long consecutive missing periods detected!\")\n",
    "            print(\"   Consider using interpolation instead of mean/median\")\n",
    "        else:\n",
    "            print(\"✅ Missing pattern suitable for mean/median imputation\")\n",
    "            \n",
    "        return {\n",
    "            'total_missing': total_missing,\n",
    "            'missing_pct': missing_pct,\n",
    "            'max_consecutive': max_consecutive_overall,\n",
    "            'missing_by_row': missing_by_row,\n",
    "            'missing_by_col': missing_by_col\n",
    "        }\n",
    "    \n",
    "    def simple_mean_imputation(self):\n",
    "        \"\"\"\n",
    "        Simple mean imputation - replaces missing values with column mean\n",
    "        ⚠️ WARNING: This distorts variance and ignores time series nature\n",
    "        \"\"\"\n",
    "        print(\"\\n📊 SIMPLE MEAN IMPUTATION\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                mean_val = imputed_data[col].mean()\n",
    "                imputed_data[col].fillna(mean_val, inplace=True)\n",
    "                print(f\"  {col}: filled with mean {mean_val:.2f}\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "                mean_val = np.nanmean(col_data)\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = mean_val\n",
    "                if col < 5:  # Print first 5 for brevity\n",
    "                    print(f\"  Asset {col+1:03d}: filled with mean {mean_val:.2f}\")\n",
    "            \n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1]-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['simple_mean'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def simple_median_imputation(self):\n",
    "        \"\"\"\n",
    "        Simple median imputation - replaces missing values with column median\n",
    "        More robust to outliers than mean\n",
    "        \"\"\"\n",
    "        print(\"\\n📊 SIMPLE MEDIAN IMPUTATION\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                median_val = imputed_data[col].median()\n",
    "                imputed_data[col].fillna(median_val, inplace=True)\n",
    "                print(f\"  {col}: filled with median {median_val:.2f}\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "                median_val = np.nanmedian(col_data)\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = median_val\n",
    "                if col < 5:  # Print first 5 for brevity\n",
    "                    print(f\"  Asset {col+1:03d}: filled with median {median_val:.2f}\")\n",
    "            \n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1]-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['simple_median'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def rolling_mean_imputation(self, window=30):\n",
    "        \"\"\"\n",
    "        Rolling mean imputation - uses local time window mean\n",
    "        Better for financial time series as it adapts to local trends\n",
    "        \"\"\"\n",
    "        print(f\"\\n📊 ROLLING MEAN IMPUTATION (Window: {window})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                # Calculate rolling mean\n",
    "                rolling_mean = imputed_data[col].rolling(window=window, center=True, min_periods=1).mean()\n",
    "                # Fill missing values\n",
    "                imputed_data[col] = imputed_data[col].fillna(rolling_mean)\n",
    "                # If still missing (edge cases), use global mean\n",
    "                global_mean = imputed_data[col].mean()\n",
    "                imputed_data[col] = imputed_data[col].fillna(global_mean)\n",
    "                \n",
    "                filled_count = self.data_missing[col].isnull().sum()\n",
    "                print(f\"  {col}: filled {filled_count} values with rolling mean\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "                \n",
    "                # Create rolling mean using pandas for convenience\n",
    "                temp_series = pd.Series(col_data)\n",
    "                rolling_mean = temp_series.rolling(window=window, center=True, min_periods=1).mean()\n",
    "                \n",
    "                # Fill missing values\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = rolling_mean[mask]\n",
    "                \n",
    "                # Handle remaining NaN with global mean\n",
    "                remaining_nan = np.isnan(imputed_data[:, col])\n",
    "                if remaining_nan.any():\n",
    "                    global_mean = np.nanmean(imputed_data[:, col])\n",
    "                    imputed_data[remaining_nan, col] = global_mean\n",
    "                \n",
    "                if col < 5:\n",
    "                    filled_count = mask.sum()\n",
    "                    print(f\"  Asset {col+1:03d}: filled {filled_count} values with rolling mean\")\n",
    "            \n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1]-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['rolling_mean'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def rolling_median_imputation(self, window=30):\n",
    "        \"\"\"\n",
    "        Rolling median imputation - uses local time window median\n",
    "        Even more robust to outliers, good for volatile financial data\n",
    "        \"\"\"\n",
    "        print(f\"\\n📊 ROLLING MEDIAN IMPUTATION (Window: {window})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                # Calculate rolling median\n",
    "                rolling_median = imputed_data[col].rolling(window=window, center=True, min_periods=1).median()\n",
    "                # Fill missing values\n",
    "                imputed_data[col] = imputed_data[col].fillna(rolling_median)\n",
    "                # If still missing (edge cases), use global median\n",
    "                global_median = imputed_data[col].median()\n",
    "                imputed_data[col] = imputed_data[col].fillna(global_median)\n",
    "                \n",
    "                filled_count = self.data_missing[col].isnull().sum()\n",
    "                print(f\"  {col}: filled {filled_count} values with rolling median\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "                \n",
    "                # Create rolling median using pandas for convenience\n",
    "                temp_series = pd.Series(col_data)\n",
    "                rolling_median = temp_series.rolling(window=window, center=True, min_periods=1).median()\n",
    "                \n",
    "                # Fill missing values\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = rolling_median[mask]\n",
    "                \n",
    "                # Handle remaining NaN with global median\n",
    "                remaining_nan = np.isnan(imputed_data[:, col])\n",
    "                if remaining_nan.any():\n",
    "                    global_median = np.nanmedian(imputed_data[:, col])\n",
    "                    imputed_data[remaining_nan, col] = global_median\n",
    "                \n",
    "                if col < 5:\n",
    "                    filled_count = mask.sum()\n",
    "                    print(f\"  Asset {col+1:03d}: filled {filled_count} values with rolling median\")\n",
    "            \n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1]-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['rolling_median'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def evaluate_imputation_quality(self, method_name, imputed_data):\n",
    "        \"\"\"\n",
    "        Evaluate imputation quality if original data is available - NO SKLEARN VERSION\n",
    "        \"\"\"\n",
    "        if self.original_data is None:\n",
    "            print(f\"\\n⚠️  No original data available for {method_name} evaluation\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\n📈 IMPUTATION QUALITY EVALUATION: {method_name.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Force everything to be numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "                \n",
    "            if hasattr(imputed_data, 'values'):\n",
    "                imputed_np = imputed_data.values.copy()\n",
    "            else:\n",
    "                imputed_np = imputed_data.copy()\n",
    "            \n",
    "            print(f\"  Data shapes - Missing: {missing_np.shape}, Original: {original_np.shape}, Imputed: {imputed_np.shape}\")\n",
    "            \n",
    "            # Create mask\n",
    "            mask = np.isnan(missing_np)\n",
    "            total_missing = np.sum(mask)\n",
    "            print(f\"  Found {total_missing} missing values to evaluate\")\n",
    "            \n",
    "            if total_missing == 0:\n",
    "                print(\"  No missing values found!\")\n",
    "                return None\n",
    "            \n",
    "            # Extract values using simple indexing\n",
    "            orig_vals = []\n",
    "            imp_vals = []\n",
    "            \n",
    "            rows, cols = missing_np.shape\n",
    "            for i in range(rows):\n",
    "                for j in range(cols):\n",
    "                    if mask[i, j]:  # This was originally missing\n",
    "                        orig_vals.append(original_np[i, j])\n",
    "                        imp_vals.append(imputed_np[i, j])\n",
    "            \n",
    "            orig_vals = np.array(orig_vals)\n",
    "            imp_vals = np.array(imp_vals)\n",
    "            \n",
    "            print(f\"  Extracted {len(orig_vals)} value pairs\")\n",
    "            \n",
    "            # Simple metrics - MANUAL CALCULATION ONLY\n",
    "            differences = orig_vals - imp_vals\n",
    "            abs_differences = np.abs(differences)\n",
    "            \n",
    "            mae = np.mean(abs_differences)\n",
    "            mse = np.mean(differences**2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Simple percentage error (avoid division by zero)\n",
    "            nonzero_mask = orig_vals != 0\n",
    "            if np.sum(nonzero_mask) > 0:\n",
    "                pct_errors = abs_differences[nonzero_mask] / np.abs(orig_vals[nonzero_mask])\n",
    "                mape = np.mean(pct_errors) * 100\n",
    "            else:\n",
    "                mape = float('inf')\n",
    "            \n",
    "            # Simple correlation\n",
    "            if len(orig_vals) > 1:\n",
    "                corr = np.corrcoef(orig_vals, imp_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 1.0\n",
    "                \n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "            print(f\"  RMSE: {rmse:.4f}\")\n",
    "            print(f\"  MAPE: {mape:.2f}%\" if np.isfinite(mape) else \"  MAPE: ∞\")\n",
    "            print(f\"  Correlation: {corr:.4f}\")\n",
    "            \n",
    "            # Financial interpretation\n",
    "            print(f\"\\n💰 FINANCIAL INTERPRETATION:\")\n",
    "            print(f\"  Average price difference: ${mae:.2f}\")\n",
    "            print(f\"  Typical error magnitude: ${rmse:.2f}\")\n",
    "            \n",
    "            if np.isfinite(mape):\n",
    "                if mape < 5:\n",
    "                    print(\"  ✅ EXCELLENT: Very accurate imputation\")\n",
    "                elif mape < 10:\n",
    "                    print(\"  ✅ GOOD: Acceptable imputation quality\")\n",
    "                elif mape < 20:\n",
    "                    print(\"  ⚠️  FAIR: Moderate imputation errors\")\n",
    "                else:\n",
    "                    print(\"  ❌ POOR: High imputation errors\")\n",
    "            else:\n",
    "                print(\"  ⚠️  Note: MAPE could not be calculated due to zero values\")\n",
    "            \n",
    "            metrics = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse, \n",
    "                'MAPE': mape,\n",
    "                'Correlation': corr,\n",
    "                'N_Values': len(orig_vals)\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics[method_name] = metrics\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR in evaluation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def compare_all_methods(self, window=30):\n",
    "        \"\"\"\n",
    "        Run all imputation methods and compare results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🎯 COMPREHENSIVE MEAN/MEDIAN IMPUTATION ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Analyze missing pattern first\n",
    "        self.analyze_missing_pattern()\n",
    "        \n",
    "        # Run all methods\n",
    "        methods = {\n",
    "            'Simple Mean': self.simple_mean_imputation,\n",
    "            'Simple Median': self.simple_median_imputation,\n",
    "            'Rolling Mean': lambda: self.rolling_mean_imputation(window),\n",
    "            'Rolling Median': lambda: self.rolling_median_imputation(window)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, method in methods.items():\n",
    "            print(f\"\\n{'='*20} {name.upper()} {'='*20}\")\n",
    "            imputed = method()\n",
    "            results[name] = imputed\n",
    "            \n",
    "            # Evaluate quality\n",
    "            metrics = self.evaluate_imputation_quality(name.lower().replace(' ', '_'), imputed)\n",
    "        \n",
    "        # Summary comparison\n",
    "        if self.performance_metrics:\n",
    "            self.print_method_comparison()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_method_comparison(self):\n",
    "        \"\"\"Print comparison of all methods\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🏆 METHOD COMPARISON SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        comparison_df = pd.DataFrame(self.performance_metrics).T\n",
    "        print(comparison_df.round(4))\n",
    "        \n",
    "        # Find best method by lowest RMSE\n",
    "        best_method = comparison_df['RMSE'].idxmin()\n",
    "        print(f\"\\n🏆 BEST PERFORMING METHOD: {best_method.replace('_', ' ').title()}\")\n",
    "        print(f\"   RMSE: {comparison_df.loc[best_method, 'RMSE']:.4f}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\n💡 RECOMMENDATIONS FOR FINANCIAL TIME SERIES:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if 'rolling_median' in self.performance_metrics:\n",
    "            print(\"🥇 BEST PRACTICE: Rolling Median\")\n",
    "            print(\"   ✅ Adapts to local market conditions\")\n",
    "            print(\"   ✅ Robust to price spikes/crashes\")\n",
    "            print(\"   ✅ Preserves time series properties\")\n",
    "        \n",
    "        print(f\"\\n🥈 ALTERNATIVE: Rolling Mean\")\n",
    "        print(\"   ✅ Good for stable market periods\")\n",
    "        print(\"   ⚠️  Sensitive to outliers\")\n",
    "        \n",
    "        print(f\"\\n⚠️  AVOID: Simple Mean/Median\")\n",
    "        print(\"   ❌ Ignores time series structure\")\n",
    "        print(\"   ❌ Can create artificial patterns\")\n",
    "        print(\"   ❌ Distorts volatility\")\n",
    "        \n",
    "        print(f\"\\n🔄 NEXT STEPS:\")\n",
    "        print(\"   1. Try interpolation methods (linear, spline)\")\n",
    "        print(\"   2. Consider forward/backward fill\")\n",
    "        print(\"   3. Test advanced methods (KNN, MICE)\")\n",
    "\n",
    "\n",
    "# READY TO USE\n",
    "print(\"=\" * 80)\n",
    "print(\"FINANCIAL IMPUTATION ANALYZER\")\n",
    "print(\"=\" * 80)\n",
    "print(\"analyzer = FinancialImputationAnalyzer(data_with_missing, original_data)\")\n",
    "print(\"results = analyzer.compare_all_methods(window=30)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1dcc66-630b-4df9-ab77-45283a820f22",
   "metadata": {},
   "source": [
    "Prompt Engineer to run the analyzeer and results + show where the imputation occured and also show some specific imputed values\n",
    "USE the two datasets\n",
    "financial_data_missing\n",
    "&\n",
    "df (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def8108-3bcf-41ff-9daf-66dd8f0fa4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialForwardBackwardFill:\n",
    "    \"\"\"\n",
    "    Forward/Backward Fill imputation for financial time series data.\n",
    "    LOCF = Last Observation Carried Forward\n",
    "    NOCB = Next Observation Carried Backward\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_with_missing, original_data=None):\n",
    "        \"\"\"\n",
    "        Initialize the forward/backward fill analyzer\n",
    "        \n",
    "        Parameters:\n",
    "        - data_with_missing: DataFrame or numpy array with missing values\n",
    "        - original_data: Original complete data for evaluation (optional)\n",
    "        \"\"\"\n",
    "        self.data_missing = data_with_missing.copy() if hasattr(data_with_missing, 'copy') else data_with_missing.copy()\n",
    "        self.original_data = original_data.copy() if original_data is not None else None\n",
    "        self.imputation_results = {}\n",
    "        self.performance_metrics = {}\n",
    "        \n",
    "    def analyze_missing_pattern(self):\n",
    "        \"\"\"Analyze missing pattern - critical for forward/backward fill\"\"\"\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            missing_info = self.data_missing.isnull()\n",
    "        else:\n",
    "            missing_info = pd.DataFrame(np.isnan(self.data_missing))\n",
    "        \n",
    "        print(\"Forward/Backward Fill - Missing Pattern Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        total_missing = missing_info.sum().sum()\n",
    "        total_cells = missing_info.shape[0] * missing_info.shape[1]\n",
    "        missing_pct = (total_missing / total_cells) * 100\n",
    "        \n",
    "        print(f\"Dataset shape: {missing_info.shape}\")\n",
    "        print(f\"Total missing values: {total_missing:,}\")\n",
    "        print(f\"Missing percentage: {missing_pct:.2f}%\")\n",
    "        \n",
    "        # Check for edge cases (first/last row missing)\n",
    "        first_row_missing = missing_info.iloc[0, :].sum()\n",
    "        last_row_missing = missing_info.iloc[-1, :].sum()\n",
    "        \n",
    "        print(f\"\\nEdge case analysis:\")\n",
    "        print(f\"  First row missing values: {first_row_missing}\")\n",
    "        print(f\"  Last row missing values: {last_row_missing}\")\n",
    "        \n",
    "        if first_row_missing > 0:\n",
    "            print(\"  WARNING: First row has missing values - forward fill will fail here\")\n",
    "        if last_row_missing > 0:\n",
    "            print(\"  WARNING: Last row has missing values - backward fill will fail here\")\n",
    "            \n",
    "        # Analyze consecutive gaps\n",
    "        max_consecutive_gaps = []\n",
    "        for col in range(missing_info.shape[1]):\n",
    "            col_missing = missing_info.iloc[:, col]\n",
    "            consecutive = 0\n",
    "            max_consecutive = 0\n",
    "            for val in col_missing:\n",
    "                if val:\n",
    "                    consecutive += 1\n",
    "                    max_consecutive = max(max_consecutive, consecutive)\n",
    "                else:\n",
    "                    consecutive = 0\n",
    "            max_consecutive_gaps.append(max_consecutive)\n",
    "        \n",
    "        overall_max_gap = max(max_consecutive_gaps) if max_consecutive_gaps else 0\n",
    "        print(f\"  Maximum consecutive gap: {overall_max_gap} periods\")\n",
    "        \n",
    "        if overall_max_gap > 10:\n",
    "            print(\"  WARNING: Long consecutive gaps detected!\")\n",
    "            print(\"  Consider interpolation for gaps > 10 periods\")\n",
    "        else:\n",
    "            print(\"  GOOD: Gap length suitable for forward/backward fill\")\n",
    "            \n",
    "        return {\n",
    "            'total_missing': total_missing,\n",
    "            'missing_pct': missing_pct,\n",
    "            'first_row_missing': first_row_missing,\n",
    "            'last_row_missing': last_row_missing,\n",
    "            'max_gap': overall_max_gap\n",
    "        }\n",
    "    \n",
    "    def forward_fill_locf(self):\n",
    "        \"\"\"\n",
    "        Forward Fill (LOCF) - Last Observation Carried Forward\n",
    "        Uses previous valid value to fill missing data\n",
    "        \"\"\"\n",
    "        print(\"\\nFORWARD FILL (LOCF) - Last Observation Carried Forward\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            \n",
    "            # Forward fill each column\n",
    "            for col in imputed_data.columns:\n",
    "                before_count = imputed_data[col].isnull().sum()\n",
    "                imputed_data[col] = imputed_data[col].fillna(method='ffill')\n",
    "                after_count = imputed_data[col].isnull().sum()\n",
    "                filled_count = before_count - after_count\n",
    "                print(f\"  {col}: filled {filled_count} values via forward fill\")\n",
    "                \n",
    "                if after_count > 0:\n",
    "                    print(f\"    WARNING: {after_count} values still missing (no prior value available)\")\n",
    "        \n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            rows, cols = imputed_data.shape\n",
    "            \n",
    "            for col in range(cols):\n",
    "                filled_count = 0\n",
    "                remaining_missing = 0\n",
    "                \n",
    "                # Forward fill column by column\n",
    "                for row in range(1, rows):  # Start from row 1\n",
    "                    if np.isnan(imputed_data[row, col]):\n",
    "                        if not np.isnan(imputed_data[row-1, col]):\n",
    "                            # Fill with previous value\n",
    "                            imputed_data[row, col] = imputed_data[row-1, col]\n",
    "                            filled_count += 1\n",
    "                        else:\n",
    "                            remaining_missing += 1\n",
    "                \n",
    "                if col < 5:  # Print first 5 for brevity\n",
    "                    print(f\"  Asset {col+1:03d}: filled {filled_count} values\")\n",
    "                    if remaining_missing > 0:\n",
    "                        print(f\"    WARNING: {remaining_missing} values still missing\")\n",
    "            \n",
    "            if cols > 5:\n",
    "                print(f\"  ... processed {cols-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['forward_fill'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def backward_fill_nocb(self):\n",
    "        \"\"\"\n",
    "        Backward Fill (NOCB) - Next Observation Carried Backward\n",
    "        Uses next valid value to fill missing data\n",
    "        \"\"\"\n",
    "        print(\"\\nBACKWARD FILL (NOCB) - Next Observation Carried Backward\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            \n",
    "            # Backward fill each column\n",
    "            for col in imputed_data.columns:\n",
    "                before_count = imputed_data[col].isnull().sum()\n",
    "                imputed_data[col] = imputed_data[col].fillna(method='bfill')\n",
    "                after_count = imputed_data[col].isnull().sum()\n",
    "                filled_count = before_count - after_count\n",
    "                print(f\"  {col}: filled {filled_count} values via backward fill\")\n",
    "                \n",
    "                if after_count > 0:\n",
    "                    print(f\"    WARNING: {after_count} values still missing (no future value available)\")\n",
    "        \n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            rows, cols = imputed_data.shape\n",
    "            \n",
    "            for col in range(cols):\n",
    "                filled_count = 0\n",
    "                remaining_missing = 0\n",
    "                \n",
    "                # Backward fill column by column (go backwards)\n",
    "                for row in range(rows-2, -1, -1):  # Start from second-to-last row, go to row 0\n",
    "                    if np.isnan(imputed_data[row, col]):\n",
    "                        if not np.isnan(imputed_data[row+1, col]):\n",
    "                            # Fill with next value\n",
    "                            imputed_data[row, col] = imputed_data[row+1, col]\n",
    "                            filled_count += 1\n",
    "                        else:\n",
    "                            remaining_missing += 1\n",
    "                \n",
    "                if col < 5:  # Print first 5 for brevity\n",
    "                    print(f\"  Asset {col+1:03d}: filled {filled_count} values\")\n",
    "                    if remaining_missing > 0:\n",
    "                        print(f\"    WARNING: {remaining_missing} values still missing\")\n",
    "            \n",
    "            if cols > 5:\n",
    "                print(f\"  ... processed {cols-5} more assets\")\n",
    "        \n",
    "        self.imputation_results['backward_fill'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def combined_fill(self):\n",
    "        \"\"\"\n",
    "        Combined Forward-Backward Fill\n",
    "        1. First apply forward fill\n",
    "        2. Then apply backward fill to remaining gaps\n",
    "        \"\"\"\n",
    "        print(\"\\nCOMBINED FORWARD-BACKWARD FILL\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Start with forward fill\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            \n",
    "            # Step 1: Forward fill\n",
    "            imputed_data = imputed_data.fillna(method='ffill')\n",
    "            # Step 2: Backward fill remaining\n",
    "            imputed_data = imputed_data.fillna(method='bfill')\n",
    "            \n",
    "            # Count what was filled\n",
    "            original_missing = self.data_missing.isnull().sum().sum()\n",
    "            final_missing = imputed_data.isnull().sum().sum()\n",
    "            filled_count = original_missing - final_missing\n",
    "            \n",
    "            print(f\"  Total values filled: {filled_count}\")\n",
    "            print(f\"  Remaining missing: {final_missing}\")\n",
    "        \n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            rows, cols = imputed_data.shape\n",
    "            \n",
    "            # Step 1: Forward fill\n",
    "            for col in range(cols):\n",
    "                for row in range(1, rows):\n",
    "                    if np.isnan(imputed_data[row, col]) and not np.isnan(imputed_data[row-1, col]):\n",
    "                        imputed_data[row, col] = imputed_data[row-1, col]\n",
    "            \n",
    "            # Step 2: Backward fill remaining\n",
    "            for col in range(cols):\n",
    "                for row in range(rows-2, -1, -1):\n",
    "                    if np.isnan(imputed_data[row, col]) and not np.isnan(imputed_data[row+1, col]):\n",
    "                        imputed_data[row, col] = imputed_data[row+1, col]\n",
    "            \n",
    "            original_missing = np.sum(np.isnan(self.data_missing))\n",
    "            final_missing = np.sum(np.isnan(imputed_data))\n",
    "            filled_count = original_missing - final_missing\n",
    "            \n",
    "            print(f\"  Total values filled: {filled_count}\")\n",
    "            print(f\"  Remaining missing: {final_missing}\")\n",
    "        \n",
    "        self.imputation_results['combined_fill'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def evaluate_imputation_quality(self, method_name, imputed_data):\n",
    "        \"\"\"\n",
    "        Evaluate fill quality - same as before but optimized for time series\n",
    "        \"\"\"\n",
    "        if self.original_data is None:\n",
    "            print(f\"\\nNo original data available for {method_name} evaluation\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nIMPUTATION QUALITY EVALUATION: {method_name.upper()}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "                \n",
    "            if hasattr(imputed_data, 'values'):\n",
    "                imputed_np = imputed_data.values.copy()\n",
    "            else:\n",
    "                imputed_np = imputed_data.copy()\n",
    "            \n",
    "            # Find missing positions\n",
    "            mask = np.isnan(missing_np)\n",
    "            total_missing = np.sum(mask)\n",
    "            \n",
    "            if total_missing == 0:\n",
    "                print(\"  No missing values found!\")\n",
    "                return None\n",
    "            \n",
    "            # Extract original and imputed values\n",
    "            orig_vals = []\n",
    "            imp_vals = []\n",
    "            \n",
    "            rows, cols = missing_np.shape\n",
    "            for i in range(rows):\n",
    "                for j in range(cols):\n",
    "                    if mask[i, j]:\n",
    "                        orig_vals.append(original_np[i, j])\n",
    "                        imp_vals.append(imputed_np[i, j])\n",
    "            \n",
    "            orig_vals = np.array(orig_vals)\n",
    "            imp_vals = np.array(imp_vals)\n",
    "            \n",
    "            print(f\"  Evaluated {len(orig_vals)} imputed values\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            differences = orig_vals - imp_vals\n",
    "            abs_differences = np.abs(differences)\n",
    "            \n",
    "            mae = np.mean(abs_differences)\n",
    "            rmse = np.sqrt(np.mean(differences**2))\n",
    "            \n",
    "            # MAPE calculation\n",
    "            nonzero_mask = orig_vals != 0\n",
    "            if np.sum(nonzero_mask) > 0:\n",
    "                mape = np.mean(abs_differences[nonzero_mask] / np.abs(orig_vals[nonzero_mask])) * 100\n",
    "            else:\n",
    "                mape = float('inf')\n",
    "            \n",
    "            # Correlation\n",
    "            if len(orig_vals) > 1:\n",
    "                corr = np.corrcoef(orig_vals, imp_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 1.0\n",
    "            \n",
    "            print(f\"  MAE: ${mae:.2f}\")\n",
    "            print(f\"  RMSE: ${rmse:.2f}\")\n",
    "            print(f\"  MAPE: {mape:.1f}%\" if np.isfinite(mape) else \"  MAPE: ∞\")\n",
    "            print(f\"  Correlation: {corr:.3f}\")\n",
    "            \n",
    "            # Time series specific evaluation\n",
    "            print(f\"\\n  TIME SERIES EVALUATION:\")\n",
    "            \n",
    "            # Check for trend preservation\n",
    "            orig_trend = np.mean(np.diff(orig_vals))\n",
    "            imp_trend = np.mean(np.diff(imp_vals))\n",
    "            trend_error = abs(orig_trend - imp_trend)\n",
    "            \n",
    "            print(f\"  Original trend: {orig_trend:.3f}/period\")\n",
    "            print(f\"  Imputed trend: {imp_trend:.3f}/period\") \n",
    "            print(f\"  Trend preservation error: {trend_error:.3f}\")\n",
    "            \n",
    "            # Financial interpretation\n",
    "            if mape < 2:\n",
    "                print(\"  EXCELLENT: Very accurate for financial data\")\n",
    "            elif mape < 5:\n",
    "                print(\"  GOOD: Acceptable for most financial analysis\")\n",
    "            elif mape < 10:\n",
    "                print(\"  FAIR: Use with caution for risk calculations\")\n",
    "            else:\n",
    "                print(\"  POOR: High errors - consider other methods\")\n",
    "            \n",
    "            metrics = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'Correlation': corr,\n",
    "                'Trend_Error': trend_error,\n",
    "                'N_Values': len(orig_vals)\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics[method_name] = metrics\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compare_all_methods(self):\n",
    "        \"\"\"\n",
    "        Compare Forward Fill, Backward Fill, and Combined approaches\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FORWARD/BACKWARD FILL COMPREHENSIVE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Analyze missing pattern\n",
    "        pattern_info = self.analyze_missing_pattern()\n",
    "        \n",
    "        # Run all methods\n",
    "        print(f\"\\n{'='*25} FORWARD FILL {'='*25}\")\n",
    "        ff_result = self.forward_fill_locf()\n",
    "        self.evaluate_imputation_quality('forward_fill', ff_result)\n",
    "        \n",
    "        print(f\"\\n{'='*25} BACKWARD FILL {'='*24}\")\n",
    "        bf_result = self.backward_fill_nocb()\n",
    "        self.evaluate_imputation_quality('backward_fill', bf_result)\n",
    "        \n",
    "        print(f\"\\n{'='*23} COMBINED FILL {'='*23}\")\n",
    "        combined_result = self.combined_fill()\n",
    "        self.evaluate_imputation_quality('combined_fill', combined_result)\n",
    "        \n",
    "        # Summary comparison\n",
    "        if self.performance_metrics:\n",
    "            self.print_comparison()\n",
    "        \n",
    "        return {\n",
    "            'Forward Fill': ff_result,\n",
    "            'Backward Fill': bf_result,\n",
    "            'Combined Fill': combined_result\n",
    "        }\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print method comparison\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"METHOD COMPARISON SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if self.performance_metrics:\n",
    "            comparison_df = pd.DataFrame(self.performance_metrics).T\n",
    "            print(comparison_df.round(3))\n",
    "            \n",
    "            # Find best method\n",
    "            best_method = comparison_df['RMSE'].idxmin()\n",
    "            print(f\"\\nBEST PERFORMING: {best_method.replace('_', ' ').title()}\")\n",
    "        \n",
    "        print(f\"\\nFINANCIAL TIME SERIES RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 45)\n",
    "        print(\"BEST CHOICE: Combined Fill\")\n",
    "        print(\"  - Fills most gaps possible\")\n",
    "        print(\"  - Uses both past and future information\")\n",
    "        print(\"  - Minimal remaining missing values\")\n",
    "        \n",
    "        print(f\"\\nFORWARD FILL (LOCF):\")\n",
    "        print(\"  - Conservative approach\")\n",
    "        print(\"  - Only uses past information\") \n",
    "        print(\"  - Good for real-time applications\")\n",
    "        \n",
    "        print(f\"\\nBACKWARD FILL (NOCB):\")\n",
    "        print(\"  - Uses future information\")\n",
    "        print(\"  - Good for filling beginning gaps\")\n",
    "        print(\"  - Less realistic for trading strategies\")\n",
    "\n",
    "\n",
    "# READY TO USE\n",
    "print(\"=\"*60)\n",
    "print(\"FORWARD/BACKWARD FILL ANALYZER READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Simple and effective for financial time series gaps\")\n",
    "print(\"\\nUSAGE:\")\n",
    "print(\"analyzer = FinancialForwardBackwardFill(data_with_missing, original_data)\")\n",
    "print(\"results = analyzer.compare_all_methods()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319798d-a79a-49a9-a80f-f2aa12479d16",
   "metadata": {},
   "source": [
    "Prompt Engineer to run the analyzeer and results + show where the imputation occured and also show some specific imputed values\n",
    "USE the two datasets\n",
    "financial_data_missing\n",
    "&\n",
    "df (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc515a-d8a4-4ed8-b9e4-4665a26592f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = FinancialForwardBackwardFill(financial_data_missing, df)\n",
    "\n",
    "# Run comprehensive analysis\n",
    "results = analyzer.compare_all_methods()\n",
    "\n",
    "# Access specific methods\n",
    "forward_filled = results['Forward Fill']\n",
    "combined_filled = results['Combined Fill']  \n",
    "print(pd.DataFrame(results['Combined Fill']).head(10))\n",
    "\n",
    "# Show some specific imputed values\n",
    "backward_filled = results['Backward Fill']\n",
    "print(\"\\nSample of imputed values (original NaN -> new value):\")\n",
    "for i in range(10):  # Show first 10 missing positions\n",
    "    row, col = np.where(missing_mask)\n",
    "    if i < len(row):\n",
    "        print(f\"Position ({row[i]}, {col[i]}): NaN -> {backward_filled.iloc[row[i], col[i]]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37840911-e869-4c38-90cd-754c3e5954b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SimpleSklearnKNNImputation:\n",
    "    \"\"\"\n",
    "    Simple K-NN imputation using sklearn KNNImputer.\n",
    "    Clean, straightforward implementation for financial time series data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_with_missing, original_data=None):\n",
    "        \"\"\"\n",
    "        Initialize sklearn KNN imputation\n",
    "        \n",
    "        Parameters:\n",
    "        - data_with_missing: DataFrame or numpy array with missing values\n",
    "        - original_data: Original complete data for evaluation (optional)\n",
    "        \"\"\"\n",
    "        self.data_missing = data_with_missing.copy() if hasattr(data_with_missing, 'copy') else data_with_missing.copy()\n",
    "        self.original_data = original_data.copy() if original_data is not None else None\n",
    "        self.imputation_results = {}\n",
    "        self.performance_metrics = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def analyze_missing_pattern(self):\n",
    "        \"\"\"Quick missing value analysis\"\"\"\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            missing_info = self.data_missing.isnull()\n",
    "            data_shape = self.data_missing.shape\n",
    "        else:\n",
    "            missing_info = pd.DataFrame(np.isnan(self.data_missing))\n",
    "            data_shape = self.data_missing.shape\n",
    "        \n",
    "        total_missing = missing_info.sum().sum()\n",
    "        missing_pct = (total_missing / (data_shape[0] * data_shape[1])) * 100\n",
    "        \n",
    "        print(\"K-NN IMPUTATION - MISSING DATA ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Dataset shape: {data_shape}\")\n",
    "        print(f\"Total missing values: {total_missing:,}\")\n",
    "        print(f\"Missing percentage: {missing_pct:.2f}%\")\n",
    "        \n",
    "        return {'total_missing': total_missing, 'missing_pct': missing_pct}\n",
    "    \n",
    "    def knn_imputation_basic(self, n_neighbors=5):\n",
    "        \"\"\"\n",
    "        Basic sklearn KNN imputation\n",
    "        \"\"\"\n",
    "        print(f\"\\nBASIC SKLEARN K-NN IMPUTATION (K={n_neighbors})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        \n",
    "        # Apply KNN imputation\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        \n",
    "        self.imputation_results['knn_basic'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def knn_imputation_scaled(self, n_neighbors=5):\n",
    "        \"\"\"\n",
    "        KNN imputation with feature scaling (better for financial data)\n",
    "        \"\"\"\n",
    "        print(f\"\\nSCALED SKLEARN K-NN IMPUTATION (K={n_neighbors})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        print(\"Applying StandardScaler for better distance calculations...\")\n",
    "        \n",
    "        # Scale the data first (handles NaN automatically)\n",
    "        scaler = StandardScaler()  # Z-score normalization : (~N(0,1))\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "        \n",
    "        # Apply KNN imputation on scaled data\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        imputed_scaled = imputer.fit_transform(data_scaled)\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        imputed_data = scaler.inverse_transform(imputed_scaled)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        \n",
    "        self.imputation_results['knn_scaled'] = imputed_data\n",
    "        self.scalers['knn_scaled'] = scaler\n",
    "        return imputed_data\n",
    "    \n",
    "    def knn_imputation_different_k(self, k_values=[3, 5, 7, 10]):\n",
    "        \"\"\"\n",
    "        Test different K values to find optimal\n",
    "        \"\"\"\n",
    "        print(f\"\\nTESTING DIFFERENT K VALUES: {k_values}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        for k in k_values:\n",
    "            print(f\"\\nTesting K={k}...\")\n",
    "            \n",
    "            # Apply KNN imputation\n",
    "            imputer = KNNImputer(n_neighbors=k)\n",
    "            imputed_data = imputer.fit_transform(data)\n",
    "            \n",
    "            # Quick evaluation if original data available\n",
    "            if self.original_data is not None:\n",
    "                metrics = self.quick_evaluate(imputed_data, f'k_{k}')\n",
    "                results[f'K={k}'] = {\n",
    "                    'data': imputed_data,\n",
    "                    'rmse': metrics['RMSE'] if metrics else None\n",
    "                }\n",
    "                print(f\"  RMSE: {metrics['RMSE']:.3f}\" if metrics else \"  No evaluation data\")\n",
    "            else:\n",
    "                results[f'K={k}'] = {'data': imputed_data, 'rmse': None}\n",
    "                print(f\"  Imputation completed\")\n",
    "        \n",
    "        # Find best K if evaluation possible\n",
    "        if self.original_data is not None:\n",
    "            best_k = min(k_values, key=lambda k: results[f'K={k}']['rmse'] if results[f'K={k}']['rmse'] else float('inf'))\n",
    "            print(f\"\\nBest K value: {best_k}\")\n",
    "            self.imputation_results['knn_best_k'] = results[f'K={best_k}']['data']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def quick_evaluate(self, imputed_data, method_name):\n",
    "        \"\"\"Quick evaluation for K comparison\"\"\"\n",
    "        if self.original_data is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "            \n",
    "            # Find missing positions and extract values\n",
    "            mask = np.isnan(missing_np)\n",
    "            orig_vals = original_np[mask]\n",
    "            imp_vals = imputed_data[mask]\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(np.mean((orig_vals - imp_vals) ** 2))\n",
    "            \n",
    "            return {'RMSE': rmse}\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def evaluate_imputation_quality(self, method_name, imputed_data):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of KNN imputation quality\n",
    "        \"\"\"\n",
    "        if self.original_data is None:\n",
    "            print(f\"\\nNo original data available for {method_name} evaluation\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nSKLEARN K-NN EVALUATION: {method_name.upper()}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "            \n",
    "            # Find missing positions\n",
    "            mask = np.isnan(missing_np)\n",
    "            total_missing = np.sum(mask)\n",
    "            \n",
    "            # Extract values\n",
    "            orig_vals = original_np[mask]\n",
    "            imp_vals = imputed_data[mask]\n",
    "            \n",
    "            print(f\"Evaluated {len(orig_vals)} imputed values\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = np.mean(np.abs(orig_vals - imp_vals))\n",
    "            rmse = np.sqrt(np.mean((orig_vals - imp_vals) ** 2))\n",
    "            \n",
    "            # MAPE calculation\n",
    "            nonzero_mask = orig_vals != 0\n",
    "            if np.sum(nonzero_mask) > 0:\n",
    "                mape = np.mean(np.abs(orig_vals - imp_vals)[nonzero_mask] / np.abs(orig_vals[nonzero_mask])) * 100\n",
    "            else:\n",
    "                mape = float('inf')\n",
    "            \n",
    "            # Correlation\n",
    "            if len(orig_vals) > 1:\n",
    "                corr = np.corrcoef(orig_vals, imp_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 1.0\n",
    "            \n",
    "            print(f\"MAE: ${mae:.2f}\")\n",
    "            print(f\"RMSE: ${rmse:.2f}\")\n",
    "            print(f\"MAPE: {mape:.1f}%\" if np.isfinite(mape) else \"MAPE: ∞\")\n",
    "            print(f\"Correlation: {corr:.3f}\")\n",
    "            \n",
    "            # Financial interpretation\n",
    "            if mape < 3:\n",
    "                print(\"EXCELLENT: Very accurate K-NN imputation\")\n",
    "            elif mape < 7:\n",
    "                print(\"GOOD: Acceptable K-NN performance\")\n",
    "            elif mape < 15:\n",
    "                print(\"FAIR: Consider different K or scaling\")\n",
    "            else:\n",
    "                print(\"POOR: K-NN may not suit this data\")\n",
    "            \n",
    "            metrics = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'Correlation': corr,\n",
    "                'N_Values': len(orig_vals)\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics[method_name] = metrics\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compare_all_methods(self, n_neighbors=5):\n",
    "        \"\"\"\n",
    "        Compare different sklearn KNN approaches\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SKLEARN K-NN IMPUTATION ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Analyze missing pattern\n",
    "        self.analyze_missing_pattern()\n",
    "        \n",
    "        # Test basic KNN\n",
    "        print(f\"\\n{'='*15} BASIC K-NN {'='*15}\")\n",
    "        basic_result = self.knn_imputation_basic(n_neighbors)\n",
    "        self.evaluate_imputation_quality('knn_basic', basic_result)\n",
    "        \n",
    "        # Test scaled KNN\n",
    "        print(f\"\\n{'='*15} SCALED K-NN {'='*14}\")\n",
    "        scaled_result = self.knn_imputation_scaled(n_neighbors)\n",
    "        self.evaluate_imputation_quality('knn_scaled', scaled_result)\n",
    "        \n",
    "        # Test different K values\n",
    "        print(f\"\\n{'='*12} K VALUE TESTING {'='*12}\")\n",
    "        k_results = self.knn_imputation_different_k([3, 5, 7, 10])\n",
    "        \n",
    "        # Summary comparison\n",
    "        if self.performance_metrics:\n",
    "            self.print_comparison()\n",
    "        \n",
    "        return {\n",
    "            'Basic KNN': basic_result,\n",
    "            'Scaled KNN': scaled_result,\n",
    "            'K Results': k_results\n",
    "        }\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print method comparison summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SKLEARN K-NN COMPARISON SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if self.performance_metrics:\n",
    "            comparison_df = pd.DataFrame(self.performance_metrics).T\n",
    "            print(comparison_df.round(3))\n",
    "            \n",
    "            # Find best method\n",
    "            best_method = comparison_df['RMSE'].idxmin()\n",
    "            print(f\"\\nBest performing: {best_method.replace('_', ' ').title()}\")\n",
    "        \n",
    "        print(f\"\\nRECOMMENDATIONS:\")\n",
    "        print(\"- Scaled K-NN usually performs best for financial data\")\n",
    "        print(\"- K=5 is good default, but aslo test  other ranges\")\n",
    "        print(\"- Feature scaling important when assets have different price ranges\")\n",
    "        print(\"- sklearn KNNImputer is robust and well-tested\")\n",
    "\n",
    "\n",
    "# READY TO USE\n",
    "print(\"=\"*50)\n",
    "print(\"SKLEARN K-NN IMPUTATION READY!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Simple sklearn-based KNN imputation\")\n",
    "print(\"\\nUSAGE:\")\n",
    "print(\"analyzer = SimpleSklearnKNNImputation(data_with_missing, original_data)\")\n",
    "print(\"results = analyzer.compare_all_methods(n_neighbors=5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2ad2c-e94a-4f87-b529-152b4e3995fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SimpleSklearnKNNImputation(financial_data_missing, df)\n",
    "\n",
    "# Run comprehensive analysis\n",
    "results = analyzer.compare_all_methods(n_neighbors = 5)\n",
    "\n",
    "# Access specific results\n",
    "best_imputed = results['Scaled KNN']  # Usually performs best\n",
    "print(pd.DataFrame(best_imputed).head(10))\n",
    "\n",
    "print(\"\\nSample of imputed values (original NaN -> new value):\")\n",
    "for i in range(10):  # Show first 10 missing positions\n",
    "    row, col = np.where(missing_mask)\n",
    "    if i < len(row):\n",
    "        print(f\"Position ({row[i]}, {col[i]}): NaN -> {best_imputed[row[i], col[i]]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e309d13-24c9-4f31-be0b-d26129ee0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # Required for IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SimpleSklearnMICEImputation:\n",
    "    \"\"\"\n",
    "    Simple MICE (Multiple Imputation by Chained Equations) using sklearn IterativeImputer.\n",
    "    Clean, straightforward implementation for financial time series data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_with_missing, original_data=None):\n",
    "        \"\"\"\n",
    "        Initialize MICE imputation\n",
    "        \n",
    "        Parameters:\n",
    "        - data_with_missing: DataFrame or numpy array with missing values\n",
    "        - original_data: Original complete data for evaluation (optional)\n",
    "        \"\"\"\n",
    "        self.data_missing = data_with_missing.copy() if hasattr(data_with_missing, 'copy') else data_with_missing.copy()\n",
    "        self.original_data = original_data.copy() if original_data is not None else None\n",
    "        self.imputation_results = {}\n",
    "        self.performance_metrics = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def analyze_missing_pattern(self):\n",
    "        \"\"\"Quick missing value analysis for MICE\"\"\"\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            missing_info = self.data_missing.isnull()\n",
    "            data_shape = self.data_missing.shape\n",
    "        else:\n",
    "            missing_info = pd.DataFrame(np.isnan(self.data_missing))\n",
    "            data_shape = self.data_missing.shape\n",
    "        \n",
    "        total_missing = missing_info.sum().sum()\n",
    "        missing_pct = (total_missing / (data_shape[0] * data_shape[1])) * 100\n",
    "        \n",
    "        print(\"MICE IMPUTATION - MISSING DATA ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Dataset shape: {data_shape}\")\n",
    "        print(f\"Total missing values: {total_missing:,}\")\n",
    "        print(f\"Missing percentage: {missing_pct:.2f}%\")\n",
    "        \n",
    "        if missing_pct > 50:\n",
    "            print(\"WARNING: High missing percentage may affect MICE performance\")\n",
    "        elif missing_pct > 20:\n",
    "            print(\"CAUTION: Moderate missing percentage - MICE will work but may be slow\")\n",
    "        else:\n",
    "            print(\"GOOD: Missing percentage suitable for MICE imputation\")\n",
    "        \n",
    "        return {'total_missing': total_missing, 'missing_pct': missing_pct}\n",
    "    \n",
    "    def mice_imputation_basic(self, max_iter=10, random_state=42):\n",
    "        \"\"\"\n",
    "        Basic MICE imputation with BayesianRidge estimator\n",
    "        \"\"\"\n",
    "        print(f\"\\nBASIC MICE IMPUTATION (max_iter={max_iter})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        print(\"Using BayesianRidge estimator (default)\")\n",
    "        \n",
    "        # Apply MICE imputation\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=BayesianRidge(),\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        print(f\"Convergence after {max_iter} iterations\")\n",
    "        \n",
    "        self.imputation_results['mice_basic'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def mice_imputation_linear(self, max_iter=10, random_state=42):\n",
    "        \"\"\"\n",
    "        MICE imputation with LinearRegression estimator\n",
    "        \"\"\"\n",
    "        print(f\"\\nMICE WITH LINEAR REGRESSION (max_iter={max_iter})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        print(\"Using LinearRegression estimator (faster)\")\n",
    "        \n",
    "        # Apply MICE imputation\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=LinearRegression(),\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        print(f\"Linear regression converged after {max_iter} iterations\")\n",
    "        \n",
    "        self.imputation_results['mice_linear'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def mice_imputation_rf(self, max_iter=5, random_state=42):\n",
    "        \"\"\"\n",
    "        MICE imputation with RandomForest estimator (more robust but slower)\n",
    "        \"\"\"\n",
    "        print(f\"\\nMICE WITH RANDOM FOREST (max_iter={max_iter})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        print(\"Using RandomForest estimator (robust, handles non-linearity)\")\n",
    "        \n",
    "        # Apply MICE imputation with RandomForest\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=RandomForestRegressor(n_estimators=10, random_state=random_state),\n",
    "            max_iter=max_iter,  # Fewer iterations for RF as it's slower\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        imputed_data = imputer.fit_transform(data)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        print(f\"RandomForest MICE completed\")\n",
    "        \n",
    "        self.imputation_results['mice_rf'] = imputed_data\n",
    "        return imputed_data\n",
    "    \n",
    "    def mice_imputation_scaled(self, max_iter=10, random_state=42):\n",
    "        \"\"\"\n",
    "        MICE imputation with feature scaling (better for financial data)\n",
    "        \"\"\"\n",
    "        print(f\"\\nSCALED MICE IMPUTATION (max_iter={max_iter})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        print(f\"Processing {data.shape[0]}x{data.shape[1]} dataset...\")\n",
    "        print(\"Applying StandardScaler before MICE...\")\n",
    "        \n",
    "        # Scale the data first (handles NaN automatically)\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "        \n",
    "        # Apply MICE imputation on scaled data\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=BayesianRidge(),\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        imputed_scaled = imputer.fit_transform(data_scaled)\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        imputed_data = scaler.inverse_transform(imputed_scaled)\n",
    "        \n",
    "        # Count imputed values\n",
    "        original_missing = np.sum(np.isnan(data))\n",
    "        final_missing = np.sum(np.isnan(imputed_data))\n",
    "        imputed_count = original_missing - final_missing\n",
    "        \n",
    "        print(f\"Successfully imputed: {imputed_count} values\")\n",
    "        print(f\"Remaining missing: {final_missing}\")\n",
    "        print(f\"Scaled MICE completed and inverse transformed\")\n",
    "        \n",
    "        self.imputation_results['mice_scaled'] = imputed_data\n",
    "        self.scalers['mice_scaled'] = scaler\n",
    "        return imputed_data\n",
    "    \n",
    "    def mice_test_iterations(self, iter_values=[5, 10, 15, 20]):\n",
    "        \"\"\"\n",
    "        Test different iteration counts to find optimal convergence\n",
    "        \"\"\"\n",
    "        print(f\"\\nTESTING DIFFERENT ITERATION COUNTS: {iter_values}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if hasattr(self.data_missing, 'values'):\n",
    "            data = self.data_missing.values.copy()\n",
    "        else:\n",
    "            data = self.data_missing.copy()\n",
    "        \n",
    "        for max_iter in iter_values:\n",
    "            print(f\"\\nTesting max_iter={max_iter}...\")\n",
    "            \n",
    "            # Apply MICE imputation\n",
    "            imputer = IterativeImputer(\n",
    "                estimator=BayesianRidge(),\n",
    "                max_iter=max_iter,\n",
    "                random_state=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            imputed_data = imputer.fit_transform(data)\n",
    "            \n",
    "            # Quick evaluation if original data available\n",
    "            if self.original_data is not None:\n",
    "                metrics = self.quick_evaluate(imputed_data, f'iter_{max_iter}')\n",
    "                results[f'iter_{max_iter}'] = {\n",
    "                    'data': imputed_data,\n",
    "                    'rmse': metrics['RMSE'] if metrics else None,\n",
    "                    'iterations': max_iter\n",
    "                }\n",
    "                print(f\"  RMSE: {metrics['RMSE']:.3f}\" if metrics else \"  No evaluation data\")\n",
    "            else:\n",
    "                results[f'iter_{max_iter}'] = {'data': imputed_data, 'rmse': None, 'iterations': max_iter}\n",
    "                print(f\"  MICE completed in {max_iter} iterations\")\n",
    "        \n",
    "        # Find best iteration count if evaluation possible\n",
    "        if self.original_data is not None:\n",
    "            best_iter = min(iter_values, key=lambda i: results[f'iter_{i}']['rmse'] if results[f'iter_{i}']['rmse'] else float('inf'))\n",
    "            print(f\"\\nBest iteration count: {best_iter}\")\n",
    "            self.imputation_results['mice_best_iter'] = results[f'iter_{best_iter}']['data']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def quick_evaluate(self, imputed_data, method_name):\n",
    "        \"\"\"Quick evaluation for iteration/method comparison\"\"\"\n",
    "        if self.original_data is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "            \n",
    "            # Find missing positions and extract values\n",
    "            mask = np.isnan(missing_np)\n",
    "            orig_vals = original_np[mask]\n",
    "            imp_vals = imputed_data[mask]\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(np.mean((orig_vals - imp_vals) ** 2))\n",
    "            \n",
    "            return {'RMSE': rmse}\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def evaluate_imputation_quality(self, method_name, imputed_data):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of MICE imputation quality\n",
    "        \"\"\"\n",
    "        if self.original_data is None:\n",
    "            print(f\"\\nNo original data available for {method_name} evaluation\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nMICE EVALUATION: {method_name.upper()}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "                \n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "            \n",
    "            # Find missing positions\n",
    "            mask = np.isnan(missing_np)\n",
    "            total_missing = np.sum(mask)\n",
    "            \n",
    "            # Extract values\n",
    "            orig_vals = original_np[mask]\n",
    "            imp_vals = imputed_data[mask]\n",
    "            \n",
    "            print(f\"Evaluated {len(orig_vals)} imputed values\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = np.mean(np.abs(orig_vals - imp_vals))\n",
    "            rmse = np.sqrt(np.mean((orig_vals - imp_vals) ** 2))\n",
    "            \n",
    "            # MAPE calculation\n",
    "            nonzero_mask = orig_vals != 0\n",
    "            if np.sum(nonzero_mask) > 0:\n",
    "                mape = np.mean(np.abs(orig_vals - imp_vals)[nonzero_mask] / np.abs(orig_vals[nonzero_mask])) * 100\n",
    "            else:\n",
    "                mape = float('inf')\n",
    "            \n",
    "            # Correlation\n",
    "            if len(orig_vals) > 1:\n",
    "                corr = np.corrcoef(orig_vals, imp_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 1.0\n",
    "            \n",
    "            # MICE-specific: Variance preservation\n",
    "            orig_var = np.var(orig_vals)\n",
    "            imp_var = np.var(imp_vals)\n",
    "            var_ratio = imp_var / orig_var if orig_var > 0 else 1.0\n",
    "            \n",
    "            print(f\"MAE: ${mae:.2f}\")\n",
    "            print(f\"RMSE: ${rmse:.2f}\")\n",
    "            print(f\"MAPE: {mape:.1f}%\" if np.isfinite(mape) else \"MAPE: ∞\")\n",
    "            print(f\"Correlation: {corr:.3f}\")\n",
    "            print(f\"Variance ratio: {var_ratio:.3f} (1.0 = perfect)\")\n",
    "            \n",
    "            # Financial interpretation\n",
    "            if mape < 2:\n",
    "                print(\"EXCELLENT: MICE captured multivariate relationships very well\")\n",
    "            elif mape < 5:\n",
    "                print(\"GOOD: MICE found meaningful patterns between variables\")\n",
    "            elif mape < 10:\n",
    "                print(\"FAIR: Some multivariate structure captured\")\n",
    "            else:\n",
    "                print(\"POOR: Consider different estimator or more iterations\")\n",
    "            \n",
    "            metrics = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'Correlation': corr,\n",
    "                'Variance_Ratio': var_ratio,\n",
    "                'N_Values': len(orig_vals)\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics[method_name] = metrics\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compare_all_mice_methods(self, max_iter=10):\n",
    "        \"\"\"\n",
    "        Compare different MICE approaches\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MICE IMPUTATION COMPREHENSIVE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Analyze missing pattern\n",
    "        self.analyze_missing_pattern()\n",
    "        \n",
    "        # Test basic MICE\n",
    "        print(f\"\\n{'='*15} BASIC MICE {'='*15}\")\n",
    "        basic_result = self.mice_imputation_basic(max_iter)\n",
    "        self.evaluate_imputation_quality('mice_basic', basic_result)\n",
    "        \n",
    "        # Test linear MICE (faster)\n",
    "        print(f\"\\n{'='*14} LINEAR MICE {'='*14}\")\n",
    "        linear_result = self.mice_imputation_linear(max_iter)\n",
    "        self.evaluate_imputation_quality('mice_linear', linear_result)\n",
    "        \n",
    "        # Test scaled MICE\n",
    "        print(f\"\\n{'='*14} SCALED MICE {'='*14}\")\n",
    "        scaled_result = self.mice_imputation_scaled(max_iter)\n",
    "        self.evaluate_imputation_quality('mice_scaled', scaled_result)\n",
    "        \n",
    "        # Test RandomForest MICE (fewer iterations as it's slower)\n",
    "        print(f\"\\n{'='*12} RANDOM FOREST MICE {'='*12}\")\n",
    "        rf_result = self.mice_imputation_rf(max_iter=5)  # Fewer iterations for RF\n",
    "        self.evaluate_imputation_quality('mice_rf', rf_result)\n",
    "        \n",
    "        # Test different iteration counts\n",
    "        print(f\"\\n{'='*10} ITERATION TESTING {'='*10}\")\n",
    "        iter_results = self.mice_test_iterations([5, 10, 15])\n",
    "        \n",
    "        # Summary comparison\n",
    "        if self.performance_metrics:\n",
    "            self.print_comparison()\n",
    "        \n",
    "        return {\n",
    "            'Basic MICE': basic_result,\n",
    "            'Linear MICE': linear_result,\n",
    "            'Scaled MICE': scaled_result,\n",
    "            'RandomForest MICE': rf_result,\n",
    "            'Iteration Results': iter_results\n",
    "        }\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print MICE method comparison summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MICE COMPARISON SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if self.performance_metrics:\n",
    "            comparison_df = pd.DataFrame(self.performance_metrics).T\n",
    "            print(comparison_df.round(3))\n",
    "            \n",
    "            # Find best method\n",
    "            best_method = comparison_df['RMSE'].idxmin()\n",
    "            print(f\"\\nBest performing: {best_method.replace('_', ' ').title()}\")\n",
    "        \n",
    "        print(f\"\\nMICE RECOMMENDATIONS:\")\n",
    "        print(\"- Basic MICE (BayesianRidge): Good balance of speed and accuracy\")\n",
    "        print(\"- Linear MICE: Fastest for large datasets\")\n",
    "        print(\"- Scaled MICE: Best for financial data with different scales\")\n",
    "        print(\"- RandomForest MICE: Best for non-linear relationships\")\n",
    "        print(\"- 10-15 iterations usually sufficient for convergence\")\n",
    "        \n",
    "        print(f\"\\nADVANTAGES OF MICE:\")\n",
    "        print(\"✅ Captures relationships between variables\")\n",
    "        print(\"✅ Preserves variance and distributions\")\n",
    "        print(\"✅ Handles different variable types\")\n",
    "        print(\"✅ Provides uncertainty estimates\")\n",
    "\n",
    "\n",
    "# READY TO USE\n",
    "print(\"=\"*50)\n",
    "print(\"SKLEARN MICE IMPUTATION READY!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Multiple Imputation by Chained Equations with sklearn\")\n",
    "print(\"\\nUSAGE:\")\n",
    "print(\"analyzer = SimpleSklearnMICEImputation(data_with_missing, original_data)\")\n",
    "print(\"results = analyzer.compare_all_mice_methods(max_iter=10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb7e0a-6484-4abb-a94f-6246c8cd7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SimpleSklearnMICEImputation(financial_data_missing, df)\n",
    "\n",
    "# Run comprehensive analysis\n",
    "results = analyzer.compare_all_mice_methods(max_iter=10)\n",
    "\n",
    "# Access specific methods\n",
    "scaled_mice = results['Scaled MICE']  # Usually best for financial data\n",
    "print(pd.DataFrame(scaled_mice).head(10))\n",
    "\n",
    "print(\"\\nSample of imputed values from Scaled MICE (original NaN -> new value):\")\n",
    "for i in range(10):  # Show first 10 missing positions\n",
    "    row, col = np.where(missing_mask)\n",
    "    if i < len(row):\n",
    "        print(f\"Position ({row[i]}, {col[i]}): NaN -> {scaled_mice[row[i], col[i]]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
