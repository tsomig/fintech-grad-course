{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ab378-4469-4c09-9c25-ddc90324ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WEEK 5: MACHINE LEARNING & REGULARIZATION\n",
    "Building on Week 4's ARIMA/GARCH forecasts\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy import stats  # ‚Üê ADD THIS LINE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32ba44-85e9-468f-9984-9b63f6661f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: LOAD WEEK 4 RESULTS (FORECASTS)\n",
    "# ============================================================================\n",
    "\n",
    "def load_week4_forecasts():\n",
    "    \"\"\"\n",
    "    Simulate loading Week 4 ARIMA/GARCH forecast results.\n",
    "    NOW WITH REALISTIC CORRELATIONS for better ML demonstration.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìÇ LOADING WEEK 4 FORECASTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    dates = pd.date_range(end=pd.Timestamp.now(), periods=100, freq='D')\n",
    "    \n",
    "    # Create BASE return with some autocorrelation (realistic market behavior)\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    base_returns = np.random.normal(0.001, 0.015, 100)\n",
    "    \n",
    "    # Add momentum/trend component\n",
    "    trend = np.linspace(0, 0.002, 100)\n",
    "    \n",
    "    # Create correlated returns (crypto assets tend to move together)\n",
    "    btc_returns = base_returns + trend + np.random.normal(0, 0.005, 100)\n",
    "    eth_returns = 0.7 * btc_returns + 0.3 * np.random.normal(0.0015, 0.015, 100)\n",
    "    doge_returns = 0.5 * btc_returns + 0.5 * np.random.normal(0.0005, 0.02, 100)\n",
    "    \n",
    "    # Volatility clustering (GARCH-like behavior)\n",
    "    btc_vol = np.abs(np.random.normal(0.04, 0.01, 100))\n",
    "    eth_vol = np.abs(np.random.normal(0.05, 0.015, 100))\n",
    "    doge_vol = np.abs(np.random.normal(0.06, 0.02, 100))\n",
    "    \n",
    "    # Add volatility spillover effects\n",
    "    btc_vol = btc_vol * (1 + 0.3 * np.abs(btc_returns))\n",
    "    eth_vol = eth_vol * (1 + 0.3 * np.abs(eth_returns))\n",
    "    doge_vol = doge_vol * (1 + 0.3 * np.abs(doge_returns))\n",
    "    \n",
    "    forecast_data = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'BTC_return_forecast': btc_returns,\n",
    "        'ETH_return_forecast': eth_returns,\n",
    "        'DOGE_return_forecast': doge_returns,\n",
    "        'BTC_volatility_forecast': btc_vol,\n",
    "        'ETH_volatility_forecast': eth_vol,\n",
    "        'DOGE_volatility_forecast': doge_vol,\n",
    "    })\n",
    "    \n",
    "    print(\"‚úÖ Loaded 100 days of forecasts\")\n",
    "    print(f\"   Assets: BTC, ETH, DOGE\")\n",
    "    print(f\"   Data: Return forecasts + Volatility forecasts (from GARCH)\")\n",
    "    print(f\"   üìä Added realistic correlations and volatility clustering\")\n",
    "    return forecast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27814b1f-bc2a-4c20-8932-3ce5d4a9909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: FEATURE ENGINEERING (Prompt-Assisted)\n",
    "# ============================================================================\n",
    "def engineer_portfolio_features(forecast_df):\n",
    "    \"\"\"\n",
    "    Create features for ML model using Week 4 forecasts.\n",
    "    This demonstrates PROMPT-ASSISTED feature engineering.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üîß FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    features = forecast_df.copy()\n",
    "    \n",
    "    # Feature 1: Risk-Adjusted Returns (Sharpe-like)\n",
    "    print(\"\\n1. Creating Risk-Adjusted Return features...\")\n",
    "    for asset in ['BTC', 'ETH', 'DOGE']:\n",
    "        features[f'{asset}_risk_adj_return'] = (\n",
    "            features[f'{asset}_return_forecast'] / features[f'{asset}_volatility_forecast']\n",
    "        )\n",
    "    \n",
    "    # Feature 2: Momentum indicators (rolling averages)\n",
    "    print(\"2. Adding momentum features (7-day rolling avg)...\")\n",
    "    for asset in ['BTC', 'ETH', 'DOGE']:\n",
    "        features[f'{asset}_momentum'] = (\n",
    "            features[f'{asset}_return_forecast'].rolling(window=7, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Feature 3: Volatility regime (high/low vol indicator)\n",
    "    print(\"3. Creating volatility regime features...\")\n",
    "    for asset in ['BTC', 'ETH', 'DOGE']:\n",
    "        vol_median = features[f'{asset}_volatility_forecast'].median()\n",
    "        features[f'{asset}_high_vol'] = (\n",
    "            features[f'{asset}_volatility_forecast'] > vol_median\n",
    "        ).astype(int)\n",
    "    \n",
    "    # Feature 4: Cross-asset features\n",
    "    print(\"4. Engineering cross-asset features...\")\n",
    "    features['BTC_ETH_return_spread'] = (\n",
    "        features['BTC_return_forecast'] - features['ETH_return_forecast']\n",
    "    )\n",
    "    features['portfolio_avg_vol'] = features[\n",
    "        ['BTC_volatility_forecast', 'ETH_volatility_forecast', 'DOGE_volatility_forecast']\n",
    "    ].mean(axis=1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created {len(features.columns) - len(forecast_df.columns)} new features\")\n",
    "    print(f\"   Total features: {len(features.columns)}\")\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b3d33-ccbe-45e6-b680-7411c2ce2848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: PREPARE DATA FOR ML (Target = Portfolio Return)\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_ml_data(features_df):\n",
    "    \"\"\"\n",
    "    Prepare X (features) and y (target) for ML models.\n",
    "    Target: Predict optimal portfolio return.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä PREPARING ML DATASET\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create target variable (next period's portfolio return)\n",
    "    # Simple equal-weighted portfolio for target\n",
    "    features_df['target_portfolio_return'] = (\n",
    "        features_df['BTC_return_forecast'].shift(-1) * 0.33 +\n",
    "        features_df['ETH_return_forecast'].shift(-1) * 0.33 +\n",
    "        features_df['DOGE_return_forecast'].shift(-1) * 0.34\n",
    "    )\n",
    "    \n",
    "    # Remove rows with NaN\n",
    "    features_df = features_df.dropna()\n",
    "    \n",
    "    # Select feature columns (exclude date and target)\n",
    "    feature_cols = [col for col in features_df.columns \n",
    "                   if col not in ['date', 'target_portfolio_return']]\n",
    "    \n",
    "    X = features_df[feature_cols].values\n",
    "    y = features_df['target_portfolio_return'].values\n",
    "    \n",
    "    print(f\"‚úÖ Dataset prepared:\")\n",
    "    print(f\"   Samples: {len(X)}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Target: Portfolio return (next period)\")\n",
    "    \n",
    "    return X, y, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107f309-4729-4eb3-9c50-9ce221ffeee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: RIDGE REGRESSION (L2 Regularization)\n",
    "# ============================================================================\n",
    "\n",
    "def fit_ridge_regression(X, y, alpha_range=np.logspace(-4, 1, 30)):  \n",
    "    \"\"\"\n",
    "    Ridge regression with cross-validation for alpha tuning.\n",
    "    Ridge penalizes large coefficients: min ||y - Xw||¬≤ + Œ±||w||¬≤\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üîµ RIDGE REGRESSION (L2 Regularization)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Cross-validation to find best alpha\n",
    "    cv_scores = []\n",
    "    \n",
    "    print(\"\\nTuning regularization parameter (alpha)...\")\n",
    "    for alpha in alpha_range:\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        # 5-fold cross-validation\n",
    "        scores = cross_val_score(ridge, X_scaled, y, cv=5, \n",
    "                                scoring='neg_mean_squared_error')\n",
    "        cv_scores.append(-scores.mean())  # Convert to positive MSE\n",
    "    \n",
    "    # Find best alpha\n",
    "    best_idx = np.argmin(cv_scores)\n",
    "    best_alpha = alpha_range[best_idx]\n",
    "    \n",
    "    print(f\"‚úÖ Best alpha: {best_alpha:.4f}\")\n",
    "    print(f\"   CV MSE: {cv_scores[best_idx]:.6f}\")\n",
    "    \n",
    "    # Fit final model with best alpha\n",
    "    best_ridge = Ridge(alpha=best_alpha)\n",
    "    best_ridge.fit(X_scaled, y)\n",
    "    \n",
    "    return best_ridge, scaler, best_alpha, cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faee494-bb18-4c58-980e-e4281bd7c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: LASSO REGRESSION (L1 Regularization - Feature Selection)\n",
    "# ============================================================================\n",
    "\n",
    "def fit_lasso_regression(X, y, alpha_range=np.logspace(-6, -1, 40)): \n",
    "    \"\"\"\n",
    "    Lasso regression for automatic feature selection.\n",
    "    Lasso creates sparse solutions: min ||y - Xw||¬≤ + Œ±||w||‚ÇÅ\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üü¢ LASSO REGRESSION (L1 Regularization)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Cross-validation to find best alpha\n",
    "    cv_scores = []\n",
    "    n_features_used = []\n",
    "    \n",
    "    print(\"\\nTuning alpha and performing feature selection...\")\n",
    "    for alpha in alpha_range:\n",
    "        lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "        scores = cross_val_score(lasso, X_scaled, y, cv=5, \n",
    "                                scoring='neg_mean_squared_error')\n",
    "        cv_scores.append(-scores.mean())\n",
    "        \n",
    "        # Fit to count non-zero features\n",
    "        lasso.fit(X_scaled, y)\n",
    "        n_features_used.append(np.sum(lasso.coef_ != 0))\n",
    "    \n",
    "    # Find best alpha\n",
    "    best_idx = np.argmin(cv_scores)\n",
    "    best_alpha = alpha_range[best_idx]\n",
    "    \n",
    "    print(f\"‚úÖ Best alpha: {best_alpha:.6f}\")\n",
    "    print(f\"   CV MSE: {cv_scores[best_idx]:.6f}\")\n",
    "    print(f\"   Features selected: {n_features_used[best_idx]}/{X.shape[1]}\")\n",
    "    \n",
    "    # Fit final model\n",
    "    best_lasso = Lasso(alpha=best_alpha, max_iter=10000)\n",
    "    best_lasso.fit(X_scaled, y)\n",
    "    \n",
    "    return best_lasso, scaler, best_alpha, n_features_used[best_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee19359-542b-4add-8287-24f45c405dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5A: K-FOLD CROSS-VALIDATION ON OPTIMIZED MODELS\n",
    "# ============================================================================\n",
    "\n",
    "def validate_optimized_models_kfold(X, y, best_ridge_alpha, best_lasso_alpha, n_splits=5):\n",
    "    \"\"\"\n",
    "    Apply K-Fold Cross-Validation to the OPTIMIZED Ridge and Lasso models.\n",
    "    This validates that the hyperparameter-tuned models generalize well.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Feature matrix\n",
    "    y : array-like\n",
    "        Target variable\n",
    "    best_ridge_alpha : float\n",
    "        Optimal alpha from Ridge hyperparameter tuning (Cell 4)\n",
    "    best_lasso_alpha : float\n",
    "        Optimal alpha from Lasso hyperparameter tuning (Cell 5)\n",
    "    n_splits : int\n",
    "        Number of K-Fold splits (default=5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Validation results for both models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üîÄ K-FOLD VALIDATION OF OPTIMIZED MODELS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nüìö Validating Hyperparameter-Tuned Models:\")\n",
    "    print(f\"   ‚Ä¢ Ridge alpha: {best_ridge_alpha:.4f} (from Cell 4)\")\n",
    "    print(f\"   ‚Ä¢ Lasso alpha: {best_lasso_alpha:.6f} (from Cell 5)\")\n",
    "    print(f\"   ‚Ä¢ K-Fold splits: {n_splits}\")\n",
    "    print(f\"   ‚Ä¢ Purpose: Verify models generalize to unseen data\\n\")\n",
    "    \n",
    "    # Initialize K-Fold\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Store results for each fold\n",
    "    ridge_results = []\n",
    "    lasso_results = []\n",
    "    \n",
    "    print(f\"{'Fold':<6} {'Size':<12} {'Ridge Train':<15} {'Ridge Test':<15} {'Lasso Train':<15} {'Lasso Test':<15} {'Lasso Features':<15}\")\n",
    "    print(\"-\" * 105)\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(X_scaled), 1):\n",
    "        # Split data\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Train Ridge with OPTIMIZED alpha\n",
    "        ridge_model = Ridge(alpha=best_ridge_alpha)\n",
    "        ridge_model.fit(X_train, y_train)\n",
    "        ridge_train_pred = ridge_model.predict(X_train)\n",
    "        ridge_test_pred = ridge_model.predict(X_test)\n",
    "        ridge_train_mse = mean_squared_error(y_train, ridge_train_pred)\n",
    "        ridge_test_mse = mean_squared_error(y_test, ridge_test_pred)\n",
    "        ridge_train_r2 = r2_score(y_train, ridge_train_pred)\n",
    "        ridge_test_r2 = r2_score(y_test, ridge_test_pred)\n",
    "        \n",
    "        # Train Lasso with OPTIMIZED alpha\n",
    "        lasso_model = Lasso(alpha=best_lasso_alpha, max_iter=10000)\n",
    "        lasso_model.fit(X_train, y_train)\n",
    "        lasso_train_pred = lasso_model.predict(X_train)\n",
    "        lasso_test_pred = lasso_model.predict(X_test)\n",
    "        lasso_train_mse = mean_squared_error(y_train, lasso_train_pred)\n",
    "        lasso_test_mse = mean_squared_error(y_test, lasso_test_pred)\n",
    "        lasso_train_r2 = r2_score(y_train, lasso_train_pred)\n",
    "        lasso_test_r2 = r2_score(y_test, lasso_test_pred)\n",
    "        \n",
    "        # Count non-zero features in Lasso\n",
    "        n_features_lasso = np.sum(np.abs(lasso_model.coef_) > 1e-5)\n",
    "        \n",
    "        ridge_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'train_size': len(train_idx),\n",
    "            'test_size': len(test_idx),\n",
    "            'train_mse': ridge_train_mse,\n",
    "            'test_mse': ridge_test_mse,\n",
    "            'train_r2': ridge_train_r2,\n",
    "            'test_r2': ridge_test_r2\n",
    "        })\n",
    "        \n",
    "        lasso_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'train_size': len(train_idx),\n",
    "            'test_size': len(test_idx),\n",
    "            'train_mse': lasso_train_mse,\n",
    "            'test_mse': lasso_test_mse,\n",
    "            'train_r2': lasso_train_r2,\n",
    "            'test_r2': lasso_test_r2,\n",
    "            'n_features': n_features_lasso\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold {fold_idx:<2} {len(train_idx)}/{len(test_idx):<8} \"\n",
    "              f\"{ridge_train_mse:<15.6f} {ridge_test_mse:<15.6f} \"\n",
    "              f\"{lasso_train_mse:<15.6f} {lasso_test_mse:<15.6f} \"\n",
    "              f\"{n_features_lasso:<15}\")\n",
    "    \n",
    "    print(\"-\" * 105)\n",
    "    \n",
    "    # Calculate statistics for Ridge\n",
    "    ridge_avg_train_mse = np.mean([r['train_mse'] for r in ridge_results])\n",
    "    ridge_avg_test_mse = np.mean([r['test_mse'] for r in ridge_results])\n",
    "    ridge_std_test_mse = np.std([r['test_mse'] for r in ridge_results])\n",
    "    ridge_avg_test_r2 = np.mean([r['test_r2'] for r in ridge_results])\n",
    "    \n",
    "    # Calculate statistics for Lasso\n",
    "    lasso_avg_train_mse = np.mean([r['train_mse'] for r in lasso_results])\n",
    "    lasso_avg_test_mse = np.mean([r['test_mse'] for r in lasso_results])\n",
    "    lasso_std_test_mse = np.std([r['test_mse'] for r in lasso_results])\n",
    "    lasso_avg_test_r2 = np.mean([r['test_r2'] for r in lasso_results])\n",
    "    lasso_avg_features = np.mean([r['n_features'] for r in lasso_results])\n",
    "    lasso_min_features = min([r['n_features'] for r in lasso_results])\n",
    "    lasso_max_features = max([r['n_features'] for r in lasso_results])\n",
    "    \n",
    "    print(f\"{'AVERAGE':<6} {'':<12} \"\n",
    "          f\"{ridge_avg_train_mse:<15.6f} {ridge_avg_test_mse:<15.6f} \"\n",
    "          f\"{lasso_avg_train_mse:<15.6f} {lasso_avg_test_mse:<15.6f} \"\n",
    "          f\"{lasso_avg_features:<15.1f}\")\n",
    "    print(f\"{'STD DEV':<6} {'':<12} \"\n",
    "          f\"{'':<15} {ridge_std_test_mse:<15.6f} \"\n",
    "          f\"{'':<15} {lasso_std_test_mse:<15.6f} \"\n",
    "          f\"{'':<15}\")\n",
    "    \n",
    "    # Detailed interpretation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä VALIDATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nüîµ RIDGE (L2) - Alpha={best_ridge_alpha:.4f}:\")\n",
    "    print(f\"   ‚Ä¢ Average Test MSE: {ridge_avg_test_mse:.6f} (¬±{ridge_std_test_mse:.6f})\")\n",
    "    print(f\"   ‚Ä¢ Average Test R¬≤: {ridge_avg_test_r2:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Train/Test Gap: {abs(ridge_avg_train_mse - ridge_avg_test_mse):.6f}\")\n",
    "    \n",
    "    if ridge_avg_train_mse < ridge_avg_test_mse * 0.75:\n",
    "        print(\"   ‚ö†Ô∏è  WARNING: Significant overfitting detected\")\n",
    "    elif abs(ridge_avg_train_mse - ridge_avg_test_mse) / ridge_avg_test_mse < 0.15:\n",
    "        print(\"   ‚úÖ GOOD: Model generalizes well to unseen data\")\n",
    "    else:\n",
    "        print(\"   üîÑ MODERATE: Acceptable generalization\")\n",
    "    \n",
    "    cv_ridge = ridge_std_test_mse / ridge_avg_test_mse\n",
    "    print(f\"   ‚Ä¢ Coefficient of Variation: {cv_ridge*100:.2f}%\", end=\"\")\n",
    "    if cv_ridge < 0.15:\n",
    "        print(\" ‚úÖ (Very stable across folds)\")\n",
    "    elif cv_ridge < 0.30:\n",
    "        print(\" üîÑ (Reasonably stable)\")\n",
    "    else:\n",
    "        print(\" ‚ö†Ô∏è  (High variability)\")\n",
    "    \n",
    "    print(f\"\\nüü¢ LASSO (L1) - Alpha={best_lasso_alpha:.6f}:\")\n",
    "    print(f\"   ‚Ä¢ Average Test MSE: {lasso_avg_test_mse:.6f} (¬±{lasso_std_test_mse:.6f})\")\n",
    "    print(f\"   ‚Ä¢ Average Test R¬≤: {lasso_avg_test_r2:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Train/Test Gap: {abs(lasso_avg_train_mse - lasso_avg_test_mse):.6f}\")\n",
    "    \n",
    "    if lasso_avg_train_mse < lasso_avg_test_mse * 0.75:\n",
    "        print(\"   ‚ö†Ô∏è  WARNING: Significant overfitting detected\")\n",
    "    elif abs(lasso_avg_train_mse - lasso_avg_test_mse) / lasso_avg_test_mse < 0.15:\n",
    "        print(\"   ‚úÖ GOOD: Model generalizes well to unseen data\")\n",
    "    else:\n",
    "        print(\"   üîÑ MODERATE: Acceptable generalization\")\n",
    "    \n",
    "    cv_lasso = lasso_std_test_mse / lasso_avg_test_mse\n",
    "    print(f\"   ‚Ä¢ Coefficient of Variation: {cv_lasso*100:.2f}%\", end=\"\")\n",
    "    if cv_lasso < 0.15:\n",
    "        print(\" ‚úÖ (Very stable across folds)\")\n",
    "    elif cv_lasso < 0.30:\n",
    "        print(\" üîÑ (Reasonably stable)\")\n",
    "    else:\n",
    "        print(\" ‚ö†Ô∏è  (High variability)\")\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Feature Selection: {lasso_avg_features:.1f}/{X.shape[1]} features \" +\n",
    "          f\"(range: {lasso_min_features}-{lasso_max_features})\")\n",
    "    \n",
    "    # Determine winner\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üèÜ MODEL COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if ridge_avg_test_mse < lasso_avg_test_mse:\n",
    "        diff_pct = (lasso_avg_test_mse - ridge_avg_test_mse) / lasso_avg_test_mse * 100\n",
    "        print(f\"\\n‚úÖ WINNER: Ridge (L2 Regularization)\")\n",
    "        print(f\"   ‚Ä¢ {diff_pct:.2f}% lower test MSE than Lasso\")\n",
    "        print(f\"   ‚Ä¢ Uses all {X.shape[1]} features with coefficient shrinkage\")\n",
    "        print(f\"   ‚Ä¢ Better when all features contain signal\")\n",
    "    else:\n",
    "        diff_pct = (ridge_avg_test_mse - lasso_avg_test_mse) / ridge_avg_test_mse * 100\n",
    "        print(f\"\\n‚úÖ WINNER: Lasso (L1 Regularization)\")\n",
    "        print(f\"   ‚Ä¢ {diff_pct:.2f}% lower test MSE than Ridge\")\n",
    "        print(f\"   ‚Ä¢ Achieves sparsity with only {lasso_avg_features:.1f}/{X.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ Better when many features are irrelevant\")\n",
    "    \n",
    "    # Statistical significance check\n",
    "    from scipy import stats\n",
    "    ridge_test_scores = [r['test_mse'] for r in ridge_results]\n",
    "    lasso_test_scores = [r['test_mse'] for r in lasso_results]\n",
    "    t_stat, p_value = stats.ttest_rel(ridge_test_scores, lasso_test_scores)\n",
    "    \n",
    "    print(f\"\\nüìâ Statistical Test (Paired t-test):\")\n",
    "    print(f\"   ‚Ä¢ p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(f\"   ‚Ä¢ Result: Difference is statistically significant (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Result: No significant difference (p ‚â• 0.05)\")\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_validation_results(ridge_results, lasso_results, X.shape[1], \n",
    "                                best_ridge_alpha, best_lasso_alpha)\n",
    "    \n",
    "    # Return validation summary\n",
    "    validation_summary = {\n",
    "        'ridge': {\n",
    "            'alpha': best_ridge_alpha,\n",
    "            'avg_test_mse': ridge_avg_test_mse,\n",
    "            'std_test_mse': ridge_std_test_mse,\n",
    "            'avg_test_r2': ridge_avg_test_r2,\n",
    "            'cv': cv_ridge\n",
    "        },\n",
    "        'lasso': {\n",
    "            'alpha': best_lasso_alpha,\n",
    "            'avg_test_mse': lasso_avg_test_mse,\n",
    "            'std_test_mse': lasso_std_test_mse,\n",
    "            'avg_test_r2': lasso_avg_test_r2,\n",
    "            'cv': cv_lasso,\n",
    "            'avg_features': lasso_avg_features\n",
    "        },\n",
    "        'winner': 'ridge' if ridge_avg_test_mse < lasso_avg_test_mse else 'lasso',\n",
    "        'p_value': p_value\n",
    "    }\n",
    "    \n",
    "    return validation_summary, ridge_results, lasso_results\n",
    "\n",
    "\n",
    "def visualize_validation_results(ridge_results, lasso_results, total_features, \n",
    "                                 ridge_alpha, lasso_alpha):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of K-Fold validation results.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "    \n",
    "    folds = [r['fold'] for r in ridge_results]\n",
    "    \n",
    "    # Extract metrics\n",
    "    ridge_train_mse = [r['train_mse'] for r in ridge_results]\n",
    "    ridge_test_mse = [r['test_mse'] for r in ridge_results]\n",
    "    ridge_test_r2 = [r['test_r2'] for r in ridge_results]\n",
    "    \n",
    "    lasso_train_mse = [r['train_mse'] for r in lasso_results]\n",
    "    lasso_test_mse = [r['test_mse'] for r in lasso_results]\n",
    "    lasso_test_r2 = [r['test_r2'] for r in lasso_results]\n",
    "    lasso_n_features = [r['n_features'] for r in lasso_results]\n",
    "    \n",
    "    # Plot 1: Ridge Train vs Test MSE\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(folds, ridge_train_mse, marker='o', label='Train MSE', \n",
    "             linewidth=2.5, markersize=9, color='#2E86AB')\n",
    "    ax1.plot(folds, ridge_test_mse, marker='s', label='Test MSE', \n",
    "             linewidth=2.5, markersize=9, color='#A23B72')\n",
    "    ax1.axhline(np.mean(ridge_test_mse), color='#A23B72', linestyle='--', alpha=0.6, linewidth=2)\n",
    "    ax1.fill_between(folds, ridge_test_mse, alpha=0.2, color='#A23B72')\n",
    "    ax1.set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('Mean Squared Error', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title(f'üîµ Ridge (Œ±={ridge_alpha:.4f}): Train vs Test', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Plot 2: Lasso Train vs Test MSE\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(folds, lasso_train_mse, marker='o', label='Train MSE', \n",
    "             linewidth=2.5, markersize=9, color='#06A77D')\n",
    "    ax2.plot(folds, lasso_test_mse, marker='s', label='Test MSE', \n",
    "             linewidth=2.5, markersize=9, color='#D64933')\n",
    "    ax2.axhline(np.mean(lasso_test_mse), color='#D64933', linestyle='--', alpha=0.6, linewidth=2)\n",
    "    ax2.fill_between(folds, lasso_test_mse, alpha=0.2, color='#D64933')\n",
    "    ax2.set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylabel('Mean Squared Error', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title(f'üü¢ Lasso (Œ±={lasso_alpha:.6f}): Train vs Test', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Plot 3: Test MSE Comparison\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    x = np.arange(len(folds))\n",
    "    width = 0.35\n",
    "    ax3.bar(x - width/2, ridge_test_mse, width, label='Ridge', \n",
    "            color='#2E86AB', alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "    ax3.bar(x + width/2, lasso_test_mse, width, label='Lasso', \n",
    "            color='#06A77D', alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "    ax3.set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    ax3.set_ylabel('Test MSE', fontsize=11, fontweight='bold')\n",
    "    ax3.set_title('Test MSE: Ridge vs Lasso', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(folds)\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    \n",
    "    # Plot 4: R¬≤ Scores Comparison\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    ax4.plot(folds, ridge_test_r2, marker='o', label='Ridge R¬≤', \n",
    "             linewidth=2.5, markersize=9, color='#2E86AB')\n",
    "    ax4.plot(folds, lasso_test_r2, marker='s', label='Lasso R¬≤', \n",
    "             linewidth=2.5, markersize=9, color='#06A77D')\n",
    "    ax4.axhline(0, color='red', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "    ax4.set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    ax4.set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "    ax4.set_title('Model Performance: R¬≤ Scores', fontsize=12, fontweight='bold')\n",
    "    ax4.legend(fontsize=10)\n",
    "    ax4.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Plot 5: Box Plot Comparison\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    box_data = [ridge_test_mse, lasso_test_mse]\n",
    "    bp = ax5.boxplot(box_data, labels=['Ridge', 'Lasso'], patch_artist=True,\n",
    "                     boxprops=dict(linewidth=2), whiskerprops=dict(linewidth=2),\n",
    "                     capprops=dict(linewidth=2), medianprops=dict(linewidth=2.5, color='red'))\n",
    "    bp['boxes'][0].set_facecolor('#2E86AB')\n",
    "    bp['boxes'][0].set_alpha(0.6)\n",
    "    bp['boxes'][1].set_facecolor('#06A77D')\n",
    "    bp['boxes'][1].set_alpha(0.6)\n",
    "    ax5.set_ylabel('Test MSE', fontsize=11, fontweight='bold')\n",
    "    ax5.set_title('Test MSE Distribution', fontsize=12, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    \n",
    "    # Plot 6: Lasso Feature Selection\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    bars = ax6.bar(folds, lasso_n_features, color='#06A77D', alpha=0.8, \n",
    "                   edgecolor='black', linewidth=1.5)\n",
    "    ax6.axhline(total_features, color='red', linestyle='--', \n",
    "                label=f'Total ({total_features})', linewidth=2.5)\n",
    "    ax6.axhline(np.mean(lasso_n_features), color='blue', linestyle='--', \n",
    "                label=f'Avg ({np.mean(lasso_n_features):.1f})', linewidth=2.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, lasso_n_features):\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(val)}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    ax6.set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    ax6.set_ylabel('Number of Features', fontsize=11, fontweight='bold')\n",
    "    ax6.set_title('üü¢ Lasso: Feature Selection', fontsize=12, fontweight='bold')\n",
    "    ax6.legend(fontsize=10)\n",
    "    ax6.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    \n",
    "    # Plot 7: Overfitting Analysis (Train-Test Gap)\n",
    "    ax7 = fig.add_subplot(gs[2, 0])\n",
    "    ridge_gaps = [train - test for train, test in zip(ridge_train_mse, ridge_test_mse)]\n",
    "    lasso_gaps = [train - test for train, test in zip(lasso_train_mse, lasso_test_mse)]\n",
    "    \n",
    "    x = np.arange(len(folds))\n",
    "    width = 0.35\n",
    "    ax7.bar(x - width/2, ridge_gaps, width, label='Ridge Gap', \n",
    "            color='#2E86AB', alpha=0.7, edgecolor='black')\n",
    "    ax7.bar(x + width/2, lasso_gaps, width, label='Lasso Gap', \n",
    "            color='#06A77D', alpha=0.7, edgecolor='black')\n",
    "    ax7.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "    ax7.set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    ax7.set_ylabel('Train MSE - Test MSE', fontsize=11, fontweight='bold')\n",
    "    ax7.set_title('Overfitting Analysis (Train-Test Gap)', fontsize=12, fontweight='bold')\n",
    "    ax7.set_xticks(x)\n",
    "    ax7.set_xticklabels(folds)\n",
    "    ax7.legend(fontsize=10)\n",
    "    ax7.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    \n",
    "    # Plot 8: Stability Analysis (CV)\n",
    "    ax8 = fig.add_subplot(gs[2, 1])\n",
    "    ridge_cv = np.std(ridge_test_mse) / np.mean(ridge_test_mse) * 100\n",
    "    lasso_cv = np.std(lasso_test_mse) / np.mean(lasso_test_mse) * 100\n",
    "    \n",
    "    models = ['Ridge', 'Lasso']\n",
    "    cvs = [ridge_cv, lasso_cv]\n",
    "    colors = ['#2E86AB', '#06A77D']\n",
    "    bars = ax8.bar(models, cvs, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, cvs):\n",
    "        height = bar.get_height()\n",
    "        ax8.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.2f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    ax8.axhline(15, color='orange', linestyle='--', label='Threshold (15%)', linewidth=2)\n",
    "    ax8.set_ylabel('Coefficient of Variation (%)', fontsize=11, fontweight='bold')\n",
    "    ax8.set_title('Model Stability (Lower = Better)', fontsize=12, fontweight='bold')\n",
    "    ax8.legend(fontsize=10)\n",
    "    ax8.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    \n",
    "    # Plot 9: Summary Statistics\n",
    "    ax9 = fig.add_subplot(gs[2, 2])\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    üìä VALIDATION SUMMARY\n",
    "    \n",
    "    Ridge (L2):\n",
    "    ‚Ä¢ Alpha: {ridge_alpha:.4f}\n",
    "    ‚Ä¢ Avg Test MSE: {np.mean(ridge_test_mse):.6f}\n",
    "    ‚Ä¢ Std Dev: {np.std(ridge_test_mse):.6f}\n",
    "    ‚Ä¢ Avg R¬≤: {np.mean(ridge_test_r2):.4f}\n",
    "    ‚Ä¢ CV: {ridge_cv:.2f}%\n",
    "    ‚Ä¢ Features: All ({total_features})\n",
    "    \n",
    "    Lasso (L1):\n",
    "    ‚Ä¢ Alpha: {lasso_alpha:.6f}\n",
    "    ‚Ä¢ Avg Test MSE: {np.mean(lasso_test_mse):.6f}\n",
    "    ‚Ä¢ Std Dev: {np.std(lasso_test_mse):.6f}\n",
    "    ‚Ä¢ Avg R¬≤: {np.mean(lasso_test_r2):.4f}\n",
    "    ‚Ä¢ CV: {lasso_cv:.2f}%\n",
    "    ‚Ä¢ Features: {np.mean(lasso_n_features):.1f}/{total_features}\n",
    "    \n",
    "    üèÜ Winner: {\"Ridge\" if np.mean(ridge_test_mse) < np.mean(lasso_test_mse) else \"Lasso\"}\n",
    "    Œî MSE: {abs(np.mean(ridge_test_mse) - np.mean(lasso_test_mse)):.6f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax9.text(0.05, 0.5, summary_text, fontsize=10, verticalalignment='center',\n",
    "             fontfamily='monospace', \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.4, pad=1))\n",
    "    \n",
    "    plt.suptitle('K-Fold Cross-Validation: Comprehensive Model Validation', \n",
    "                 fontsize=14, fontweight='bold', y=0.998)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ Key Validation Insights:\")\n",
    "    print(\"   ‚úÖ Optimized hyperparameters validated across multiple folds\")\n",
    "    print(\"   ‚úÖ Train-test gaps indicate generalization capability\")\n",
    "    print(\"   ‚úÖ Low CV% shows model stability and robustness\")\n",
    "    print(\"   ‚úÖ R¬≤ scores confirm predictive power\")\n",
    "    print(\"   ‚úÖ Lasso feature selection is consistent across folds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c371c14-907d-4ea2-a492-56066a298956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: COMPARE MODELS & FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "\n",
    "def compare_models(ridge_model, lasso_model, X, y, feature_names):\n",
    "    \"\"\"\n",
    "    Compare Ridge vs Lasso performance and feature importance.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìà MODEL COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Standardize for both models\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Predictions\n",
    "    ridge_pred = ridge_model.predict(X_scaled)\n",
    "    lasso_pred = lasso_model.predict(X_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    ridge_r2 = r2_score(y, ridge_pred)\n",
    "    lasso_r2 = r2_score(y, lasso_pred)\n",
    "    ridge_mse = mean_squared_error(y, ridge_pred)\n",
    "    lasso_mse = mean_squared_error(y, lasso_pred)\n",
    "    \n",
    "    print(\"\\nüìä Performance Metrics:\")\n",
    "    print(f\"{'Model':<15} {'R¬≤':<10} {'MSE':<12} {'Features':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Ridge':<15} {ridge_r2:<10.4f} {ridge_mse:<12.6f} {'All':<10}\")\n",
    "    print(f\"{'Lasso':<15} {lasso_r2:<10.4f} {lasso_mse:<12.6f} {np.sum(lasso_model.coef_ != 0):<10}\")\n",
    "    \n",
    "    # Feature importance (Lasso)\n",
    "    print(\"\\nüéØ Top 5 Important Features (Lasso):\")\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': np.abs(lasso_model.coef_)\n",
    "    }).sort_values('coefficient', ascending=False)\n",
    "    \n",
    "    for i, row in coef_df.head(5).iterrows():\n",
    "        if row['coefficient'] > 0:\n",
    "            print(f\"   {row['feature']:<30} {row['coefficient']:.4f}\")\n",
    "    \n",
    "    return ridge_pred, lasso_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d28ec0-0050-4b9c-9a36-11bf6f4ebfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: PORTFOLIO OPTIMIZATION WITH REGULARIZATION\n",
    "# ===========================================================================\n",
    "def optimize_portfolio_weights(features_df, lasso_model, scaler):\n",
    "    \"\"\"\n",
    "    Use Lasso predictions to optimize portfolio weights.\n",
    "    NOW: Uses rolling average + volatility penalty for diversification.\n",
    "    Apply constraints: weights sum to 1, no short selling.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üíº PORTFOLIO WEIGHT OPTIMIZATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Use ROLLING AVERAGE of last 30 days (not just last row)\n",
    "    lookback_period = min(30, len(features_df))\n",
    "    recent_data = features_df.tail(lookback_period)\n",
    "    \n",
    "    # Calculate average risk-adjusted returns\n",
    "    risk_adj_returns = {\n",
    "        'BTC': recent_data['BTC_risk_adj_return'].mean(),\n",
    "        'ETH': recent_data['ETH_risk_adj_return'].mean(),\n",
    "        'DOGE': recent_data['DOGE_risk_adj_return'].mean()\n",
    "    }\n",
    "    \n",
    "    # Calculate average volatilities\n",
    "    avg_volatility = {\n",
    "        'BTC': recent_data['BTC_volatility_forecast'].mean(),\n",
    "        'ETH': recent_data['ETH_volatility_forecast'].mean(),\n",
    "        'DOGE': recent_data['DOGE_volatility_forecast'].mean()\n",
    "    }\n",
    "    \n",
    "    # Sharpe-like score: Higher return/vol is better\n",
    "    sharpe_scores = {\n",
    "        asset: risk_adj_returns[asset] / (avg_volatility[asset] + 1e-8)\n",
    "        for asset in ['BTC', 'ETH', 'DOGE']\n",
    "    }\n",
    "    \n",
    "    # Convert to positive scores (shift if negative)\n",
    "    min_score = min(sharpe_scores.values())\n",
    "    if min_score < 0:\n",
    "        sharpe_scores = {k: v - min_score + 0.1 for k, v in sharpe_scores.items()}\n",
    "    \n",
    "    # Apply softmax with temperature parameter for diversification\n",
    "    temperature = 2.0  # Higher = more diversified (try 0.5 to 5.0)\n",
    "    exp_scores = {k: np.exp(v / temperature) for k, v in sharpe_scores.items()}\n",
    "    total = sum(exp_scores.values())\n",
    "    optimal_weights = {k: v / total for k, v in exp_scores.items()}\n",
    "    \n",
    "    # Diversification penalty: Reduce concentration\n",
    "    # If any weight > 50%, redistribute\n",
    "    max_weight = max(optimal_weights.values())\n",
    "    if max_weight > 0.5:\n",
    "        # Apply diversification constraint\n",
    "        for asset in optimal_weights:\n",
    "            if optimal_weights[asset] > 0.5:\n",
    "                excess = optimal_weights[asset] - 0.5\n",
    "                optimal_weights[asset] = 0.5\n",
    "                # Redistribute excess to others proportionally\n",
    "                other_assets = [a for a in optimal_weights if a != asset]\n",
    "                for other in other_assets:\n",
    "                    optimal_weights[other] += excess / len(other_assets)\n",
    "    \n",
    "    # Ensure weights sum to 1.0\n",
    "    total_weight = sum(optimal_weights.values())\n",
    "    optimal_weights = {k: v / total_weight for k, v in optimal_weights.items()}\n",
    "    \n",
    "    # Calculate expected portfolio metrics\n",
    "    expected_return = sum(\n",
    "        optimal_weights[asset] * risk_adj_returns[asset] \n",
    "        for asset in optimal_weights\n",
    "    )\n",
    "    \n",
    "    expected_vol = sum(\n",
    "        optimal_weights[asset] * avg_volatility[asset] \n",
    "        for asset in optimal_weights\n",
    "    )\n",
    "    \n",
    "    portfolio_sharpe = expected_return / expected_vol if expected_vol > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Optimal Portfolio Allocation (Last {lookback_period} days):\")\n",
    "    print(f\"{'Asset':<10} {'Weight':<12} {'Avg Risk-Adj Ret':<20} {'Avg Vol':<12} {'Sharpe':<10}\")\n",
    "    print(\"-\" * 75)\n",
    "    for asset in ['BTC', 'ETH', 'DOGE']:\n",
    "        print(f\"{asset:<10} {optimal_weights[asset]*100:>6.2f}%     \"\n",
    "              f\"{risk_adj_returns[asset]:>10.4f}          \"\n",
    "              f\"{avg_volatility[asset]:>8.4f}      \"\n",
    "              f\"{sharpe_scores[asset]:>8.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 75)\n",
    "    print(f\"üìà Expected Portfolio Return:  {expected_return:>10.4f}\")\n",
    "    print(f\"üìâ Expected Portfolio Vol:     {expected_vol:>10.4f}\")\n",
    "    print(f\"‚≠ê Portfolio Sharpe Ratio:     {portfolio_sharpe:>10.4f}\")\n",
    "    print(f\"üéØ Diversification Score:      {1 - max(optimal_weights.values()):.2%}\")\n",
    "    \n",
    "    return optimal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f543acc-1d67-496f-abf6-472ec7a56a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7A: PORTFOLIO VISUALIZATION & ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_portfolio_analysis(optimal_weights, features_df, lasso_model, \n",
    "                                 feature_names, validation_summary):\n",
    "    \"\"\"\n",
    "    Create comprehensive portfolio and model analysis visualizations.\n",
    "    \"\"\"\n",
    "    import matplotlib.patches as mpatches\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.35, wspace=0.35)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 1: Portfolio Allocation Pie Chart\n",
    "    # ========================================================================\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    explode = (0.05, 0.05, 0.05)\n",
    "    \n",
    "    wedges, texts, autotexts = ax1.pie(\n",
    "        optimal_weights.values(),\n",
    "        labels=optimal_weights.keys(),\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=colors,\n",
    "        explode=explode,\n",
    "        shadow=True,\n",
    "        textprops={'fontsize': 12, 'fontweight': 'bold'}\n",
    "    )\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontsize(14)\n",
    "    \n",
    "    ax1.set_title('üíº Optimal Portfolio Allocation', \n",
    "                  fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add diversification score\n",
    "    div_score = 1 - max(optimal_weights.values())\n",
    "    ax1.text(0, -1.4, f'Diversification Score: {div_score:.1%}', \n",
    "             ha='center', fontsize=11, style='italic',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 2: Asset Weights Bar Chart with Metrics\n",
    "    # ========================================================================\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    lookback = min(30, len(features_df))\n",
    "    recent_data = features_df.tail(lookback)\n",
    "    \n",
    "    metrics = {\n",
    "        'BTC': {\n",
    "            'weight': optimal_weights['BTC'],\n",
    "            'return': recent_data['BTC_risk_adj_return'].mean(),\n",
    "            'vol': recent_data['BTC_volatility_forecast'].mean()\n",
    "        },\n",
    "        'ETH': {\n",
    "            'weight': optimal_weights['ETH'],\n",
    "            'return': recent_data['ETH_risk_adj_return'].mean(),\n",
    "            'vol': recent_data['ETH_volatility_forecast'].mean()\n",
    "        },\n",
    "        'DOGE': {\n",
    "            'weight': optimal_weights['DOGE'],\n",
    "            'return': recent_data['DOGE_risk_adj_return'].mean(),\n",
    "            'vol': recent_data['DOGE_volatility_forecast'].mean()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    assets = list(metrics.keys())\n",
    "    weights = [m['weight'] * 100 for m in metrics.values()]\n",
    "    \n",
    "    bars = ax2.barh(assets, weights, color=colors, alpha=0.8, \n",
    "                    edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, weight in zip(bars, weights):\n",
    "        ax2.text(weight + 1, bar.get_y() + bar.get_height()/2, \n",
    "                f'{weight:.1f}%',\n",
    "                va='center', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    ax2.set_xlabel('Portfolio Weight (%)', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('üìä Asset Allocation Breakdown', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlim(0, max(weights) * 1.15)\n",
    "    ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 3: Risk-Return Scatter\n",
    "    # ========================================================================\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    \n",
    "    for i, (asset, data) in enumerate(metrics.items()):\n",
    "        ax3.scatter(data['vol']*100, data['return']*100, \n",
    "                   s=data['weight']*3000,  # Size by weight\n",
    "                   c=[colors[i]], alpha=0.7, \n",
    "                   edgecolors='black', linewidth=2,\n",
    "                   label=f\"{asset} ({data['weight']*100:.1f}%)\")\n",
    "    \n",
    "    ax3.set_xlabel('Volatility (%)', fontsize=11, fontweight='bold')\n",
    "    ax3.set_ylabel('Risk-Adjusted Return (%)', fontsize=11, fontweight='bold')\n",
    "    ax3.set_title('üìà Risk-Return Profile\\n(Bubble size = Portfolio weight)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.axhline(0, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax3.axvline(0, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 4: Lasso Feature Importance\n",
    "    # ========================================================================\n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    \n",
    "    # Get non-zero coefficients\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': np.abs(lasso_model.coef_)\n",
    "    }).sort_values('coefficient', ascending=False).head(10)\n",
    "    \n",
    "    coef_df = coef_df[coef_df['coefficient'] > 0]\n",
    "    \n",
    "    bars = ax4.barh(range(len(coef_df)), coef_df['coefficient'], \n",
    "                    color='#06A77D', alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    ax4.set_yticks(range(len(coef_df)))\n",
    "    ax4.set_yticklabels([f.replace('_', ' ').title() for f in coef_df['feature']], \n",
    "                        fontsize=9)\n",
    "    ax4.set_xlabel('|Coefficient|', fontsize=11, fontweight='bold')\n",
    "    ax4.set_title(f'üéØ Top {len(coef_df)} Lasso Features\\n(Out of {len(feature_names)} total)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax4.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    ax4.invert_yaxis()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 5: Model Performance Comparison\n",
    "    # ========================================================================\n",
    "    ax5 = fig.add_subplot(gs[1, 0:2])\n",
    "    \n",
    "    models = ['Ridge', 'Lasso']\n",
    "    test_mse = [\n",
    "        validation_summary['ridge']['avg_test_mse'],\n",
    "        validation_summary['lasso']['avg_test_mse']\n",
    "    ]\n",
    "    test_std = [\n",
    "        validation_summary['ridge']['std_test_mse'],\n",
    "        validation_summary['lasso']['std_test_mse']\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    bars = ax5.bar(x, test_mse, yerr=test_std, capsize=10,\n",
    "                   color=['#2E86AB', '#06A77D'], alpha=0.8,\n",
    "                   edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mse, std in zip(bars, test_mse, test_std):\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height + std,\n",
    "                f'{mse:.6f}',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels(models, fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Test MSE (with std dev)', fontsize=11, fontweight='bold')\n",
    "    ax5.set_title('üèÜ K-Fold Cross-Validation: Model Performance', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax5.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add winner annotation\n",
    "    winner_idx = 0 if test_mse[0] < test_mse[1] else 1\n",
    "    ax5.text(winner_idx, test_mse[winner_idx] + test_std[winner_idx] * 2,\n",
    "            '‚≠ê WINNER', ha='center', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='gold', alpha=0.8))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 6: Portfolio Metrics Gauge Chart\n",
    "    # ========================================================================\n",
    "    ax6 = fig.add_subplot(gs[1, 2:4])\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Calculate portfolio metrics\n",
    "    portfolio_return = sum(\n",
    "        optimal_weights[asset] * metrics[asset]['return'] \n",
    "        for asset in optimal_weights\n",
    "    )\n",
    "    portfolio_vol = sum(\n",
    "        optimal_weights[asset] * metrics[asset]['vol'] \n",
    "        for asset in optimal_weights\n",
    "    )\n",
    "    portfolio_sharpe = portfolio_return / portfolio_vol if portfolio_vol > 0 else 0\n",
    "    \n",
    "    metrics_text = f\"\"\"\n",
    "    üìä PORTFOLIO PERFORMANCE METRICS\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    Expected Return:        {portfolio_return*100:>8.2f}%\n",
    "    Expected Volatility:    {portfolio_vol*100:>8.2f}%\n",
    "    Sharpe Ratio:           {portfolio_sharpe:>8.4f}\n",
    "    \n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    üîµ Ridge Regularization:\n",
    "       ‚Ä¢ Alpha:            {validation_summary['ridge']['alpha']:>8.4f}\n",
    "       ‚Ä¢ Test MSE:         {validation_summary['ridge']['avg_test_mse']:>8.6f}\n",
    "       ‚Ä¢ Test R¬≤:          {validation_summary['ridge']['avg_test_r2']:>8.4f}\n",
    "       ‚Ä¢ Features Used:    All {len(feature_names)}\n",
    "    \n",
    "    üü¢ Lasso Regularization:\n",
    "       ‚Ä¢ Alpha:            {validation_summary['lasso']['alpha']:>8.6f}\n",
    "       ‚Ä¢ Test MSE:         {validation_summary['lasso']['avg_test_mse']:>8.6f}\n",
    "       ‚Ä¢ Test R¬≤:          {validation_summary['lasso']['avg_test_r2']:>8.4f}\n",
    "       ‚Ä¢ Features Used:    {int(validation_summary['lasso']['avg_features'])}/{len(feature_names)}\n",
    "    \n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    üìâ Statistical Test:\n",
    "       ‚Ä¢ p-value:          {validation_summary['p_value']:>8.4f}\n",
    "       ‚Ä¢ Winner:           {validation_summary['winner'].upper()}\n",
    "       ‚Ä¢ Significance:     {\"‚úÖ Yes (p < 0.05)\" if validation_summary['p_value'] < 0.05 else \"‚ùå No\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.1, 0.5, metrics_text, \n",
    "             fontfamily='monospace', fontsize=11,\n",
    "             verticalalignment='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', \n",
    "                      alpha=0.3, pad=1.5))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 7: Time Series of Portfolio Components\n",
    "    # ========================================================================\n",
    "    ax7 = fig.add_subplot(gs[2, :2])\n",
    "    \n",
    "    # Plot cumulative returns for each asset\n",
    "    lookback = min(50, len(features_df))\n",
    "    recent = features_df.tail(lookback).copy()\n",
    "    \n",
    "    for i, asset in enumerate(['BTC', 'ETH', 'DOGE']):\n",
    "        cumulative = (1 + recent[f'{asset}_return_forecast']).cumprod()\n",
    "        ax7.plot(range(len(cumulative)), cumulative, \n",
    "                label=f'{asset} ({optimal_weights[asset]*100:.1f}%)',\n",
    "                color=colors[i], linewidth=2.5, alpha=0.8)\n",
    "    \n",
    "    # Plot weighted portfolio\n",
    "    portfolio_returns = sum(\n",
    "        optimal_weights[asset] * recent[f'{asset}_return_forecast']\n",
    "        for asset in ['BTC', 'ETH', 'DOGE']\n",
    "    )\n",
    "    portfolio_cumulative = (1 + portfolio_returns).cumprod()\n",
    "    ax7.plot(range(len(portfolio_cumulative)), portfolio_cumulative,\n",
    "            label='Portfolio (Weighted)', color='black', \n",
    "            linewidth=3, linestyle='--', alpha=0.9)\n",
    "    \n",
    "    ax7.set_xlabel(f'Days (Last {lookback})', fontsize=11, fontweight='bold')\n",
    "    ax7.set_ylabel('Cumulative Return', fontsize=11, fontweight='bold')\n",
    "    ax7.set_title('üìà Cumulative Returns: Assets vs Portfolio', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax7.legend(fontsize=10, loc='best')\n",
    "    ax7.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax7.axhline(1, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 8: Volatility Comparison\n",
    "    # ========================================================================\n",
    "    ax8 = fig.add_subplot(gs[2, 2:])\n",
    "    \n",
    "    vol_data = {\n",
    "        'BTC': recent['BTC_volatility_forecast'].mean(),\n",
    "        'ETH': recent['ETH_volatility_forecast'].mean(),\n",
    "        'DOGE': recent['DOGE_volatility_forecast'].mean(),\n",
    "        'Portfolio': portfolio_vol\n",
    "    }\n",
    "    \n",
    "    bars = ax8.bar(vol_data.keys(), [v*100 for v in vol_data.values()],\n",
    "                   color=colors + ['black'], alpha=0.8,\n",
    "                   edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, (name, vol) in zip(bars, vol_data.items()):\n",
    "        height = bar.get_height()\n",
    "        ax8.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{vol*100:.2f}%',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    ax8.set_ylabel('Volatility (%)', fontsize=11, fontweight='bold')\n",
    "    ax8.set_title('üìâ Volatility Comparison\\n(Diversification Effect)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax8.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add diversification benefit annotation\n",
    "    avg_vol = np.mean([vol_data['BTC'], vol_data['ETH'], vol_data['DOGE']])\n",
    "    benefit = (avg_vol - vol_data['Portfolio']) / avg_vol * 100\n",
    "    ax8.text(0.5, 0.95, f'Diversification Benefit: {benefit:.1f}%',\n",
    "            transform=ax8.transAxes, ha='center', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "    \n",
    "    plt.suptitle('Week 5: Portfolio Optimization with Regularization - Comprehensive Analysis', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä VISUALIZATION COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n‚úÖ All 8 charts successfully generated:\")\n",
    "    print(\"   1. Portfolio allocation pie chart\")\n",
    "    print(\"   2. Asset weights breakdown\")\n",
    "    print(\"   3. Risk-return scatter plot\")\n",
    "    print(\"   4. Lasso feature importance\")\n",
    "    print(\"   5. Model performance comparison\")\n",
    "    print(\"   6. Portfolio metrics summary\")\n",
    "    print(\"   7. Cumulative returns time series\")\n",
    "    print(\"   8. Volatility comparison\")\n",
    "    print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae45a3-6cc7-45e1-9ff2-86c0d5e4ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION \n",
    "# ============================================================================\n",
    "def main():\n",
    "    print(\"\\n\" + \"üéì\" * 35)\n",
    "    print(\" WEEK 5: ML & REGULARIZATION FOR PORTFOLIO OPTIMIZATION\")\n",
    "    print(\"üéì\" * 35 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Load Week 4 forecasts\n",
    "    forecasts = load_week4_forecasts()\n",
    "    \n",
    "    # Step 2: Engineer features (prompt-assisted)\n",
    "    features = engineer_portfolio_features(forecasts)\n",
    "    \n",
    "    # Step 3: Prepare ML data\n",
    "    X, y, feature_names = prepare_ml_data(features)\n",
    "    \n",
    "    # Step 4: Ridge Regression (hyperparameter tuning)\n",
    "    # Note: ridge_scaler not used (only lasso_scaler needed for portfolio optimization)\n",
    "    ridge_model, _, best_ridge_alpha, _ = fit_ridge_regression(X, y)\n",
    "    \n",
    "    # Step 5: Lasso Regression (hyperparameter tuning)\n",
    "    lasso_model, lasso_scaler, best_lasso_alpha, lasso_n_features = fit_lasso_regression(X, y)\n",
    "    \n",
    "    # Step 5A: K-Fold Validation of OPTIMIZED models\n",
    "    # Note: individual fold results used only within validation function\n",
    "    validation_summary, _, _ = validate_optimized_models_kfold(\n",
    "        X, y, best_ridge_alpha, best_lasso_alpha, n_splits=5\n",
    "    )\n",
    "    \n",
    "    # Step 6: Compare models\n",
    "    # Note: predictions used for internal metrics display only\n",
    "    _, _ = compare_models(ridge_model, lasso_model, X, y, feature_names)\n",
    "    \n",
    "    # Step 7: Portfolio optimization\n",
    "    optimal_weights = optimize_portfolio_weights(features, lasso_model, lasso_scaler)\n",
    "    \n",
    "    # Step 8: Comprehensive visualization\n",
    "    visualize_portfolio_analysis(optimal_weights, features, lasso_model, \n",
    "                                 feature_names, validation_summary)\n",
    "    \n",
    "    # Execution verification\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üîç EXECUTION VERIFICATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   ‚úì Forecasts loaded: {len(forecasts)} days\")\n",
    "    print(f\"   ‚úì Features engineered: {len(features.columns)} columns\")\n",
    "    print(f\"   ‚úì ML data prepared: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "    print(f\"   ‚úì Ridge alpha: {best_ridge_alpha:.4f}\")\n",
    "    print(f\"   ‚úì Lasso alpha: {best_lasso_alpha:.6f}\")\n",
    "    print(f\"   ‚úì Lasso features selected: {lasso_n_features}/{X.shape[1]}\")\n",
    "    print(f\"   ‚úì K-Fold validation complete: {validation_summary['winner'].upper()} won\")\n",
    "    print(f\"   ‚úì Portfolio weights optimized:\")\n",
    "    for asset, weight in optimal_weights.items():\n",
    "        print(f\"      ‚Ä¢ {asset}: {weight*100:.2f}%\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ WEEK 5 COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nüéØ Key Takeaways:\")\n",
    "    print(\"   1. Hyperparameter tuning finds optimal alpha values\")\n",
    "    print(\"   2. K-Fold CV validates models generalize to unseen data\")\n",
    "    print(\"   3. Ridge keeps all features, Lasso selects important ones\")\n",
    "    print(\"   4. Regularization prevents overfitting in portfolio optimization\")\n",
    "    print(\"   5. Statistical tests confirm model performance differences\")\n",
    "    print(\"\\nüìö Next Week: Tree Ensembles (Random Forest, XGBoost)\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    return validation_summary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264f784-0bb9-4549-a77a-a45bf4d6adea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
