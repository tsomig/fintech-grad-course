{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c5a329-08e3-425a-813f-c6bad96ae6ee",
   "metadata": {},
   "source": [
    "# Week 7: Dimensionality Reduction & Clustering for Finance Analytics\n",
    "\n",
    "---\n",
    "\n",
    "This week, we'll explore powerful techniques for understanding high-dimensional Financial data:\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Principal Component Analysis (PCA)**: Reduce complexity while keeping important information\n",
    "- **K-Means Clustering**: Group similar assets (or tokens) for better portfolio decisions\n",
    "- **Integration**: Combine both techniques for comprehensive market analysis\n",
    "\n",
    "**Why this matters in Finance:**\n",
    "- Financial assets have many features (price, volume, TVL, volatility, sentiment, etc.)\n",
    "- Understanding relationships between these features is crucial\n",
    "- Grouping similar assets helps with diversification and risk management\n",
    "\n",
    "**By the end of this notebook, you'll be able to:**\n",
    "\n",
    "‚úÖ Reduce 20+ features to just 3-5 principal components  \n",
    "‚úÖ Identify hidden patterns in financial   \n",
    "‚úÖ Segment assets  into meaningful clusters  \n",
    "‚úÖ Build better diversified portfolios  \n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Setup: Installing Required Libraries\n",
    "\n",
    "Before we begin, we need to import our tools. Here's what each library does:\n",
    "\n",
    "- **NumPy**: Mathematical operations and arrays\n",
    "- **Pandas**: Data manipulation and analysis\n",
    "- **Matplotlib & Seaborn**: Creating beautiful visualizations\n",
    "- **Scikit-learn**: Machine learning algorithms (PCA, K-Means, Factor Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728b8b4-7c4d-46fd-b956-82b57e68afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Confirm successful import\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ ALL LIBRARIES IMPORTED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nLibraries loaded:\")\n",
    "print(\"  ‚Ä¢ NumPy version:\", np.__version__)\n",
    "print(\"  ‚Ä¢ Pandas version:\", pd.__version__)\n",
    "print(\"  ‚Ä¢ Scikit-learn: Ready for PCA and K-Means\")\n",
    "print(\"  ‚Ä¢ Matplotlib & Seaborn: Ready for visualizations\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9a3c9-78ea-41dd-8612-1c33a5c90544",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Principal Component Analysis (PCA)\n",
    "\n",
    "## üéØ What is PCA and Why Do We Need It?\n",
    "\n",
    "### The Problem: Too Many Features\n",
    "\n",
    "Imagine you're analyzing 50 DeFi tokens, each with 20 different features, for example:\n",
    "- Price returns (daily, weekly, monthly)\n",
    "- Volatility measures\n",
    "- Trading volume\n",
    "- Total Value Locked (TVL)\n",
    "- Market capitalization\n",
    "- Liquidity scores\n",
    "- Correlation with ETH/BTC\n",
    "- Social sentiment scores\n",
    "- ... and more!\n",
    "\n",
    "**That's 1,000 data points to understand!** \n",
    "\n",
    "### The Solution: PCA\n",
    "\n",
    "**Principal Component Analysis (PCA)** helps us by:\n",
    "- Finding the most important patterns in our data\n",
    "- Reducing 20 features down to 3-5 \"principal components\"\n",
    "- Keeping 80-90% of the information while removing noise\n",
    "- Making data easier to visualize and understand\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ The Intuition Behind PCA\n",
    "\n",
    "Think of PCA like taking a photograph:\n",
    "\n",
    "**Imagine you're in a room full of people:**\n",
    "- You could describe each person individually (many features: height, weight, age, etc.)\n",
    "- OR you could take a photo from the best angle that captures everyone (one principal component)\n",
    "\n",
    "**PCA finds the \"best angles\" (directions) that capture the most variation in your data.**\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Principal Component 1 (PC1)**: The direction where data varies the most\n",
    "   - Example: In DeFi, PC1 might represent \"token size\" (combining market cap, volume, TVL)\n",
    "\n",
    "2. **Principal Component 2 (PC2)**: The next direction (perpendicular to PC1) with most remaining variation\n",
    "   - Example: PC2 might represent \"risk\" (combining volatility and returns)\n",
    "\n",
    "3. **Variance Explained**: How much information each component captures\n",
    "   - We want components that explain 80-90% of total variance\n",
    "\n",
    "4. **Loadings**: How each original feature contributes to a principal component\n",
    "   - High loading = feature is important for that component\n",
    "\n",
    "---\n",
    "\n",
    "## Some Maths \n",
    "\n",
    "**Step 1:** Start with the data matrix (tokens √ó features)\n",
    "```\n",
    "         Return  Volatility  Volume  TVL  ...\n",
    "TOKEN_1   0.05      0.12      1000   5M\n",
    "TOKEN_2  -0.02      0.08       500   2M\n",
    "...\n",
    "```\n",
    "\n",
    "**Step 2:** Standardize the data (mean=0, std=1)\n",
    "- This ensures all features are on the same scale\n",
    "- Without this, large numbers (like market cap in billions) would dominate\n",
    "\n",
    "**Step 3:** Find directions of maximum variance\n",
    "- PCA calculates these using eigenvalue decomposition of the covariance matrix\n",
    "- This is done automatically in Python!\n",
    "\n",
    "**Step 4:** Transform data to the new principal component space\n",
    "```\n",
    "         PC1    PC2    PC3  ...\n",
    "TOKEN_1  2.3   -0.5    0.1\n",
    "TOKEN_2  -1.1   1.2    0.3\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üí° When Should You Use PCA in Finance?\n",
    "\n",
    "‚úÖ **USE PCA when you have:**\n",
    "- Many correlated features (e.g., different volume metrics)\n",
    "- Need to reduce dimensionality for visualization\n",
    "- Want to identify underlying market factors\n",
    "- Need to remove noise from your data\n",
    "- Want to avoid overfitting in models\n",
    "\n",
    "‚ùå **BE CAREFUL when:**\n",
    "- You have very few features (< 5)\n",
    "- Features are already uncorrelated!\n",
    "- You need to interpret individual features exactly\n",
    "- Your data has many outliers\n",
    "\n",
    "---\n",
    "\n",
    "## üè¶ Real-World Finance Applications of PCA\n",
    "\n",
    "1. **Risk Management**: Identify main sources of portfolio risk\n",
    "2. **Market Analysis**: Discover hidden market factors (size, risk, sentiment)\n",
    "3. **Portfolio Construction**: Select tokens based on different PC exposures\n",
    "4. **Regime Detection**: Identify when market structure changes\n",
    "5. **Feature Engineering**: Create better features for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dfc4cd-9c3c-40ee-9b3f-5aec076b2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePCA:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis for DeFi Portfolio Analysis\n",
    "    \n",
    "    This class wraps sklearn's PCA with helpful methods for:\n",
    "    - Fitting PCA to DeFi data\n",
    "    - Transforming data to PC space\n",
    "    - Analyzing variance explained\n",
    "    - Visualizing results\n",
    "    - Interpreting component loadings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=None):\n",
    "        \"\"\"\n",
    "        Initialize PCA\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int or None\n",
    "            Number of components to keep\n",
    "            - None = keep all components\n",
    "            - int = keep specific number (e.g., 5)\n",
    "            - Tip: Start with None to see all, then choose optimal number\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.scaler = StandardScaler()  # For standardizing features\n",
    "        self.pca = None  # Will hold the fitted PCA model\n",
    "        self.feature_names = None  # To remember original feature names\n",
    "        \n",
    "    def fit(self, X, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit PCA model to data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data matrix (e.g., 50 tokens √ó 8 features)\n",
    "        feature_names : list of str, optional\n",
    "            Names of features for better interpretation\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        # Step 1: Standardize features (VERY IMPORTANT!)\n",
    "        # This ensures all features have mean=0 and std=1\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Step 2: Fit PCA\n",
    "        self.pca = PCA(n_components=self.n_components)\n",
    "        self.pca.fit(X_scaled)\n",
    "        \n",
    "        # Step 3: Save feature names for later use\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n‚úÖ PCA fitted successfully!\")\n",
    "        print(f\"   Components: {self.pca.n_components_}\")\n",
    "        print(f\"   Total variance explained: {self.pca.explained_variance_ratio_.sum():.2%}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data to principal component space\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data to transform\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_transformed : array, shape (n_samples, n_components)\n",
    "            Data in PC space\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.pca.transform(X_scaled)\n",
    "    \n",
    "    def fit_transform(self, X, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit PCA and transform data in one step\n",
    "        \n",
    "        This is a convenience method that combines fit() and transform()\n",
    "        \"\"\"\n",
    "        self.fit(X, feature_names)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X_pca):\n",
    "        \"\"\"\n",
    "        Reconstruct original features from principal components\n",
    "        \n",
    "        Useful for understanding what PCs represent\n",
    "        \"\"\"\n",
    "        X_scaled = self.pca.inverse_transform(X_pca)\n",
    "        return self.scaler.inverse_transform(X_scaled)\n",
    "    \n",
    "    def get_variance_summary(self):\n",
    "        \"\"\"\n",
    "        Get detailed variance statistics for each component\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with columns:\n",
    "        - PC: Component name (PC1, PC2, ...)\n",
    "        - Variance_Explained: Proportion of variance (0 to 1)\n",
    "        - Cumulative_Variance: Running total of variance\n",
    "        - Eigenvalue: The actual eigenvalue (variance in that direction)\n",
    "        \"\"\"\n",
    "        var_exp = self.pca.explained_variance_ratio_\n",
    "        cum_var = np.cumsum(var_exp)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'PC': [f'PC{i+1}' for i in range(len(var_exp))],\n",
    "            'Variance_Explained': var_exp,\n",
    "            'Cumulative_Variance': cum_var,\n",
    "            'Eigenvalue': self.pca.explained_variance_\n",
    "        })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_loadings(self):\n",
    "        \"\"\"\n",
    "        Get feature loadings (how features relate to PCs)\n",
    "        \n",
    "        Loadings tell us:\n",
    "        - Which features contribute most to each PC\n",
    "        - Direction of relationship (positive or negative)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with features as rows, PCs as columns\n",
    "        \n",
    "        Interpretation:\n",
    "        - High positive loading: Feature increases with PC\n",
    "        - High negative loading: Feature decreases with PC  \n",
    "        - Near-zero loading: Feature unrelated to PC\n",
    "        \"\"\"\n",
    "        loadings = pd.DataFrame(\n",
    "            self.pca.components_.T,\n",
    "            columns=[f'PC{i+1}' for i in range(self.pca.n_components_)],\n",
    "            index=self.feature_names if self.feature_names else range(self.pca.n_components_)\n",
    "        )\n",
    "        return loadings\n",
    "    \n",
    "    def plot_scree(self, figsize=(14, 5)):\n",
    "        \"\"\"\n",
    "        Create scree plot to help choose optimal number of components\n",
    "        \n",
    "        The scree plot shows:\n",
    "        - Left: Variance per component (look for \"elbow\")\n",
    "        - Right: Cumulative variance (aim for 80-90%)\n",
    "        \"\"\"\n",
    "        var_summary = self.get_variance_summary()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Left plot: Scree Plot\n",
    "        axes[0].bar(var_summary['PC'], var_summary['Variance_Explained'], \n",
    "                    alpha=0.7, color='steelblue')\n",
    "        axes[0].plot(var_summary['PC'], var_summary['Variance_Explained'], \n",
    "                     'ro-', linewidth=2, markersize=8)\n",
    "        axes[0].set_xlabel('Principal Component', fontsize=12)\n",
    "        axes[0].set_ylabel('Variance Explained', fontsize=12)\n",
    "        axes[0].set_title('Scree Plot\\n(Look for the \"elbow\")', \n",
    "                         fontsize=13, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Right plot: Cumulative Variance\n",
    "        axes[1].plot(var_summary['PC'], var_summary['Cumulative_Variance'], \n",
    "                     'bo-', linewidth=2, markersize=8)\n",
    "        axes[1].axhline(y=0.80, color='green', linestyle='--', \n",
    "                        linewidth=2, label='80% threshold')\n",
    "        axes[1].axhline(y=0.90, color='orange', linestyle='--', \n",
    "                        linewidth=2, label='90% threshold')\n",
    "        axes[1].set_xlabel('Number of Components', fontsize=12)\n",
    "        axes[1].set_ylabel('Cumulative Variance Explained', fontsize=12)\n",
    "        axes[1].set_title('Cumulative Variance Explained', \n",
    "                         fontsize=13, fontweight='bold')\n",
    "        axes[1].legend(fontsize=10)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        axes[1].set_ylim([0, 1.05])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_loadings(self, n_components=3, n_top_features=10, figsize=(15, 5)):\n",
    "        \"\"\"\n",
    "        Visualize feature loadings for top components\n",
    "        \n",
    "        Shows which features are most important for each PC\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int\n",
    "            Number of PCs to display (default: 3)\n",
    "        n_top_features : int\n",
    "            How many features to show per PC (default: 10)\n",
    "        \"\"\"\n",
    "        loadings = self.get_loadings()\n",
    "        n_components = min(n_components, loadings.shape[1])\n",
    "        \n",
    "        fig, axes = plt.subplots(1, n_components, figsize=figsize)\n",
    "        if n_components == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i in range(n_components):\n",
    "            pc_col = f'PC{i+1}'\n",
    "            \n",
    "            # Get top features by absolute loading\n",
    "            top_features = loadings[pc_col].abs().nlargest(n_top_features)\n",
    "            sorted_loadings = loadings.loc[top_features.index, pc_col].sort_values()\n",
    "            \n",
    "            # Color code: red=negative, green=positive\n",
    "            colors = ['red' if x < 0 else 'green' for x in sorted_loadings.values]\n",
    "            \n",
    "            axes[i].barh(range(len(sorted_loadings)), sorted_loadings.values, \n",
    "                        color=colors, alpha=0.7)\n",
    "            axes[i].set_yticks(range(len(sorted_loadings)))\n",
    "            axes[i].set_yticklabels(sorted_loadings.index, fontsize=9)\n",
    "            axes[i].set_xlabel('Loading', fontsize=11)\n",
    "            axes[i].set_title(f'{pc_col}\\n({self.pca.explained_variance_ratio_[i]:.1%} variance)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "            axes[i].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "            axes[i].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ SimplePCA class created successfully!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nAvailable methods:\")\n",
    "print(\"  ‚Ä¢ fit(X, feature_names) - Fit PCA to data\")\n",
    "print(\"  ‚Ä¢ transform(X) - Transform data to PC space\")\n",
    "print(\"  ‚Ä¢ fit_transform(X, feature_names) - Fit and transform in one step\")\n",
    "print(\"  ‚Ä¢ get_variance_summary() - Get variance statistics\")\n",
    "print(\"  ‚Ä¢ get_loadings() - Get feature loadings\")\n",
    "print(\"  ‚Ä¢ plot_scree() - Visualize variance explained\")\n",
    "print(\"  ‚Ä¢ plot_loadings() - Visualize feature importance\")\n",
    "#print(\"\\nüëâ Next: We'll generate some synthetic DeFi data to practice with\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cc3a0-e20b-4935-a2dc-c539783921ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Generating Synthetic DeFi Portfolio Data\n",
    "\n",
    "### Why Start with Synthetic Data?\n",
    "\n",
    "Before working with real data (which can be messy and incomplete), we'll create **realistic synthetic data** to:\n",
    "\n",
    "‚úÖ **Learn the techniques** without API complications  \n",
    "‚úÖ **Understand patterns** we deliberately create  \n",
    "‚úÖ **Control the experiment** - we know the \"ground truth\"  \n",
    "‚úÖ **Practice interpretation** in a safe environment  \n",
    "\n",
    "Later, you can apply these exact same techniques to real data from CoinGecko, CoinMarketCap, or on-chain sources!\n",
    "\n",
    "---\n",
    "\n",
    "### üé≤ What Data Will We Generate?\n",
    "\n",
    "We'll create a portfolio of **50 DeFi tokens**, each with **8 realistic features**:\n",
    "\n",
    "| Feature | Description | Range | Why It Matters |\n",
    "|---------|-------------|-------|----------------|\n",
    "| **Mean_Return** | Average daily return | -5% to +5% | Profitability measure |\n",
    "| **Volatility** | Price volatility (30-day) | 2% to 10% | Risk measure |\n",
    "| **Volume_USD** | Daily trading volume | $100K to $100M | Liquidity indicator |\n",
    "| **TVL_USD** | Total Value Locked | $1M to $1B | Protocol size (epresents the total value of cryptocurrency assets locked in a decentralized finance protocol or ecosystem. |\n",
    "| **Market_Cap** | Token market cap | $10M to $10B | Token size |\n",
    "| **Liquidity_Score** | Ease of trading | 0 to 100 | Trading friction |\n",
    "| **ETH_Correlation** | Correlation with Ethereum | -1 to +1 | Market exposure |\n",
    "| **Sentiment_Score** | Social sentiment | 0 to 100 | Market perception |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Creating Realistic Token Groups\n",
    "\n",
    "To make this realistic, we'll create **3 distinct groups** of tokens:\n",
    "\n",
    "#### 1. **High-Cap Stable Tokens** (30% of portfolio)\n",
    "- Large, established protocols (like Uniswap, Aave)\n",
    "- **Characteristics:**\n",
    "  - Lower volatility (more stable)\n",
    "  - Higher liquidity (easy to trade)\n",
    "  - Large market cap\n",
    "  - Lower but steady returns\n",
    "\n",
    "#### 2. **Mid-Cap Growth Tokens** (40% of portfolio)\n",
    "- Medium-sized protocols with growth potential\n",
    "- **Characteristics:**\n",
    "  - Moderate volatility\n",
    "  - Moderate liquidity\n",
    "  - Higher returns (but riskier)\n",
    "  - Good balance of risk/reward\n",
    "\n",
    "#### 3. **Low-Cap High-Risk Tokens** (30% of portfolio)\n",
    "- Smaller, newer protocols (higher risk/reward)\n",
    "- **Characteristics:**\n",
    "  - High volatility (price swings)\n",
    "  - Lower liquidity (harder to trade)\n",
    "  - Potential for high returns\n",
    "  - Higher risk of loss\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ The Statistical Process\n",
    "\n",
    "We'll use different **probability distributions** to model each feature:\n",
    "\n",
    "- **Normal distribution** ‚Üí Returns, Sentiment (symmetric around mean)\n",
    "- **Gamma distribution** ‚Üí Volatility (always on the positive axis, skewed right)\n",
    "- **Log-normal distribution** ‚Üí Volume, TVL, Market Cap (highly skewed, realistic for financial data)\n",
    "- **Beta distribution** ‚Üí Liquidity, Correlation (defined on the interval [0, 1] or (0, 1) and bounded between values)\n",
    "\n",
    "**Key Point**: different features have different natural patterns, and we're simulating them realistically!\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Why This Matters for PCA\n",
    "\n",
    "By creating **structured groups**, we're simulating what PCA should discover:\n",
    "\n",
    "- **PC1** might capture \"token size\" (combining Volume, TVL, Market Cap)\n",
    "- **PC2** might capture \"risk profile\" (combining Volatility, Returns)\n",
    "- **PC3** might capture \"market exposure\" (ETH correlation, sentiment)\n",
    "\n",
    "**PCA should find these patterns automatically** \n",
    "\n",
    "---\n",
    "\n",
    "### üéì Learning Objective\n",
    "\n",
    "After generating this data, you'll be able to:\n",
    "1. See what realistic DeFi portfolio data looks like\n",
    "2. Understand correlations between features\n",
    "3. Have clean data to practice PCA on\n",
    "4. Later compare with your own real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea6a12-e0bd-4614-9b28-a0774145a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_defi_portfolio_data(n_tokens=50, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic DeFi portfolio data with realistic features\n",
    "    \n",
    "    This function creates a dataset that mimics real DeFi token characteristics,\n",
    "    including natural correlations and three distinct token groups.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_tokens : int, default=50\n",
    "        Number of tokens to generate\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility (same seed = same data)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : DataFrame\n",
    "        Portfolio data with tokens as rows, features as columns\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate token names\n",
    "    token_names = [f'TOKEN_{i+1}' for i in range(n_tokens)]\n",
    "    \n",
    "    print(\"üé≤ Generating synthetic DeFi portfolio data...\")\n",
    "    print(f\"   Creating {n_tokens} tokens with 8 features each\\n\")\n",
    "    \n",
    "    # Generate base features using realistic distributions\n",
    "    data = {\n",
    "        # Returns: Normal distribution (can be positive or negative)\n",
    "        # Mean slightly positive (0.1% daily), with variation\n",
    "        'Mean_Return': np.random.normal(0.001, 0.05, n_tokens),\n",
    "        \n",
    "        # Volatility: Gamma distribution (always positive, right-skewed)\n",
    "        # Typical crypto volatility: 2-8% daily\n",
    "        'Volatility': np.random.gamma(2, 0.02, n_tokens),\n",
    "        \n",
    "        # Volume: Log-normal (highly skewed, like real trading volume)\n",
    "        # Range: roughly $100K to $100M\n",
    "        'Volume_USD': np.random.lognormal(15, 2, n_tokens),\n",
    "        \n",
    "        # TVL: Log-normal (realistic for protocol sizes)\n",
    "        # Range: roughly $1M to $1B\n",
    "        'TVL_USD': np.random.lognormal(17, 2, n_tokens),\n",
    "        \n",
    "        # Market Cap: Log-normal (realistic for token valuations)\n",
    "        # Range: roughly $10M to $10B\n",
    "        'Market_Cap': np.random.lognormal(18, 2.5, n_tokens),\n",
    "        \n",
    "        # Liquidity Score: Beta distribution (bounded 0-100)\n",
    "        # Most tokens have decent liquidity\n",
    "        'Liquidity_Score': np.random.beta(5, 2, n_tokens) * 100,\n",
    "        \n",
    "        # ETH Correlation: Beta distribution scaled to [-1, 1]\n",
    "        # Most tokens positively correlated with ETH\n",
    "        'ETH_Correlation': np.random.beta(5, 2, n_tokens) * 2 - 1,\n",
    "        \n",
    "        # Sentiment: Normal distribution around 50\n",
    "        # Range: 0-100, centered at 50 (neutral)\n",
    "        'Sentiment_Score': np.random.normal(50, 15, n_tokens)\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, index=token_names)\n",
    "    \n",
    "    # Now add structure: create 3 distinct token groups\n",
    "    print(\"üìä Creating token groups with distinct characteristics...\")\n",
    "    \n",
    "    # Calculate group sizes\n",
    "    n_group1 = int(n_tokens * 0.3)  # 30% - High-cap stable\n",
    "    n_group2 = int(n_tokens * 0.4)  # 40% - Mid-cap growth\n",
    "    # Remaining 30% - Low-cap high-risk\n",
    "    \n",
    "    # GROUP 1: High-cap stable tokens (like Uniswap, Aave)\n",
    "    # Lower volatility, higher liquidity\n",
    "    print(f\"   Group 1 (High-cap stable): {n_group1} tokens\")\n",
    "    df.iloc[:n_group1, df.columns.get_loc('Volatility')] *= 0.5  # 50% less volatile\n",
    "    df.iloc[:n_group1, df.columns.get_loc('Liquidity_Score')] *= 1.3  # 30% more liquid\n",
    "    df.iloc[:n_group1, df.columns.get_loc('Market_Cap')] *= 1.5  # Larger market cap\n",
    "    \n",
    "    # GROUP 2: Mid-cap growth tokens\n",
    "    # Higher returns, moderate risk\n",
    "    print(f\"   Group 2 (Mid-cap growth): {n_group2} tokens\")\n",
    "    df.iloc[n_group1:n_group1+n_group2, df.columns.get_loc('Mean_Return')] *= 1.5  # 50% higher returns\n",
    "    df.iloc[n_group1:n_group1+n_group2, df.columns.get_loc('Volatility')] *= 1.2  # 20% more volatile\n",
    "    \n",
    "    # GROUP 3: Low-cap high-risk tokens (remaining tokens)\n",
    "    # High volatility, lower liquidity, high potential returns\n",
    "    print(f\"   Group 3 (Low-cap high-risk): {n_tokens - n_group1 - n_group2} tokens\")\n",
    "    df.iloc[n_group1+n_group2:, df.columns.get_loc('Volatility')] *= 2.0  # 2x more volatile\n",
    "    df.iloc[n_group1+n_group2:, df.columns.get_loc('Mean_Return')] *= 2.0  # 2x returns (high risk/reward)\n",
    "    df.iloc[n_group1+n_group2:, df.columns.get_loc('Liquidity_Score')] *= 0.6  # 40% less liquid\n",
    "    df.iloc[n_group1+n_group2:, df.columns.get_loc('Market_Cap')] *= 0.3  # Smaller market cap\n",
    "    \n",
    "    # Clip values to reasonable ranges\n",
    "    df['Liquidity_Score'] = df['Liquidity_Score'].clip(0, 100)\n",
    "    df['Sentiment_Score'] = df['Sentiment_Score'].clip(0, 100)\n",
    "    df['ETH_Correlation'] = df['ETH_Correlation'].clip(-1, 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Generate the data\n",
    "print(\"=\" * 70)\n",
    "defi_data = generate_defi_portfolio_data(n_tokens=50, random_state=42)\n",
    "print(\"\\n‚úÖ Data generation complete!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nüìã Dataset Overview:\")\n",
    "print(f\"   Shape: {defi_data.shape[0]} tokens √ó {defi_data.shape[1]} features\")\n",
    "print(f\"   Memory usage: {defi_data.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\nüìä First 5 tokens:\")\n",
    "display(defi_data.head())\n",
    "\n",
    "print(\"\\nüìà Statistical Summary:\")\n",
    "display(defi_data.describe().round(3))\n",
    "\n",
    "print(\"\\nüîç Feature Correlations (top 3):\")\n",
    "corr_matrix = defi_data.corr()\n",
    "# Get the upper triangle of correlation matrix (avoid duplicates)\n",
    "corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append({\n",
    "            'Feature 1': corr_matrix.columns[i],\n",
    "            'Feature 2': corr_matrix.columns[j],\n",
    "            'Correlation': corr_matrix.iloc[i, j]\n",
    "        })\n",
    "corr_df = pd.DataFrame(corr_pairs)\n",
    "corr_df = corr_df.reindex(corr_df['Correlation'].abs().sort_values(ascending=False).index)\n",
    "print(\"\\nStrongest correlations:\")\n",
    "display(corr_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53311f-5d13-4208-930b-45d4ed84a5ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Applying PCA to Our DeFi Data\n",
    "\n",
    "### What We're About to Do\n",
    "\n",
    "Now that we have our synthetic DeFi portfolio data, let's use PCA to:\n",
    "1. **Reduce** 8 features down to a smaller number of principal components\n",
    "2. **Discover** which features naturally group together\n",
    "3. **Understand** the main sources of variation in our portfolio\n",
    "4. **Visualize** complex relationships in 2D space\n",
    "\n",
    "---\n",
    "\n",
    "### Key Questions We'll Answer\n",
    "\n",
    "Before running PCA, think about these questions:\n",
    "\n",
    "**Q1: How many principal components do we need?**\n",
    "- We have 8 features, so we can have up to 8 PCs\n",
    "- But we probably don't need all 8!\n",
    "- Goal: Keep enough PCs to explain 80-90% of variance\n",
    "\n",
    "**Q2: What will the principal components represent?**\n",
    "- PC1: Likely a \"size factor\" (Market Cap + TVL + Volume)\n",
    "- PC2: Possibly a \"risk factor\" (Volatility + Returns)\n",
    "- PC3: Maybe \"market dynamics\" (ETH Correlation + Sentiment)\n",
    "\n",
    "**Q3: Which features are most important?**\n",
    "- Loadings will tell us which features contribute most to each PC\n",
    "- High loadings = important features\n",
    "- Low loadings = less important features\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Step-by-Step Process\n",
    "\n",
    "Here's what happens when we call `fit_transform()`:\n",
    "```\n",
    "Step 1: Standardization\n",
    "‚îú‚îÄ Original data: different scales (Market Cap in billions, Returns in decimals)\n",
    "‚îú‚îÄ Standardized data: all features have mean=0, std=1\n",
    "‚îî‚îÄ Why? So large numbers don't dominate the analysis\n",
    "\n",
    "Step 2: Calculate Covariance Matrix\n",
    "‚îú‚îÄ Measures how features vary together\n",
    "‚îú‚îÄ High covariance = features move together\n",
    "‚îî‚îÄ This matrix is 8√ó8 (one entry for each feature pair)\n",
    "\n",
    "Step 3: Eigendecomposition\n",
    "‚îú‚îÄ Find eigenvectors (directions of maximum variance)\n",
    "‚îú‚îÄ Find eigenvalues (amount of variance in each direction)\n",
    "‚îî‚îÄ Sort by eigenvalues (largest first = PC1, PC2, ...)\n",
    "\n",
    "Step 4: Transform Data\n",
    "‚îú‚îÄ Project original data onto principal components\n",
    "‚îú‚îÄ Input: 50 tokens √ó 8 features\n",
    "‚îî‚îÄ Output: 50 tokens √ó k components (we'll use k=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "After running PCA, we'll examine:\n",
    "\n",
    "**1. Variance Explained**\n",
    "```\n",
    "PC1: 35% of variance  ‚Üê Most important\n",
    "PC2: 25% of variance  ‚Üê Second most important\n",
    "PC3: 15% of variance\n",
    "PC4: 10% of variance\n",
    "PC5: 8% of variance\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Total: 93% of variance ‚Üê Good! We kept most information\n",
    "```\n",
    "\n",
    "**2. Loadings (Feature Importance)**\n",
    "```\n",
    "PC1 Loadings:\n",
    "  Market_Cap:  0.85  ‚Üê High positive loading\n",
    "  TVL_USD:     0.82  ‚Üê High positive loading\n",
    "  Volume_USD:  0.78  ‚Üê High positive loading\n",
    "  \n",
    "Interpretation: PC1 represents \"TOKEN SIZE\"\n",
    "```\n",
    "\n",
    "**3. Scree Plot**\n",
    "- Shows variance per component\n",
    "- Look for \"elbow\" where variance drops sharply\n",
    "- Components after elbow add little value\n",
    "\n",
    "---\n",
    "\n",
    "###  Success Criteria\n",
    "\n",
    "Our PCA is successful if:\n",
    "\n",
    "‚úÖ **First 3-5 PCs explain 80-90% of variance**\n",
    "- Means we successfully reduced dimensionality\n",
    "\n",
    "‚úÖ **PC loadings are interpretable**\n",
    "- We can give meaningful names to PCs\n",
    "\n",
    "‚úÖ **Scree plot shows clear elbow**\n",
    "- Easy to decide how many PCs to keep\n",
    "\n",
    "‚úÖ **Features group logically**\n",
    "- Size features load on same PC\n",
    "- Risk features load on same PC\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Important Reminders\n",
    "\n",
    "**Remember:**\n",
    "- PCA assumes **linear relationships** between features\n",
    "- **Outliers** can affect results (our synthetic data is clean)\n",
    "- **Standardization is critical** (SimplePCA does this automatically)\n",
    "- PCs are **orthogonal** (perpendicular) - no correlation between them\n",
    "\n",
    "---\n",
    "\n",
    "**We'll use **n_components=5** to start (keeping 5 out of 8 dimensions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07534e4-310f-4587-aff7-09d4a298a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FITTING PCA TO DeFi PORTFOLIO DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize PCA with 5 components\n",
    "# Why 5? It's a good starting point - not too few, not too many\n",
    "# We'll use the scree plot to decide if we should adjust\n",
    "print(\"\\nüîß Initializing PCA model...\")\n",
    "print(\"   Setting n_components=5 (keeping 5 out of 8 dimensions)\")\n",
    "\n",
    "pca_model = SimplePCA(n_components=5)\n",
    "\n",
    "# Fit PCA and transform data in one step\n",
    "print(\"\\n‚öôÔ∏è  Fitting PCA and transforming data...\")\n",
    "print(\"   This will:\")\n",
    "print(\"   1. Standardize all features (mean=0, std=1)\")\n",
    "print(\"   2. Calculate principal components\")\n",
    "print(\"   3. Transform data to PC space\")\n",
    "\n",
    "X_pca = pca_model.fit_transform(\n",
    "    defi_data.values,  # The actual data (as numpy array)\n",
    "    feature_names=defi_data.columns.tolist()  # Feature names for interpretation\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRANSFORMATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show dimensionality reduction\n",
    "print(f\"\\nüìä Dimensionality Reduction:\")\n",
    "print(f\"   Original shape: {defi_data.shape[0]} tokens √ó {defi_data.shape[1]} features\")\n",
    "print(f\"   Reduced shape:  {X_pca.shape[0]} tokens √ó {X_pca.shape[1]} components\")\n",
    "print(f\"   Reduction: {defi_data.shape[1]} ‚Üí {X_pca.shape[1]} dimensions\")\n",
    "print(f\"   Space savings: {(1 - X_pca.shape[1]/defi_data.shape[1])*100:.1f}% fewer dimensions!\")\n",
    "\n",
    "# Get variance summary\n",
    "variance_summary = pca_model.get_variance_summary()\n",
    "\n",
    "print(f\"\\nüìà Variance Explained by Each Component:\")\n",
    "print(\"=\" * 70)\n",
    "display(variance_summary.round(4))\n",
    "\n",
    "# Highlight key findings\n",
    "total_var = variance_summary['Cumulative_Variance'].iloc[-1]\n",
    "pc1_var = variance_summary['Variance_Explained'].iloc[0]\n",
    "pc2_var = variance_summary['Variance_Explained'].iloc[1]\n",
    "pc3_var = variance_summary['Variance_Explained'].iloc[2]\n",
    "\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "print(f\"   ‚Ä¢ PC1 alone captures {pc1_var:.1%} of all variation\")\n",
    "print(f\"   ‚Ä¢ PC1 + PC2 together capture {variance_summary['Cumulative_Variance'].iloc[1]:.1%}\")\n",
    "print(f\"   ‚Ä¢ First 3 PCs capture {variance_summary['Cumulative_Variance'].iloc[2]:.1%}\")\n",
    "print(f\"   ‚Ä¢ All 5 PCs capture {total_var:.1%} of total variance\")\n",
    "\n",
    "# Interpretation\n",
    "if total_var >= 0.90:\n",
    "    print(f\"\\n‚úÖ Excellent! {total_var:.1%} variance retained with just 5 components!\")\n",
    "elif total_var >= 0.80:\n",
    "    print(f\"\\n‚úÖ Good! {total_var:.1%} variance retained - acceptable for most applications\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Only {total_var:.1%} variance retained - might need more components\")\n",
    "\n",
    "# Show a preview of transformed data\n",
    "print(f\"\\nüìã Sample of Transformed Data (PC space):\")\n",
    "print(\"   First 5 tokens in principal component coordinates:\")\n",
    "X_pca_df = pd.DataFrame(\n",
    "    X_pca[:5],  # First 5 tokens\n",
    "    columns=[f'PC{i+1}' for i in range(X_pca.shape[1])],\n",
    "    index=defi_data.index[:5]\n",
    ")\n",
    "display(X_pca_df.round(3))\n",
    "\n",
    "print(\"\\nüí° Understanding the transformed data:\")\n",
    "print(\"   ‚Ä¢ Each row is still a token\")\n",
    "print(\"   ‚Ä¢ Each column is now a principal component (not original features)\")\n",
    "print(\"   ‚Ä¢ Values represent the token's 'score' on that component\")\n",
    "print(\"   ‚Ä¢ Positive values = high on that component\")\n",
    "print(\"   ‚Ä¢ Negative values = low on that component\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ PCA fitting complete! Data successfully transformed.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981a73a-cad6-4dc0-a507-b8db7852d661",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Understanding the Scree Plot\n",
    "\n",
    "### What is a Scree Plot?\n",
    "\n",
    "A **scree plot** is your main tool for deciding how many principal components to keep.\n",
    "\n",
    "The name comes from geology: \"scree\" is the pile of rocks at the bottom of a cliff. The plot looks like a cliff face - it starts steep (important components) and flattens out (less important components).\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Two Plots, One Decision\n",
    "\n",
    "We'll create **two complementary visualizations**:\n",
    "\n",
    "#### Plot 1: Scree Plot (Variance per Component)\n",
    "```\n",
    "Variance\n",
    "   |  ‚óè\n",
    "   |     ‚óè\n",
    "   |        ‚óè\n",
    "   |           ‚óè___‚óè___‚óè___‚óè  ‚Üê \"Scree\" (less important)\n",
    "   |  ^\n",
    "   |  |\n",
    "   |  ‚îî‚îÄ \"Elbow\" (cutoff point)\n",
    "   |\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "      PC1  PC2  PC3  PC4  PC5\n",
    "```\n",
    "\n",
    "**What to look for:**\n",
    "- **Steep drop** at the beginning = important components\n",
    "- **Elbow point** = where the curve flattens\n",
    "- **Flat part** (the \"scree\") = components that add little value\n",
    "\n",
    "**Rule of thumb:** Keep components up to (and including) the elbow.\n",
    "\n",
    "---\n",
    "\n",
    "#### Plot 2: Cumulative Variance\n",
    "```\n",
    "Cumulative\n",
    "Variance\n",
    "   100% |                    ‚óè___\n",
    "    90% |               ‚óè        ‚Üê Goal: reach here\n",
    "    80% |          ‚óè            ‚Üê Minimum acceptable\n",
    "        |      ‚óè\n",
    "        |  ‚óè\n",
    "     0% |\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "           PC1  PC2  PC3  PC4  PC5\n",
    "```\n",
    "\n",
    "**What to look for:**\n",
    "- **80% line (green)** = minimum threshold for most applications\n",
    "- **90% line (orange)** = ideal threshold\n",
    "- **Where curve hits threshold** = how many PCs you need\n",
    "\n",
    "**Rule of thumb:** \n",
    "- Keep enough PCs to explain **80-90%** of variance\n",
    "- More than 90% = excellent\n",
    "- Less than 80% = might be losing too much information\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ How to Decide: The Decision Matrix\n",
    "\n",
    "| Variance Retained | Number of PCs | Decision |\n",
    "|------------------|---------------|----------|\n",
    "| < 70% | Too few | ‚ö†Ô∏è Add more components |\n",
    "| 70-80% | Acceptable | ‚úÖ OK for exploratory analysis |\n",
    "| 80-90% | Good | ‚úÖ Ideal for most applications |\n",
    "| 90-95% | Excellent | ‚úÖ Great balance |\n",
    "| > 95% | Possibly too many | ‚ö†Ô∏è May be keeping noise |\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Real-World Example Interpretations\n",
    "\n",
    "**Scenario 1: Clear Elbow**\n",
    "```\n",
    "Scree Plot: 40% ‚Üí 25% ‚Üí 15% ‚Üí 8% ‚Üí 5% ‚Üí 3% ‚Üí 2%\n",
    "                          ‚Üë\n",
    "                       Elbow here!\n",
    "Decision: Keep 3 components (80% variance)\n",
    "```\n",
    "\n",
    "**Scenario 2: Gradual Decline**\n",
    "```\n",
    "Scree Plot: 30% ‚Üí 20% ‚Üí 15% ‚Üí 12% ‚Üí 10% ‚Üí 8% ‚Üí 5%\n",
    "                          No obvious elbow\n",
    "Decision: Use cumulative variance plot, aim for 80-90%\n",
    "```\n",
    "\n",
    "**Scenario 3: Dominant First Component**\n",
    "```\n",
    "Scree Plot: 70% ‚Üí 10% ‚Üí 8% ‚Üí 5% ‚Üí 4% ‚Üí 2% ‚Üí 1%\n",
    "            ‚Üë\n",
    "         PC1 dominates!\n",
    "Decision: PC1 captures most variation - other PCs less important\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Why This Matters in DeFi\n",
    "\n",
    "**Keeping too few components:**\n",
    "\n",
    "‚ùå Lose important information about token diversity  \n",
    "‚ùå Miss subtle risk patterns  \n",
    "‚ùå Oversimplify market structure  \n",
    "\n",
    "**Keeping too many components:**\n",
    "\n",
    "‚ùå Include noise instead of signal  \n",
    "‚ùå Make models overly complex  \n",
    "‚ùå Harder to interpret results  \n",
    "\n",
    "**Getting it just right:**\n",
    "\n",
    "‚úÖ Balance between simplicity and completeness  \n",
    "‚úÖ Retain meaningful patterns  \n",
    "‚úÖ Remove noise and redundancy  \n",
    "\n",
    "---\n",
    "\n",
    "###  What You'll Learn from Œøur Plot\n",
    "\n",
    "After running the next cell, you should be able to answer:\n",
    "\n",
    "1. **Where is the elbow?** (Which PC does the curve flatten at?)\n",
    "2. **How many PCs for 80% variance?** (Read from cumulative plot)\n",
    "3. **How many PCs for 90% variance?** (Read from cumulative plot)\n",
    "4. **Is PC1 dominant?** (Does it explain > 40% alone?)\n",
    "5. **What's our recommendation?** (Based on both plots)\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation Tips\n",
    "\n",
    "**Strong first component (PC1 > 40%):**\n",
    "- Suggests one dominant factor drives variation\n",
    "- In DeFi: Often \"size\" (Market Cap, TVL, Volume)\n",
    "- Interpretation: Market dominated by large vs. small tokens\n",
    "\n",
    "**Balanced components (PC1 ‚âà PC2 ‚âà PC3):**\n",
    "- Multiple independent factors at play\n",
    "- In DeFi: Size, risk, sentiment all matter equally\n",
    "- Interpretation: Complex market with multiple dynamics\n",
    "\n",
    "**Gradual decline (no clear elbow):**\n",
    "- Variation spread across many factors\n",
    "- In DeFi: Many small, independent patterns\n",
    "- Interpretation: Diverse portfolio with many micro-factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa60270-bae4-4e7c-9ad4-8c120bcc6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"VISUALIZING VARIANCE EXPLAINED - SCREE PLOT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Creating scree plot to determine optimal number of components...\")\n",
    "print(\"   This will help us decide: 'How many PCs should we keep?'\\n\")\n",
    "\n",
    "# Create the scree plot\n",
    "fig = pca_model.plot_scree(figsize=(14, 5))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPRETING THE SCREE PLOT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get variance data for analysis\n",
    "variance_summary = pca_model.get_variance_summary()\n",
    "\n",
    "# Analyze the results\n",
    "print(\"\\nüìà LEFT PLOT - Individual Variance (Scree Plot):\")\n",
    "print(\"   This shows how much variance each PC explains individually.\")\n",
    "print()\n",
    "\n",
    "# Find the \"elbow\" - where variance drops significantly\n",
    "variances = variance_summary['Variance_Explained'].values\n",
    "var_diffs = np.diff(variances)  # Differences between consecutive PCs\n",
    "elbow_candidate = np.argmax(np.abs(var_diffs)) + 1  # +1 because diff reduces array size\n",
    "\n",
    "print(f\"   ‚Ä¢ PC1 explains: {variances[0]:.1%} ‚Üê Most important!\")\n",
    "print(f\"   ‚Ä¢ PC2 explains: {variances[1]:.1%}\")\n",
    "print(f\"   ‚Ä¢ PC3 explains: {variances[2]:.1%}\")\n",
    "\n",
    "if variances[0] > 0.40:\n",
    "    print(f\"\\n   üí° PC1 dominates with {variances[0]:.1%}!\")\n",
    "    print(f\"      This suggests ONE main factor drives variation in our portfolio\")\n",
    "    print(f\"      (Likely 'size': Market Cap + TVL + Volume)\")\n",
    "elif variances[0] < 0.30:\n",
    "    print(f\"\\n   üí° Variance is distributed across multiple components\")\n",
    "    print(f\"      This suggests MULTIPLE independent factors are important\")\n",
    "else:\n",
    "    print(f\"\\n   üí° Balanced importance across first few components\")\n",
    "\n",
    "print(f\"\\n   üîç Elbow appears around: PC{elbow_candidate}\")\n",
    "print(f\"      (Largest drop in variance between components)\")\n",
    "\n",
    "print(\"\\nüìà RIGHT PLOT - Cumulative Variance:\")\n",
    "print(\"   This shows total variance as we add more components.\")\n",
    "print()\n",
    "\n",
    "# Find how many PCs needed for different thresholds\n",
    "cumvar = variance_summary['Cumulative_Variance'].values\n",
    "n_80 = np.argmax(cumvar >= 0.80) + 1\n",
    "n_90 = np.argmax(cumvar >= 0.90) + 1\n",
    "\n",
    "print(f\"   ‚Ä¢ To explain 80% variance: Need {n_80} components\")\n",
    "print(f\"   ‚Ä¢ To explain 90% variance: Need {n_90} components\")\n",
    "print(f\"   ‚Ä¢ Our 5 components explain: {cumvar[4]:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Make recommendation\n",
    "if cumvar[2] >= 0.80:  # First 3 PCs explain 80%+\n",
    "    optimal = 3\n",
    "    reason = \"First 3 PCs capture sufficient variance (>80%)\"\n",
    "elif cumvar[3] >= 0.85:  # First 4 PCs explain 85%+\n",
    "    optimal = 4\n",
    "    reason = \"First 4 PCs provide good balance of simplicity and completeness\"\n",
    "else:\n",
    "    optimal = 5\n",
    "    reason = \"5 PCs needed to reach 85%+ variance threshold\"\n",
    "\n",
    "print(f\"\\n‚úÖ OPTIMAL NUMBER OF COMPONENTS: {optimal}\")\n",
    "print(f\"   Reason: {reason}\")\n",
    "print(f\"   Variance explained: {cumvar[optimal-1]:.1%}\")\n",
    "print(f\"   Dimensionality reduction: {defi_data.shape[1]} ‚Üí {optimal} ({optimal/defi_data.shape[1]*100:.0f}% of original)\")\n",
    "\n",
    "print(\"\\nüìä Summary Statistics:\")\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Original Features',\n",
    "        'Optimal PCs',\n",
    "        'Variance Retained',\n",
    "        'Information Lost',\n",
    "        'Compression Ratio'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{defi_data.shape[1]}\",\n",
    "        f\"{optimal}\",\n",
    "        f\"{cumvar[optimal-1]:.1%}\",\n",
    "        f\"{(1-cumvar[optimal-1]):.1%}\",\n",
    "        f\"{defi_data.shape[1]/optimal:.1f}x\"\n",
    "    ]\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\nüí° What this means:\")\n",
    "print(f\"   ‚Ä¢ We can represent our portfolio with just {optimal} numbers per token\")\n",
    "print(f\"   ‚Ä¢ Instead of tracking 8 features, we track {optimal} principal components\")\n",
    "print(f\"   ‚Ä¢ We keep {cumvar[optimal-1]:.1%} of the information\")\n",
    "print(f\"   ‚Ä¢ We lose only {(1-cumvar[optimal-1]):.1%} of the information\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Examine component loadings (which features contribute to each PC)\")\n",
    "print(\"   2. Interpret what each PC represents\")\n",
    "print(\"   3. Name the PCs based on their loadings\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c759cae-c6c7-4915-bab8-61f074964a61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Understanding Principal Component Loadings\n",
    "\n",
    "### What Are Loadings?\n",
    "\n",
    "**Loadings** are the bridge between our original features and the mysterious principal components.\n",
    "\n",
    "Think of loadings as **weights** or **contributions**:\n",
    "- They tell us how each original feature contributes to each principal component\n",
    "- High loading = feature is important for that PC\n",
    "- Low loading = feature is unimportant for that PC\n",
    "\n",
    "---\n",
    "\n",
    "### üìä The Loadings Matrix\n",
    "\n",
    "After PCA, we get a **loadings matrix** that looks like this:\n",
    "```\n",
    "                    PC1      PC2      PC3      PC4      PC5\n",
    "Mean_Return        0.15     0.82    -0.23     0.11     0.05\n",
    "Volatility         0.08     0.78     0.31    -0.15     0.22\n",
    "Volume_USD         0.89     0.12     0.08     0.25    -0.11\n",
    "TVL_USD            0.91    -0.05     0.15    -0.08     0.18\n",
    "Market_Cap         0.87     0.18    -0.12     0.32     0.09\n",
    "Liquidity_Score    0.45    -0.32     0.65     0.28    -0.15\n",
    "ETH_Correlation    0.22     0.15    -0.18     0.85     0.31\n",
    "Sentiment_Score    0.11    -0.21     0.71    -0.12     0.58\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ How to Read Loadings\n",
    "\n",
    "**Magnitude (Absolute Value):**\n",
    "- **|Loading| > 0.7** ‚Üí STRONG relationship (feature defines this PC)\n",
    "- **|Loading| = 0.4-0.7** ‚Üí MODERATE relationship (feature contributes)\n",
    "- **|Loading| < 0.4** ‚Üí WEAK relationship (feature barely matters)\n",
    "\n",
    "**Sign (Positive or Negative):**\n",
    "- **Positive (+)** ‚Üí Feature increases as PC increases\n",
    "- **Negative (‚àí)** ‚Üí Feature decreases as PC increases\n",
    "\n",
    "**Example from above:**\n",
    "```\n",
    "PC1 has high loadings on:\n",
    "  ‚Ä¢ Volume_USD:  +0.89  ‚Üê Strongly positive\n",
    "  ‚Ä¢ TVL_USD:     +0.91  ‚Üê Strongly positive\n",
    "  ‚Ä¢ Market_Cap:  +0.87  ‚Üê Strongly positive\n",
    "\n",
    "Interpretation: PC1 = \"TOKEN SIZE FACTOR\"\n",
    "High PC1 score = large, liquid tokens\n",
    "Low PC1 score = small, illiquid tokens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ Step-by-Step Interpretation Process\n",
    "\n",
    "#### Step 1: Look at Each PC Separately\n",
    "\n",
    "For each principal component:\n",
    "1. **Sort loadings by absolute value** (largest to smallest)\n",
    "2. **Identify top 3-5 features** with highest |loadings|\n",
    "3. **Note the signs** (all positive? all negative? mixed?)\n",
    "\n",
    "#### Step 2: Find the Theme\n",
    "\n",
    "Ask yourself: *\"What do these high-loading features have in common?\"*\n",
    "\n",
    "**Example interpretations:**\n",
    "\n",
    "**If high loadings on Market_Cap, TVL, Volume:**\n",
    "‚Üí \"Size Factor\" or \"Liquidity Factor\"\n",
    "\n",
    "**If high loadings on Volatility, Mean_Return:**\n",
    "‚Üí \"Risk Factor\" or \"Risk-Return Factor\"\n",
    "\n",
    "**If high loadings on ETH_Correlation, Sentiment:**\n",
    "‚Üí \"Market Exposure Factor\" or \"Sentiment Factor\"\n",
    "\n",
    "#### Step 3: Name the Component\n",
    "\n",
    "Give it a meaningful name that captures its essence:\n",
    "- PC1: \"Token Size\"\n",
    "- PC2: \"Risk Profile\"\n",
    "- PC3: \"Market Sentiment\"\n",
    "- etc.\n",
    "\n",
    "---\n",
    "\n",
    "### üé® Visualizing Loadings\n",
    "\n",
    "We'll create **horizontal bar charts** for each PC showing:\n",
    "\n",
    "**Green bars** = Positive loadings (feature increases with PC)\n",
    "```\n",
    "Market_Cap     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà +0.87\n",
    "TVL_USD        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà +0.91\n",
    "Volume_USD     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà +0.89\n",
    "```\n",
    "\n",
    "**Red bars** = Negative loadings (feature decreases with PC)\n",
    "```\n",
    "Sentiment      ‚ñà‚ñà‚ñà -0.21\n",
    "Liquidity      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà -0.32\n",
    "```\n",
    "\n",
    "**Length** = Strength of relationship (longer = stronger)\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Common Patterns in DeFi\n",
    "\n",
    "Based on experience with DeFi data, here's what you typically see:\n",
    "\n",
    "#### Pattern 1: Size/Liquidity Factor (Usually PC1)\n",
    "```\n",
    "High positive loadings:\n",
    "  ‚Ä¢ Market_Cap\n",
    "  ‚Ä¢ TVL_USD\n",
    "  ‚Ä¢ Volume_USD\n",
    "  ‚Ä¢ Liquidity_Score\n",
    "\n",
    "Interpretation: Large, established protocols vs. small, new ones\n",
    "```\n",
    "\n",
    "#### Pattern 2: Risk/Return Factor (Usually PC2)\n",
    "```\n",
    "High positive loadings:\n",
    "  ‚Ä¢ Volatility\n",
    "  ‚Ä¢ Mean_Return\n",
    "\n",
    "Interpretation: High-risk/high-return vs. stable/conservative\n",
    "```\n",
    "\n",
    "#### Pattern 3: Market Exposure (Usually PC3)\n",
    "```\n",
    "High positive loadings:\n",
    "  ‚Ä¢ ETH_Correlation\n",
    "  ‚Ä¢ Sentiment_Score\n",
    "\n",
    "Interpretation: Market-following vs. independent tokens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üö´ Common Interpretation Mistakes\n",
    "\n",
    "**Mistake 1: Focusing only on the highest loading**\n",
    "‚ùå \"PC1 is just Market_Cap\"\n",
    "‚úÖ \"PC1 represents overall size, combining Market_Cap, TVL, and Volume\"\n",
    "\n",
    "**Mistake 2: Ignoring signs**\n",
    "‚ùå \"These features all load on PC2\"\n",
    "‚úÖ \"Volatility loads positively, while Liquidity loads negatively on PC2\"\n",
    "\n",
    "**Mistake 3: Over-interpreting weak loadings**\n",
    "‚ùå \"This 0.15 loading is important\"\n",
    "‚úÖ \"Loadings below 0.4 are generally not meaningful\"\n",
    "\n",
    "**Mistake 4: Forcing an interpretation**\n",
    "‚ùå \"This PC must represent something!\"\n",
    "‚úÖ \"Sometimes a PC captures residual noise - that's okay\"\n",
    "\n",
    "---\n",
    "\n",
    "###  Why This Matters\n",
    "\n",
    "Understanding loadings helps you:\n",
    "\n",
    "1. **Name your PCs meaningfully** (not just \"PC1, PC2, PC3\")\n",
    "2. **Explain results to stakeholders** (\"We're selecting tokens based on size and risk factors\")\n",
    "3. **Make portfolio decisions** (\"Let's balance our exposure across all 3 factors\")\n",
    "4. **Validate your analysis** (\"Does this make economic sense?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b70c4-b612-494d-8a0b-e6041bb1cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ANALYZING PRINCIPAL COMPONENT LOADINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüîç Loadings show how each original feature contributes to each PC\")\n",
    "print(\"   High |loading| = feature is important for that component\")\n",
    "print(\"   Sign indicates direction: + or -\\n\")\n",
    "\n",
    "# Get the loadings matrix\n",
    "loadings = pca_model.get_loadings()\n",
    "\n",
    "print(\"üìä LOADINGS MATRIX:\")\n",
    "print(\"   (Rows = Features, Columns = Principal Components)\")\n",
    "print(\"=\" * 70)\n",
    "display(loadings.round(3))\n",
    "\n",
    "print(\"\\nüí° How to read this table:\")\n",
    "print(\"   ‚Ä¢ Values close to ¬±1.0 = STRONG relationship\")\n",
    "print(\"   ‚Ä¢ Values close to ¬±0.5 = MODERATE relationship\")\n",
    "print(\"   ‚Ä¢ Values close to 0.0 = WEAK/NO relationship\")\n",
    "print(\"   ‚Ä¢ Positive = feature increases with PC\")\n",
    "print(\"   ‚Ä¢ Negative = feature decreases with PC\")\n",
    "\n",
    "# Analyze each PC\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPRETING EACH PRINCIPAL COMPONENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(min(3, loadings.shape[1])):  # Analyze first 3 PCs\n",
    "    pc_name = f'PC{i+1}'\n",
    "    pc_loadings = loadings[pc_name]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{pc_name} - {pca_model.pca.explained_variance_ratio_[i]:.1%} of variance\")\n",
    "    print('='*70)\n",
    "    \n",
    "    # Get top positive and negative loadings\n",
    "    top_positive = pc_loadings.nlargest(3)\n",
    "    top_negative = pc_loadings.nsmallest(3)\n",
    "    \n",
    "    print(f\"\\nüî∫ Top POSITIVE loadings (features that increase with {pc_name}):\")\n",
    "    for feature, loading in top_positive.items():\n",
    "        if loading > 0.3:  # Only show meaningful loadings\n",
    "            bar = '‚ñà' * int(abs(loading) * 20)\n",
    "            print(f\"   {feature:20s} {loading:+.3f}  {bar}\")\n",
    "    \n",
    "    print(f\"\\nüîª Top NEGATIVE loadings (features that decrease with {pc_name}):\")\n",
    "    for feature, loading in top_negative.items():\n",
    "        if loading < -0.3:  # Only show meaningful loadings\n",
    "            bar = '‚ñà' * int(abs(loading) * 20)\n",
    "            print(f\"   {feature:20s} {loading:+.3f}  {bar}\")\n",
    "    \n",
    "    # Suggest interpretation\n",
    "    print(f\"\\nüí≠ Interpretation Hint:\")\n",
    "    \n",
    "    # Analyze dominant features\n",
    "    abs_loadings = pc_loadings.abs()\n",
    "    dominant_features = abs_loadings[abs_loadings > 0.5].sort_values(ascending=False)\n",
    "    \n",
    "    if len(dominant_features) > 0:\n",
    "        print(f\"   Strong loadings on: {', '.join(dominant_features.index.tolist())}\")\n",
    "        \n",
    "        # Pattern matching for common DeFi factors\n",
    "        if any(feat in dominant_features.index for feat in ['Market_Cap', 'TVL_USD', 'Volume_USD']):\n",
    "            print(f\"   ‚Üí This looks like a SIZE/LIQUIDITY factor\")\n",
    "            print(f\"   ‚Üí Separates large established tokens from small new ones\")\n",
    "        \n",
    "        if any(feat in dominant_features.index for feat in ['Volatility', 'Mean_Return']):\n",
    "            print(f\"   ‚Üí This looks like a RISK/RETURN factor\")\n",
    "            print(f\"   ‚Üí Separates high-risk/high-return from stable tokens\")\n",
    "        \n",
    "        if any(feat in dominant_features.index for feat in ['ETH_Correlation', 'Sentiment_Score']):\n",
    "            print(f\"   ‚Üí This looks like a MARKET EXPOSURE factor\")\n",
    "            print(f\"   ‚Üí Separates market-following from independent tokens\")\n",
    "    else:\n",
    "        print(f\"   No dominant features - might be capturing residual variance\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Create visual representation of loadings\n",
    "print(\"\\nüìä Creating loading visualizations...\")\n",
    "print(\"   Green bars = positive loadings (feature increases with PC)\")\n",
    "print(\"   Red bars = negative loadings (feature decreases with PC)\")\n",
    "print(\"   Length = strength of relationship\\n\")\n",
    "\n",
    "fig = pca_model.plot_loadings(n_components=3, n_top_features=8, figsize=(15, 5))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY: NAMING OUR PRINCIPAL COMPONENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a summary table\n",
    "pc_interpretations = []\n",
    "for i in range(min(3, loadings.shape[1])):\n",
    "    pc_name = f'PC{i+1}'\n",
    "    variance = pca_model.pca.explained_variance_ratio_[i]\n",
    "    \n",
    "    # Get dominant features\n",
    "    pc_loadings = loadings[pc_name]\n",
    "    top_features = pc_loadings.abs().nlargest(3)\n",
    "    \n",
    "    # Suggest a name based on features\n",
    "    feature_list = top_features.index.tolist()\n",
    "    if any(f in feature_list for f in ['Market_Cap', 'TVL_USD', 'Volume_USD']):\n",
    "        suggested_name = \"Token Size/Liquidity\"\n",
    "    elif any(f in feature_list for f in ['Volatility', 'Mean_Return']):\n",
    "        suggested_name = \"Risk/Return Profile\"\n",
    "    elif any(f in feature_list for f in ['ETH_Correlation', 'Sentiment_Score']):\n",
    "        suggested_name = \"Market Exposure\"\n",
    "    else:\n",
    "        suggested_name = \"Mixed Factor\"\n",
    "    \n",
    "    pc_interpretations.append({\n",
    "        'Component': pc_name,\n",
    "        'Variance': f\"{variance:.1%}\",\n",
    "        'Top Features': ', '.join(feature_list),\n",
    "        'Suggested Name': suggested_name\n",
    "    })\n",
    "\n",
    "interpretation_df = pd.DataFrame(pc_interpretations)\n",
    "display(interpretation_df)\n",
    "\n",
    "print(\"\\nüí° Using These Interpretations:\")\n",
    "print(\"   ‚Ä¢ PC1 (Size): Select tokens across different size categories\")\n",
    "print(\"   ‚Ä¢ PC2 (Risk): Balance high-risk and low-risk tokens\")\n",
    "print(\"   ‚Ä¢ PC3 (Exposure): Diversify market exposure\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd81db-d5af-4782-8d26-e2ef14954721",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: K-Means Clustering\n",
    "\n",
    "## üéØ What is Clustering?\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Imagine you walk into a room with 50 people. Your brain automatically groups them:\n",
    "- **Group 1**: Tall people\n",
    "- **Group 2**: Medium height people  \n",
    "- **Group 3**: Short people\n",
    "\n",
    "**Clustering does the same thing with data** - it finds natural groups of similar observations.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Cluster DeFi Tokens?\n",
    "\n",
    "### The Portfolio Problem\n",
    "\n",
    "You have 50 DeFi tokens. You want to:\n",
    "- **Build a diversified portfolio** \n",
    "- **Understand market segments** (which tokens behave similarly?)\n",
    "- **Manage risk** (identify high-risk vs. low-risk groups)\n",
    "- **Find opportunities** (discover undervalued tokens in each cluster)\n",
    "\n",
    "**Without clustering:** You analyze each token individually (50 decisions)  \n",
    "**With clustering:** You analyze clusters (3-5 groups), then pick from each\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ What is K-Means?\n",
    "\n",
    "**K-Means** is the most popular clustering algorithm. It's simple but powerful.\n",
    "\n",
    "### The Algorithm in Plain English\n",
    "```\n",
    "1. Start: Randomly place k \"cluster centers\" in your data\n",
    "\n",
    "2. Assignment: Assign each token to its nearest center\n",
    "   ‚Ä¢ TOKEN_1 is closest to Center A ‚Üí assign to Cluster A\n",
    "   ‚Ä¢ TOKEN_2 is closest to Center B ‚Üí assign to Cluster B\n",
    "   ‚Ä¢ ... repeat for all tokens\n",
    "\n",
    "3. Update: Move each center to the average of its assigned tokens\n",
    "   ‚Ä¢ Center A moves to the middle of all Cluster A tokens\n",
    "   ‚Ä¢ Center B moves to the middle of all Cluster B tokens\n",
    "\n",
    "4. Repeat: Keep doing steps 2-3 until centers stop moving\n",
    "\n",
    "5. Done: You now have k clusters of similar tokens!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## The Maths\n",
    "\n",
    "**Goal:** Minimize the distance between tokens and their cluster centers\n",
    "```\n",
    "Minimize: Sum of (distance from each token to its cluster center)¬≤\n",
    "\n",
    "Or mathematically:\n",
    "J = Œ£ Œ£ ||token - center||¬≤\n",
    "```\n",
    "\n",
    "**In practice:** The algorithm finds clusters where:\n",
    "- **Within-cluster similarity is HIGH** (tokens in same cluster are similar)\n",
    "- **Between-cluster similarity is LOW** (different clusters are distinct)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Concepts\n",
    "\n",
    "### 1. **k** = Number of Clusters\n",
    "- You must choose k before running the algorithm\n",
    "- Common values: k=3, 4, or 5 for portfolio segmentation\n",
    "- **Too few (k=2):** Oversimplification (only \"big\" and \"small\")\n",
    "- **Too many (k=10):** Over-segmentation (too complex)\n",
    "\n",
    "### 2. **Centroid** = Cluster Center\n",
    "- The \"average\" token in each cluster\n",
    "- Not a real token - it's the mathematical center\n",
    "- Represents the typical characteristics of that cluster\n",
    "\n",
    "### 3. **Inertia** = Within-Cluster Distance\n",
    "- Lower is better\n",
    "- Measures how tight/compact the clusters are\n",
    "- Used in the \"elbow method\" to find optimal k\n",
    "\n",
    "### 4. **Silhouette Score** = Cluster Quality\n",
    "- Range: -1 (wrong cluster) to +1 (perfect cluster)\n",
    "- Measures how similar tokens are to their cluster vs. other clusters\n",
    "- **> 0.5** = Good clustering\n",
    "- **< 0.2** = Poor clustering\n",
    "\n",
    "---\n",
    "\n",
    "## üè¶ DeFi Applications of K-Means\n",
    "\n",
    "### Application 1: Portfolio Diversification\n",
    "```\n",
    "Cluster 0: Large-cap stable (Uniswap, Aave, Compound)\n",
    "Cluster 1: Mid-cap growth (newer protocols with traction)\n",
    "Cluster 2: Small-cap high-risk (early stage, high potential)\n",
    "Cluster 3: DeFi blue chips (established, high TVL)\n",
    "\n",
    "Strategy: Pick 2 tokens from each cluster ‚Üí instant diversification!\n",
    "```\n",
    "\n",
    "### Application 2: Risk Management\n",
    "```\n",
    "Cluster 0: Low volatility group (conservative)\n",
    "Cluster 1: Medium volatility group (balanced)\n",
    "Cluster 2: High volatility group (aggressive)\n",
    "\n",
    "Strategy: Allocate capital based on risk tolerance\n",
    "```\n",
    "\n",
    "### Application 3: Market Segmentation\n",
    "```\n",
    "Cluster 0: DEX tokens (Uniswap-style)\n",
    "Cluster 1: Lending protocols (Aave-style)\n",
    "Cluster 2: Yield aggregators (Yearn-style)\n",
    "\n",
    "Strategy: Ensure exposure across DeFi categories\n",
    "```\n",
    "\n",
    "### Application 4: Peer Comparison\n",
    "```\n",
    "Find which cluster a token belongs to\n",
    "Compare it to other tokens in same cluster\n",
    "Identify outliers (tokens that don't fit any cluster well)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  K-Means vs. Other Clustering Methods\n",
    "\n",
    "| Method | Pros | Cons | When to Use |\n",
    "|--------|------|------|-------------|\n",
    "| **K-Means** | Fast, simple, works well | Requires choosing k, assumes spherical clusters | Most DeFi portfolios |\n",
    "| **DBSCAN** | Finds arbitrary shapes, handles outliers | Slower, sensitive to parameters | Complex market structures |\n",
    "| **Hierarchical** | Shows cluster relationships, no k needed | Very slow for large data | Exploratory analysis |\n",
    "| **GMM** | Probabilistic, soft clustering | More complex, slower | When uncertainty matters |\n",
    "\n",
    "**For this course:** We use K-Means because it's the most practical for DeFi portfolios.\n",
    "\n",
    "---\n",
    "\n",
    "## üö¶ The K-Means Process\n",
    "\n",
    "### Step 1: Choose k (number of clusters)\n",
    "**Tools:** Elbow method, Silhouette analysis  \n",
    "**Goal:** Find the \"right\" number of groups  \n",
    "\n",
    "### Step 2: Standardize features\n",
    "**Why:** So large numbers (Market Cap) don't dominate small numbers (Returns)  \n",
    "**How:** Our `SimpleKMeans` class does this automatically  \n",
    "\n",
    "### Step 3: Run K-Means\n",
    "**Process:** Algorithm finds optimal cluster centers  \n",
    "**Time:** Usually very fast (seconds)  \n",
    "\n",
    "### Step 4: Analyze clusters\n",
    "**Questions:**\n",
    "- How many tokens in each cluster?\n",
    "- What are the characteristics of each cluster?\n",
    "- Do the clusters make economic sense?\n",
    "\n",
    "### Step 5: Use for decisions\n",
    "**Applications:**\n",
    "- Portfolio construction\n",
    "- Risk assessment\n",
    "- Token selection\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Assumptions and Limitations\n",
    "\n",
    "### Assumptions:\n",
    "\n",
    "‚úÖ **Clusters are roughly spherical** (similar variance in all directions)  \n",
    "‚úÖ **Clusters are similar in size** (K-Means prefers balanced clusters)  \n",
    "‚úÖ **Distance metric is meaningful** (Euclidean distance works for our features)  \n",
    "\n",
    "### Limitations:\n",
    "\n",
    "‚ùå **Must choose k beforehand** (trial and error required)  \n",
    "‚ùå **Sensitive to initialization** (different runs may give different results)  \n",
    "‚ùå **Doesn't handle outliers well** (outliers pull centroids)  \n",
    "‚ùå **Assumes convex clusters** (can't find complex shapes)  \n",
    "\n",
    "**Solution:** We'll use multiple techniques to validate our clusters!\n",
    "\n",
    "---\n",
    "\n",
    "## What Makes Good Clusters?\n",
    "\n",
    "**Good clusters have:**\n",
    "1. **High within-cluster similarity** (tokens in cluster are similar)\n",
    "2. **Low between-cluster similarity** (clusters are distinct)\n",
    "3. **Economic interpretability** (clusters make business sense)\n",
    "4. **Balanced sizes** (not one huge cluster and tiny others)\n",
    "5. **Stability** (same clusters on different data samples)\n",
    "\n",
    "**Bad clusters:**\n",
    "- One huge cluster with many tiny ones\n",
    "- Clusters that overlap significantly\n",
    "- No clear interpretation (random groupings)\n",
    "- Change dramatically with slight parameter changes\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives for This Section\n",
    "\n",
    "By the end of clustering, you'll be able to:\n",
    "\n",
    "‚úÖ Use the elbow method to find optimal k  \n",
    "‚úÖ Fit K-Means to DeFi portfolio data  \n",
    "‚úÖ Interpret cluster centroids  \n",
    "‚úÖ Assign meaningful names to clusters  \n",
    "‚úÖ Build diversified portfolios using clusters  \n",
    "‚úÖ Validate cluster quality with metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007b032-2753-4136-8e13-25a2138bd5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleKMeans:\n",
    "    \"\"\"\n",
    "    K-Means Clustering for DeFi Portfolio Segmentation\n",
    "    \n",
    "    This class wraps sklearn's KMeans with helpful methods for:\n",
    "    - Finding optimal number of clusters\n",
    "    - Fitting K-Means to DeFi data\n",
    "    - Analyzing cluster characteristics\n",
    "    - Visualizing clusters\n",
    "    - Interpreting results for portfolio decisions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters=3, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize K-Means\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_clusters : int, default=3\n",
    "            Number of clusters (k)\n",
    "            - Start with 3-5 for portfolio segmentation\n",
    "            - Use elbow method to find optimal value\n",
    "        random_state : int, default=42\n",
    "            Random seed for reproducibility\n",
    "            - Same seed = same results every time\n",
    "            - Important for comparing different runs\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()  # For standardizing features\n",
    "        self.kmeans = None  # Will hold the fitted KMeans model\n",
    "        self.feature_names = None  # To remember feature names\n",
    "        \n",
    "    def fit(self, X, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit K-Means model to data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data matrix (e.g., 50 tokens √ó 8 features)\n",
    "        feature_names : list of str, optional\n",
    "            Names of features for interpretation\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        # Step 1: Standardize features (CRITICAL!)\n",
    "        # Without this, features with large values dominate\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Step 2: Fit K-Means\n",
    "        self.kmeans = KMeans(\n",
    "            n_clusters=self.n_clusters,\n",
    "            random_state=self.random_state,\n",
    "            n_init=10  # Run algorithm 10 times, keep best result\n",
    "        )\n",
    "        self.kmeans.fit(X_scaled)\n",
    "        \n",
    "        # Step 3: Save feature names\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n‚úÖ K-Means fitted successfully!\")\n",
    "        print(f\"   Number of clusters: {self.n_clusters}\")\n",
    "        print(f\"   Inertia (within-cluster sum of squares): {self.kmeans.inertia_:.2f}\")\n",
    "        print(f\"   Lower inertia = tighter clusters\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Assign clusters to new data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            New tokens to classify\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        labels : array, shape (n_samples,)\n",
    "            Cluster assignments (0, 1, 2, ...)\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.kmeans.predict(X_scaled)\n",
    "    \n",
    "    def fit_predict(self, X, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit K-Means and predict clusters in one step\n",
    "        \n",
    "        Convenience method combining fit() and predict()\n",
    "        \"\"\"\n",
    "        self.fit(X, feature_names)\n",
    "        return self.kmeans.labels_\n",
    "    \n",
    "    def get_cluster_centers(self):\n",
    "        \"\"\"\n",
    "        Get cluster centroids in original feature scale\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with clusters as rows, features as columns\n",
    "        Shows the \"average token\" in each cluster\n",
    "        \"\"\"\n",
    "        # Get centroids from scaled space\n",
    "        centers_scaled = self.kmeans.cluster_centers_\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        centers = self.scaler.inverse_transform(centers_scaled)\n",
    "        \n",
    "        df = pd.DataFrame(\n",
    "            centers,\n",
    "            columns=self.feature_names if self.feature_names else range(centers.shape[1]),\n",
    "            index=[f'Cluster_{i}' for i in range(self.n_clusters)]\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_cluster_summary(self, X):\n",
    "        \"\"\"\n",
    "        Get summary statistics for each cluster\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with cluster sizes and percentages\n",
    "        \"\"\"\n",
    "        clusters = self.kmeans.labels_\n",
    "        \n",
    "        summary = []\n",
    "        for i in range(self.n_clusters):\n",
    "            mask = clusters == i\n",
    "            summary.append({\n",
    "                'Cluster': i,\n",
    "                'Size': mask.sum(),\n",
    "                'Percentage': f\"{100 * mask.sum() / len(clusters):.1f}%\"\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary)\n",
    "    \n",
    "    def plot_elbow(self, X, max_k=10, figsize=(12, 5)):\n",
    "        \"\"\"\n",
    "        Create elbow plot to find optimal number of clusters\n",
    "        \n",
    "        The \"elbow method\" helps choose k by showing:\n",
    "        - Left plot: Inertia vs k (look for elbow)\n",
    "        - Right plot: Silhouette score vs k (higher is better)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Data to cluster\n",
    "        max_k : int, default=10\n",
    "            Maximum number of clusters to test\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        inertias = []\n",
    "        silhouettes = []\n",
    "        K_range = range(2, max_k + 1)\n",
    "        \n",
    "        print(f\"\\nüîÑ Testing different numbers of clusters (k = 2 to {max_k})...\")\n",
    "        print(\"   This may take a moment...\\n\")\n",
    "        \n",
    "        for k in K_range:\n",
    "            # Fit K-Means with k clusters\n",
    "            kmeans = KMeans(n_clusters=k, random_state=self.random_state, n_init=10)\n",
    "            kmeans.fit(X_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            inertias.append(kmeans.inertia_)\n",
    "            silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "            \n",
    "            print(f\"   k={k}: inertia={kmeans.inertia_:>8.2f}, silhouette={silhouettes[-1]:>6.3f}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Testing complete!\\n\")\n",
    "        \n",
    "        # Create plots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Left: Elbow plot (Inertia)\n",
    "        axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "        axes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "        axes[0].set_ylabel('Inertia (Within-Cluster SS)', fontsize=12)\n",
    "        axes[0].set_title('Elbow Method\\n(Look for the \"elbow\")', \n",
    "                         fontsize=13, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Mark potential elbow\n",
    "        inertia_diffs = np.diff(inertias)\n",
    "        elbow_k = np.argmax(np.diff(inertia_diffs)) + 2  # +2 due to double diff\n",
    "        axes[0].axvline(x=elbow_k, color='red', linestyle='--', \n",
    "                       linewidth=2, alpha=0.5, label=f'Potential elbow at k={elbow_k}')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Right: Silhouette Score\n",
    "        axes[1].plot(K_range, silhouettes, 'ro-', linewidth=2, markersize=8)\n",
    "        axes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "        axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "        axes[1].set_title('Silhouette Score\\n(Higher is better)', \n",
    "                         fontsize=13, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Mark best silhouette\n",
    "        best_k = K_range[np.argmax(silhouettes)]\n",
    "        axes[1].axvline(x=best_k, color='green', linestyle='--', \n",
    "                       linewidth=2, alpha=0.5, label=f'Best silhouette at k={best_k}')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_clusters_2d(self, X_2d, labels=None, figsize=(10, 8)):\n",
    "        \"\"\"\n",
    "        Plot clusters in 2D space (after dimensionality reduction)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_2d : array-like, shape (n_samples, 2)\n",
    "            2D representation of data (e.g., from PCA)\n",
    "        labels : list of str, optional\n",
    "            Token names for annotation\n",
    "        \"\"\"\n",
    "        clusters = self.kmeans.labels_\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Use distinct colors for each cluster\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, self.n_clusters))\n",
    "        \n",
    "        # Plot each cluster separately\n",
    "        for i in range(self.n_clusters):\n",
    "            mask = clusters == i\n",
    "            ax.scatter(X_2d[mask, 0], X_2d[mask, 1], \n",
    "                      c=[colors[i]], label=f'Cluster {i}',\n",
    "                      s=80, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        # Add labels to some points (not all, to avoid clutter)\n",
    "        if labels is not None:\n",
    "            step = max(1, len(labels)//20)  # Label ~20 points\n",
    "            for i in range(0, len(labels), step):\n",
    "                ax.annotate(labels[i], (X_2d[i, 0], X_2d[i, 1]),\n",
    "                          fontsize=8, alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Dimension 1', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Dimension 2', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'K-Means Clustering ({self.n_clusters} clusters)', \n",
    "                    fontsize=13, fontweight='bold')\n",
    "        ax.legend(fontsize=10, loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ SimpleKMeans class created successfully!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nAvailable methods:\")\n",
    "print(\"  ‚Ä¢ fit(X, feature_names) - Fit K-Means to data\")\n",
    "print(\"  ‚Ä¢ predict(X) - Assign clusters to new tokens\")\n",
    "print(\"  ‚Ä¢ fit_predict(X, feature_names) - Fit and predict in one step\")\n",
    "print(\"  ‚Ä¢ get_cluster_centers() - Get centroid characteristics\")\n",
    "print(\"  ‚Ä¢ get_cluster_summary(X) - Get cluster sizes\")\n",
    "print(\"  ‚Ä¢ plot_elbow(X, max_k) - Find optimal number of clusters\")\n",
    "print(\"  ‚Ä¢ plot_clusters_2d(X_2d, labels) - Visualize clusters in 2D\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828294af-50fc-42a5-9848-cea4dcf75acd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Finding the Optimal Number of Clusters\n",
    "\n",
    "### The Big Question: How Many Clusters?\n",
    "\n",
    "This is the **most important decision** in K-Means clustering:\n",
    "- **Too few clusters (k=2):** Oversimplification, miss important patterns\n",
    "- **Too many clusters (k=10):** Over-segmentation, clusters become meaningless\n",
    "- **Just right (k=3-5):** Clear groups, actionable insights\n",
    "\n",
    "**The problem:** K-Means requires you to choose k BEFORE running the algorithm!\n",
    "\n",
    "**The solution:** Use the **Elbow Method** and **Silhouette Analysis** to find the optimal k\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Method 1: The Elbow Method\n",
    "\n",
    "### How It Works\n",
    "\n",
    "We run K-Means multiple times with different values of k (2, 3, 4, ..., 10) and measure **inertia** each time.\n",
    "\n",
    "**Inertia** = Sum of squared distances from each point to its cluster center\n",
    "- **Lower inertia** = tighter, more compact clusters\n",
    "- **Higher inertia** = loose, spread-out clusters\n",
    "\n",
    "### The Pattern\n",
    "```\n",
    "Inertia\n",
    "  |\n",
    "  |  ‚óè                        k=2: High inertia (loose clusters)\n",
    "  |    ‚óè                      k=3: Much better!\n",
    "  |      ‚óè                    k=4: Still improving\n",
    "  |        ‚óè___‚óè___‚óè___‚óè      k=5+: Diminishing returns\n",
    "  |            ^\n",
    "  |            |\n",
    "  |         \"Elbow\"\n",
    "  |\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "     2   3   4   5   6   7   8   k\n",
    "```\n",
    "\n",
    "### Reading the Elbow Plot\n",
    "\n",
    "**Before the elbow:**\n",
    "- Steep decline in inertia\n",
    "- Each new cluster adds significant value\n",
    "- **Still room for improvement**\n",
    "\n",
    "**At the elbow:**\n",
    "- Curve starts to flatten\n",
    "- **Optimal trade-off** between simplicity and fit\n",
    "- **This is your optimal k!**\n",
    "\n",
    "**After the elbow:**\n",
    "- Slow, gradual decline\n",
    "- Diminishing returns (little benefit from more clusters)\n",
    "- **Adding clusters just creates complexity**\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Method 2: Silhouette Score\n",
    "\n",
    "### What It Measures\n",
    "\n",
    "The **silhouette score** tells you how well-separated your clusters are.\n",
    "\n",
    "**Formula (simplified):**\n",
    "```\n",
    "For each point:\n",
    "  a = average distance to points in same cluster (within-cluster)\n",
    "  b = average distance to points in nearest other cluster (between-cluster)\n",
    "  \n",
    "  silhouette = (b - a) / max(a, b)\n",
    "```\n",
    "\n",
    "### Interpreting Silhouette Scores\n",
    "\n",
    "| Score Range | Interpretation | Quality |\n",
    "|-------------|----------------|---------|\n",
    "| **0.7 to 1.0** | Strong separation | Excellent ‚≠ê‚≠ê‚≠ê |\n",
    "| **0.5 to 0.7** | Reasonable separation | Good ‚≠ê‚≠ê |\n",
    "| **0.25 to 0.5** | Weak separation | Acceptable ‚≠ê |\n",
    "| **< 0.25** | Poor separation | Bad ‚ùå |\n",
    "| **Negative** | Wrong clustering | Very Bad ‚ùå‚ùå |\n",
    "\n",
    "**Goal:** Find k with the **highest silhouette score**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Decision Process\n",
    "\n",
    "### Step 1: Run Both Methods\n",
    "\n",
    "We'll test k from 2 to 10 and create two plots:\n",
    "1. **Elbow plot** (inertia vs k)\n",
    "2. **Silhouette plot** (silhouette score vs k)\n",
    "\n",
    "### Step 2: Look for Agreement\n",
    "\n",
    "**Best case scenario:** Both methods agree!\n",
    "```\n",
    "Elbow at k=4\n",
    "Highest silhouette at k=4\n",
    "‚Üí Decision: Use k=4 ‚úÖ\n",
    "```\n",
    "\n",
    "**Common scenario:** Methods suggest different k\n",
    "```\n",
    "Elbow at k=4\n",
    "Highest silhouette at k=3\n",
    "‚Üí Decision: Try both, see which makes more economic sense\n",
    "```\n",
    "\n",
    "### Step 3: Apply Domain Knowledge\n",
    "\n",
    "**Consider your use case:**\n",
    "\n",
    "**For portfolio diversification:**\n",
    "- k=3: Conservative (Low/Medium/High risk)\n",
    "- k=4: Balanced (Small/Mid/Large cap + separate risk dimension)\n",
    "- k=5: Detailed (Multiple risk and size categories)\n",
    "\n",
    "**Rule of thumb for DeFi portfolios:**\n",
    "- **k=3:** Simple, easy to interpret\n",
    "- **k=4:** Most common, good balance\n",
    "- **k=5:** More nuanced, for larger portfolios (50+ tokens)\n",
    "- **k=6+:** Usually too many unless you have 100+ tokens\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Real-World Examples\n",
    "\n",
    "### Example 1: Clear Elbow\n",
    "```\n",
    "k=2: inertia=150\n",
    "k=3: inertia=80   ‚Üê Big drop!\n",
    "k=4: inertia=65   ‚Üê Still good\n",
    "k=5: inertia=58   ‚Üê Smaller drop\n",
    "k=6: inertia=54   ‚Üê Even smaller\n",
    "k=7: inertia=51   ‚Üê Minimal improvement\n",
    "\n",
    "Elbow: k=4 or k=5\n",
    "```\n",
    "\n",
    "### Example 2: No Clear Elbow\n",
    "```\n",
    "k=2: inertia=120\n",
    "k=3: inertia=95\n",
    "k=4: inertia=75\n",
    "k=5: inertia=60\n",
    "k=6: inertia=48\n",
    "k=7: inertia=38\n",
    "\n",
    "Gradual decline, no obvious elbow\n",
    "‚Üí Use silhouette score to decide\n",
    "```\n",
    "\n",
    "### Example 3: Silhouette Suggests Different k\n",
    "```\n",
    "Inertia elbow: k=5\n",
    "Silhouette peak: k=3 (score=0.62)\n",
    "\n",
    "Analysis:\n",
    "- k=3: Cleaner, more separated clusters\n",
    "- k=5: More detailed segmentation\n",
    "  \n",
    "Decision: Start with k=3 for interpretability,\n",
    "          try k=5 if you need finer granularity\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "### Pitfall 1: Forcing an Elbow Where None Exists\n",
    "‚ùå \"I see an elbow at k=7!\"  \n",
    "‚úÖ \"The decline is gradual; I'll use silhouette to decide\"\n",
    "\n",
    "### Pitfall 2: Ignoring Silhouette\n",
    "‚ùå \"Elbow says k=5, so k=5 it is\"  \n",
    "‚úÖ \"Elbow suggests k=5, but silhouette is 0.15 (poor). Let me check k=3\"\n",
    "\n",
    "### Pitfall 3: Over-clustering\n",
    "‚ùå \"More clusters = better understanding\"  \n",
    "‚úÖ \"More clusters = more complexity. Simpler is often better\"\n",
    "\n",
    "### Pitfall 4: Ignoring Economic Sense\n",
    "‚ùå \"Math says k=7, so that's what I'll use\"  \n",
    "‚úÖ \"k=7 gives me clusters I can't interpret. k=4 makes economic sense\"\n",
    "\n",
    "\n",
    "##  Decision Framework\n",
    "\n",
    "Use this framework to make your decision:\n",
    "```\n",
    "IF (elbow_k == silhouette_k):\n",
    "    optimal_k = elbow_k\n",
    "    confidence = \"High\"\n",
    "\n",
    "ELSE IF (silhouette_score[elbow_k] > 0.5):\n",
    "    optimal_k = elbow_k\n",
    "    confidence = \"Medium - elbow takes precedence\"\n",
    "\n",
    "ELSE IF (silhouette_score[silhouette_k] > 0.6):\n",
    "    optimal_k = silhouette_k\n",
    "    confidence = \"Medium - silhouette takes precedence\"\n",
    "\n",
    "ELSE:\n",
    "    # Try both and see which is more interpretable\n",
    "    optimal_k = [elbow_k, silhouette_k]\n",
    "    confidence = \"Low - need domain knowledge\"\n",
    "\n",
    "FINALLY:\n",
    "    IF (optimal_k > n_tokens / 8):\n",
    "        # Too many clusters relative to data\n",
    "        optimal_k = max(3, n_tokens / 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223698d-6dc1-440e-8202-0d3497551e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINDING OPTIMAL NUMBER OF CLUSTERS - ELBOW METHOD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüéØ Goal: Determine the best value of k (number of clusters)\")\n",
    "print(\"   We'll test k from 2 to 10 and measure two metrics:\")\n",
    "print(\"   1. Inertia (within-cluster distance) - look for 'elbow'\")\n",
    "print(\"   2. Silhouette score (cluster separation) - look for peak\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  This will take about 10-20 seconds...\")\n",
    "print(\"   (We're running K-Means 9 times with different k values)\\n\")\n",
    "\n",
    "# Create temporary K-Means object for testing\n",
    "kmeans_temp = SimpleKMeans(n_clusters=3)  # Placeholder, we'll test multiple k\n",
    "\n",
    "# Run elbow method\n",
    "fig = kmeans_temp.plot_elbow(defi_data.values, max_k=10, figsize=(14, 5))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYZING THE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Re-run the analysis to extract metrics for decision making\n",
    "X_scaled = kmeans_temp.scaler.fit_transform(defi_data.values)\n",
    "\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Find the elbow using the \"elbow detection\" heuristic\n",
    "# Calculate second derivative (rate of change of rate of change)\n",
    "inertia_diffs = np.diff(inertias)\n",
    "inertia_diffs2 = np.diff(inertia_diffs)\n",
    "elbow_k = np.argmax(inertia_diffs2) + 2  # +2 due to double diff\n",
    "\n",
    "# Find best silhouette\n",
    "best_silhouette_idx = np.argmax(silhouettes)\n",
    "best_silhouette_k = K_range[best_silhouette_idx]\n",
    "best_silhouette_score = silhouettes[best_silhouette_idx]\n",
    "\n",
    "print(\"\\nüìä ELBOW METHOD (Inertia):\")\n",
    "print(f\"   Detected elbow at: k={elbow_k}\")\n",
    "print(f\"   Inertia at k={elbow_k}: {inertias[elbow_k-2]:.2f}\")\n",
    "print(f\"\\n   Interpretation:\")\n",
    "print(f\"   ‚Ä¢ Adding clusters before k={elbow_k}: significant improvement\")\n",
    "print(f\"   ‚Ä¢ Adding clusters after k={elbow_k}: diminishing returns\")\n",
    "\n",
    "print(\"\\nüìà SILHOUETTE METHOD (Cluster Quality):\")\n",
    "print(f\"   Best silhouette score at: k={best_silhouette_k}\")\n",
    "print(f\"   Silhouette score: {best_silhouette_score:.3f}\")\n",
    "\n",
    "# Interpret silhouette score\n",
    "if best_silhouette_score > 0.7:\n",
    "    quality = \"Excellent ‚≠ê‚≠ê‚≠ê\"\n",
    "elif best_silhouette_score > 0.5:\n",
    "    quality = \"Good ‚≠ê‚≠ê\"\n",
    "elif best_silhouette_score > 0.25:\n",
    "    quality = \"Acceptable ‚≠ê\"\n",
    "else:\n",
    "    quality = \"Poor ‚ùå\"\n",
    "\n",
    "print(f\"   Quality rating: {quality}\")\n",
    "\n",
    "# Show silhouette scores for key k values\n",
    "print(f\"\\n   Silhouette scores for common k values:\")\n",
    "for k in [3, 4, 5]:\n",
    "    if k <= 10:\n",
    "        score = silhouettes[k-2]\n",
    "        print(f\"   ‚Ä¢ k={k}: {score:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DECISION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Make recommendation\n",
    "print(f\"\\nü§î Comparing methods:\")\n",
    "print(f\"   Elbow method suggests:     k={elbow_k}\")\n",
    "print(f\"   Silhouette method suggests: k={best_silhouette_k}\")\n",
    "\n",
    "if elbow_k == best_silhouette_k:\n",
    "    recommended_k = elbow_k\n",
    "    confidence = \"HIGH\"\n",
    "    reason = \"Both methods agree\"\n",
    "    print(f\"\\n‚úÖ STRONG AGREEMENT: Both methods suggest k={recommended_k}\")\n",
    "elif abs(elbow_k - best_silhouette_k) <= 1:\n",
    "    recommended_k = best_silhouette_k  # Prefer silhouette if close\n",
    "    confidence = \"MEDIUM-HIGH\"\n",
    "    reason = \"Methods suggest similar values, choosing based on silhouette\"\n",
    "    print(f\"\\n‚úÖ REASONABLE AGREEMENT: Methods suggest k={elbow_k} and k={best_silhouette_k}\")\n",
    "    print(f\"   Recommending k={recommended_k} (better cluster quality)\")\n",
    "else:\n",
    "    # Check silhouette at elbow_k\n",
    "    elbow_silhouette = silhouettes[elbow_k-2]\n",
    "    if elbow_silhouette > 0.5:\n",
    "        recommended_k = elbow_k\n",
    "        confidence = \"MEDIUM\"\n",
    "        reason = \"Elbow point with acceptable silhouette score\"\n",
    "    else:\n",
    "        recommended_k = best_silhouette_k\n",
    "        confidence = \"MEDIUM\"\n",
    "        reason = \"Prioritizing cluster quality (silhouette)\"\n",
    "    print(f\"\\n‚ö†Ô∏è  DISAGREEMENT: Elbow at k={elbow_k}, best silhouette at k={best_silhouette_k}\")\n",
    "    print(f\"   Recommending k={recommended_k}\")\n",
    "\n",
    "# Additional validation\n",
    "print(f\"\\nüìã RECOMMENDATION SUMMARY:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "recommendation_table = pd.DataFrame({\n",
    "    'Metric': ['Recommended k', 'Confidence Level', 'Reasoning', \n",
    "               'Inertia', 'Silhouette Score', 'Expected Quality'],\n",
    "    'Value': [\n",
    "        recommended_k,\n",
    "        confidence,\n",
    "        reason,\n",
    "        f\"{inertias[recommended_k-2]:.2f}\",\n",
    "        f\"{silhouettes[recommended_k-2]:.3f}\",\n",
    "        quality if recommended_k == best_silhouette_k else \"Good\" if silhouettes[recommended_k-2] > 0.5 else \"Acceptable\"\n",
    "    ]\n",
    "})\n",
    "display(recommendation_table)\n",
    "\n",
    "print(\"\\nüí° Practical Interpretation:\")\n",
    "if recommended_k == 3:\n",
    "    print(\"   k=3: Simple segmentation (e.g., Low/Medium/High risk)\")\n",
    "    print(\"   ‚Ä¢ Easy to interpret and communicate\")\n",
    "    print(\"   ‚Ä¢ Good for basic portfolio diversification\")\n",
    "elif recommended_k == 4:\n",
    "    print(\"   k=4: Balanced segmentation\")\n",
    "    print(\"   ‚Ä¢ Captures multiple dimensions (size + risk)\")\n",
    "    print(\"   ‚Ä¢ Most common choice for DeFi portfolios\")\n",
    "elif recommended_k == 5:\n",
    "    print(\"   k=5: Detailed segmentation\")\n",
    "    print(\"   ‚Ä¢ Nuanced categorization\")\n",
    "    print(\"   ‚Ä¢ Better for larger portfolios (50+ tokens)\")\n",
    "else:\n",
    "    print(f\"   k={recommended_k}: Custom segmentation\")\n",
    "    print(\"   ‚Ä¢ Specific to your data structure\")\n",
    "\n",
    "# Compare with our artificial groups\n",
    "print(\"\\nüî¨ Validation Note:\")\n",
    "print(f\"   Remember: We artificially created 3 distinct token groups\")\n",
    "print(f\"   Recommended k={recommended_k}\")\n",
    "if recommended_k == 3:\n",
    "    print(\"   ‚úÖ Perfect! K-Means detected our 3 groups!\")\n",
    "elif recommended_k == 4:\n",
    "    print(\"   ‚ö†Ô∏è  K-Means suggests 4 clusters (one more than we created)\")\n",
    "    print(\"   ‚Üí Likely splitting one group or finding subtle patterns\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  K-Means suggests {recommended_k} clusters (different from 3)\")\n",
    "    print(\"   ‚Üí Algorithm may be detecting additional structure or noise\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL DECISION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüéØ We will use k={recommended_k} clusters for our analysis\")\n",
    "print(f\"   Confidence: {confidence}\")\n",
    "print(f\"   Silhouette score: {silhouettes[recommended_k-2]:.3f}\")\n",
    "\n",
    "# Store the decision\n",
    "optimal_k = recommended_k\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"‚úÖ Elbow analysis complete, Optimal k={optimal_k}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067c649-37cf-4155-8737-5d2b11d637da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Fitting K-Means and Interpreting Clusters\n",
    "\n",
    "### What Happens Next?\n",
    "\n",
    "Now that we've determined the optimal number of clusters (k), we'll:\n",
    "1. **Fit K-Means** with our chosen k\n",
    "2. **Assign each token** to a cluster\n",
    "3. **Analyze cluster characteristics** (what makes each cluster unique?)\n",
    "4. **Name the clusters** based on their profiles\n",
    "5. **Validate** that clusters make economic sense\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ The K-Means Fitting Process\n",
    "\n",
    "### Step-by-Step Breakdown\n",
    "```\n",
    "Input: 50 tokens √ó 8 features\n",
    "       k = optimal number of clusters\n",
    "\n",
    "Step 1: STANDARDIZATION\n",
    "‚îú‚îÄ Scale all features to mean=0, std=1\n",
    "‚îú‚îÄ Why? Prevent large values from dominating\n",
    "‚îî‚îÄ Result: Fair comparison across all features\n",
    "\n",
    "Step 2: INITIALIZATION\n",
    "‚îú‚îÄ Randomly place k cluster centers\n",
    "‚îú‚îÄ Use k-means++ for smart initialization\n",
    "‚îî‚îÄ Result: Good starting positions\n",
    "\n",
    "Step 3: ASSIGNMENT\n",
    "‚îú‚îÄ Calculate distance from each token to each center\n",
    "‚îú‚îÄ Assign token to nearest center\n",
    "‚îî‚îÄ Result: Each token belongs to one cluster\n",
    "\n",
    "Step 4: UPDATE\n",
    "‚îú‚îÄ Move each center to mean of its assigned tokens\n",
    "‚îú‚îÄ Recalculate optimal center positions\n",
    "‚îî‚îÄ Result: Better cluster centers\n",
    "\n",
    "Step 5: ITERATION\n",
    "‚îú‚îÄ Repeat steps 3-4 until convergence\n",
    "‚îú‚îÄ Convergence = centers stop moving\n",
    "‚îî‚îÄ Result: Final optimal clustering\n",
    "\n",
    "Output: 50 tokens with cluster labels (0, 1, 2, ...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Understanding Cluster Assignments\n",
    "\n",
    "### What the Labels Mean\n",
    "\n",
    "After fitting, each token gets a **cluster label**:\n",
    "```\n",
    "TOKEN_1  ‚Üí Cluster 0\n",
    "TOKEN_2  ‚Üí Cluster 2\n",
    "TOKEN_3  ‚Üí Cluster 1\n",
    "TOKEN_4  ‚Üí Cluster 0\n",
    "...\n",
    "```\n",
    "\n",
    "**Important notes:**\n",
    "- **Labels are arbitrary** (0, 1, 2, ... have no inherent meaning)\n",
    "- **Order doesn't matter** (Cluster 0 isn't \"better\" than Cluster 1)\n",
    "- **Labels can change** if you re-run K-Means with different random_state\n",
    "- **Interpretation matters** (what makes each cluster unique?)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Analyzing Cluster Centroids\n",
    "\n",
    "### What is a Centroid?\n",
    "\n",
    "The **centroid** is the \"center\" or \"average token\" of each cluster.\n",
    "\n",
    "**It's NOT a real token** - it's the mathematical average of all tokens in that cluster.\n",
    "\n",
    "### Example Centroid Analysis\n",
    "```\n",
    "Cluster 0 Centroid:\n",
    "  Mean_Return:      0.002  (0.2% daily)\n",
    "  Volatility:       0.025  (2.5% daily)\n",
    "  Volume_USD:       5.0M\n",
    "  TVL_USD:          50M\n",
    "  Market_Cap:       200M\n",
    "  Liquidity_Score:  85\n",
    "  ETH_Correlation:  0.75\n",
    "  Sentiment_Score:  65\n",
    "\n",
    "Interpretation: \"High-cap stable tokens\"\n",
    "- Low volatility (2.5%)\n",
    "- High liquidity (score 85)\n",
    "- Large size (200M market cap)\n",
    "- Strong ETH correlation (0.75)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üé® Cluster Interpretation Strategy\n",
    "\n",
    "### Step 1: Compare Centroids\n",
    "\n",
    "Look at centroid values **relative to each other**:\n",
    "```\n",
    "                Cluster 0   Cluster 1   Cluster 2\n",
    "Volatility:        2.5%        4.0%        8.0%  ‚Üê Cluster 2 is riskiest!\n",
    "Market_Cap:        200M        80M         20M   ‚Üê Cluster 0 is largest!\n",
    "Liquidity:         85          70          45    ‚Üê Cluster 0 most liquid!\n",
    "```\n",
    "\n",
    "### Step 2: Find Distinguishing Features\n",
    "\n",
    "Which features **differ most** between clusters?\n",
    "\n",
    "**High variance across clusters** = important distinguishing feature\n",
    "**Low variance across clusters** = not useful for differentiation\n",
    "\n",
    "### Step 3: Name Each Cluster\n",
    "\n",
    "Give meaningful, descriptive names\n",
    "\n",
    "## üìã Cluster Profile Template\n",
    "\n",
    "For each cluster, document:\n",
    "```\n",
    "CLUSTER 0: [Your Name Here]\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Size: 15 tokens (30% of portfolio)\n",
    "\n",
    "Key Characteristics:\n",
    "  ‚Ä¢ Volatility: LOW (2.5%)\n",
    "  ‚Ä¢ Market Cap: HIGH ($200M avg)\n",
    "  ‚Ä¢ Liquidity: HIGH (score 85)\n",
    "  ‚Ä¢ Returns: MODERATE (0.2% daily)\n",
    "\n",
    "Interpretation:\n",
    "  Large, established protocols with stable performance.\n",
    "  Lower risk, lower return profile. Good for conservative allocation.\n",
    "\n",
    "Sample Tokens:\n",
    "  TOKEN_1, TOKEN_5, TOKEN_8, ...\n",
    "\n",
    "Portfolio Strategy:\n",
    "  Allocate 40-50% of portfolio for stability and liquidity.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Common DeFi Cluster Patterns\n",
    "\n",
    "### Pattern 1: Size-Based Clustering\n",
    "```\n",
    "Cluster 0: Large-cap (Uniswap, Aave, Compound)\n",
    "Cluster 1: Mid-cap (newer established protocols)\n",
    "Cluster 2: Small-cap (early-stage projects)\n",
    "```\n",
    "\n",
    "**Distinguishing features:** Market Cap, TVL, Volume, Liquidity\n",
    "\n",
    "### Pattern 2: Risk-Based Clustering\n",
    "```\n",
    "Cluster 0: Conservative (low volatility, stable returns)\n",
    "Cluster 1: Balanced (moderate volatility and returns)\n",
    "Cluster 2: Aggressive (high volatility, high potential returns)\n",
    "```\n",
    "\n",
    "**Distinguishing features:** Volatility, Returns, Sentiment\n",
    "\n",
    "### Pattern 3: Market Exposure Clustering\n",
    "```\n",
    "Cluster 0: ETH-correlated (follow Ethereum closely)\n",
    "Cluster 1: Independent (move independently)\n",
    "Cluster 2: Counter-cyclical (negative correlation)\n",
    "```\n",
    "\n",
    "**Distinguishing features:** ETH_Correlation, Sentiment, Returns\n",
    "\n",
    "### Pattern 4: Hybrid Clustering (Most Common)\n",
    "```\n",
    "Cluster 0: Large stable (high cap, low vol)\n",
    "Cluster 1: Small aggressive (low cap, high vol)\n",
    "Cluster 2: Medium balanced (mid cap, mid vol)\n",
    "Cluster 3: High-sentiment plays (driven by community)\n",
    "```\n",
    "\n",
    "**Distinguishing features:** Combination of size, risk, and sentiment\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Cluster Validation Checklist\n",
    "\n",
    "After analyzing clusters, verify:\n",
    "\n",
    "### ‚úÖ Size Balance\n",
    "- [ ] No cluster has > 70% of tokens\n",
    "- [ ] No cluster has < 10% of tokens\n",
    "- [ ] Relatively balanced distribution\n",
    "\n",
    "### ‚úÖ Clear Separation\n",
    "- [ ] Centroids are clearly different\n",
    "- [ ] Can identify 2-3 key distinguishing features per cluster\n",
    "- [ ] Silhouette score > 0.25\n",
    "\n",
    "### ‚úÖ Economic Interpretability\n",
    "- [ ] Each cluster has a meaningful name\n",
    "- [ ] Clusters match known market categories\n",
    "- [ ] Can explain clusters to non-technical stakeholders\n",
    "\n",
    "### ‚úÖ Actionable Insights\n",
    "- [ ] Can use clusters for portfolio decisions\n",
    "- [ ] Understand risk profile of each cluster\n",
    "- [ ] Know which clusters to over/underweight\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Using Clusters for Portfolio Decisions\n",
    "\n",
    "### Strategy 1: Equal Weight Across Clusters\n",
    "```\n",
    "Select 2 tokens from each cluster\n",
    "‚Üí Ensures diversification across all market segments\n",
    "```\n",
    "\n",
    "### Strategy 2: Risk-Adjusted Allocation\n",
    "```\n",
    "Conservative portfolio:\n",
    "  ‚Ä¢ 50% from low-risk cluster\n",
    "  ‚Ä¢ 30% from medium-risk cluster\n",
    "  ‚Ä¢ 20% from high-risk cluster\n",
    "\n",
    "Aggressive portfolio:\n",
    "  ‚Ä¢ 20% from low-risk cluster\n",
    "  ‚Ä¢ 30% from medium-risk cluster\n",
    "  ‚Ä¢ 50% from high-risk cluster\n",
    "```\n",
    "\n",
    "### Strategy 3: Cluster Rotation\n",
    "```\n",
    "Bull market: Overweight high-risk cluster\n",
    "Bear market: Overweight low-risk cluster\n",
    "Sideways: Equal weight across all clusters\n",
    "```\n",
    "\n",
    "### Strategy 4: Within-Cluster Selection\n",
    "```\n",
    "For each cluster:\n",
    "  1. Calculate Sharpe ratio (Return/Volatility)\n",
    "  2. Select top 2 tokens by Sharpe ratio\n",
    "  3. Ensures best risk-adjusted returns within each segment\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Interpretation Mistakes\n",
    "\n",
    "### Mistake 1: Treating Labels as Rankings\n",
    "\n",
    "‚ùå \"Cluster 0 is better than Cluster 2\"\n",
    "‚úÖ \"Cluster 0 represents different characteristics than Cluster 2\"\n",
    "\n",
    "### Mistake 2: Ignoring Cluster Sizes\n",
    "\n",
    "‚ùå \"We have 4 equal clusters\"\n",
    "‚úÖ \"Cluster 0 has 25 tokens, Cluster 3 has only 5 tokens\"\n",
    "\n",
    "### Mistake 3: Over-interpreting Small Differences\n",
    "\n",
    "‚ùå \"Cluster 0 volatility: 2.5%, Cluster 1: 2.7% - totally different!\"\n",
    "‚úÖ \"Volatility is similar across these clusters; focus on other features\"\n",
    "\n",
    "### Mistake 4: Forcing Interpretations\n",
    "\n",
    "‚ùå \"This cluster MUST represent something specific\"\n",
    "‚úÖ \"This cluster captures residual variation; it's okay if interpretation is unclear\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275754d7-955b-436a-bae2-6197ad1b6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(f\"FITTING K-MEANS WITH OPTIMAL k={optimal_k}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüîß Initializing K-Means with k={optimal_k} clusters...\")\n",
    "print(\"   This will:\")\n",
    "print(\"   1. Standardize all features\")\n",
    "print(\"   2. Run K-Means algorithm (may take a few seconds)\")\n",
    "print(\"   3. Assign each token to a cluster\")\n",
    "print(\"   4. Calculate cluster centroids\\n\")\n",
    "\n",
    "# Initialize K-Means with optimal k\n",
    "kmeans_model = SimpleKMeans(n_clusters=optimal_k, random_state=42)\n",
    "\n",
    "# Fit K-Means and get cluster assignments\n",
    "clusters = kmeans_model.fit_predict(\n",
    "    defi_data.values,\n",
    "    feature_names=defi_data.columns.tolist()\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLUSTER ASSIGNMENT COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Add cluster labels to our data\n",
    "defi_data_clustered = defi_data.copy()\n",
    "defi_data_clustered['Cluster'] = clusters\n",
    "\n",
    "print(f\"\\n‚úÖ All {len(defi_data)} tokens assigned to {optimal_k} clusters\")\n",
    "\n",
    "# Get cluster summary\n",
    "cluster_summary = kmeans_model.get_cluster_summary(defi_data.values)\n",
    "\n",
    "print(\"\\nüìä CLUSTER DISTRIBUTION:\")\n",
    "print(\"=\" * 70)\n",
    "display(cluster_summary)\n",
    "\n",
    "# Visualize distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(cluster_summary['Cluster'], cluster_summary['Size'], \n",
    "              alpha=0.7, color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, (idx, row) in enumerate(cluster_summary.iterrows()):\n",
    "    ax.text(row['Cluster'], row['Size'] + 0.5, row['Percentage'], \n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Cluster', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Number of Tokens', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Cluster Size Distribution (k={optimal_k})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_xticks(cluster_summary['Cluster'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for imbalanced clusters\n",
    "print(\"\\nüîç Balance Analysis:\")\n",
    "max_size = cluster_summary['Size'].max()\n",
    "min_size = cluster_summary['Size'].min()\n",
    "size_ratio = max_size / min_size\n",
    "\n",
    "if size_ratio > 5:\n",
    "    print(f\"   ‚ö†Ô∏è  IMBALANCED: Largest cluster is {size_ratio:.1f}x bigger than smallest\")\n",
    "    print(f\"   ‚Üí Consider adjusting k or checking for outliers\")\n",
    "elif size_ratio > 2.5:\n",
    "    print(f\"   ‚ö†Ô∏è  SOMEWHAT IMBALANCED: Size ratio is {size_ratio:.1f}x\")\n",
    "    print(f\"   ‚Üí Acceptable, but one cluster dominates\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ WELL BALANCED: Size ratio is {size_ratio:.1f}x\")\n",
    "    print(f\"   ‚Üí Good distribution across clusters\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLUSTER CHARACTERISTICS (CENTROIDS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get cluster centroids (average values)\n",
    "centroids = kmeans_model.get_cluster_centers()\n",
    "\n",
    "print(\"\\nüìã Centroid Table:\")\n",
    "print(\"   (Average feature values for each cluster)\")\n",
    "print()\n",
    "display(centroids.round(4))\n",
    "\n",
    "print(\"\\nüí° How to read this table:\")\n",
    "print(\"   ‚Ä¢ Each row = a cluster's 'average token'\")\n",
    "print(\"   ‚Ä¢ Each column = average value of that feature in the cluster\")\n",
    "print(\"   ‚Ä¢ Compare rows to see what distinguishes clusters\")\n",
    "\n",
    "# Analyze feature variance across clusters\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"IDENTIFYING DISTINGUISHING FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate coefficient of variation (std/mean) for each feature across clusters\n",
    "feature_cv = centroids.std() / (centroids.mean().abs() + 1e-10)  # Avoid division by zero\n",
    "distinguishing_features = feature_cv.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nüéØ Most distinguishing features (high variance across clusters):\")\n",
    "print(\"   These features differ the most between clusters:\\n\")\n",
    "\n",
    "for i, (feature, cv) in enumerate(distinguishing_features.head(5).items(), 1):\n",
    "    print(f\"   {i}. {feature:20s} (variation: {cv:.2f})\")\n",
    "    \n",
    "    # Show range across clusters\n",
    "    min_val = centroids[feature].min()\n",
    "    max_val = centroids[feature].max()\n",
    "    print(f\"      Range: {min_val:.4f} to {max_val:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° These features are most useful for interpreting cluster differences!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED CLUSTER ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze each cluster in detail\n",
    "for i in range(optimal_k):\n",
    "    cluster_data = defi_data_clustered[defi_data_clustered['Cluster'] == i]\n",
    "    cluster_size = len(cluster_data)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CLUSTER {i} - {cluster_size} tokens ({cluster_size/len(defi_data)*100:.1f}%)\")\n",
    "    print('='*70)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    cluster_features = cluster_data.drop('Cluster', axis=1)\n",
    "    avg = cluster_features.mean()\n",
    "    std = cluster_features.std()\n",
    "    \n",
    "    print(\"\\nüìä Average Characteristics:\")\n",
    "    for feature in defi_data.columns:\n",
    "        # Compare to overall average\n",
    "        overall_avg = defi_data[feature].mean()\n",
    "        diff_pct = ((avg[feature] - overall_avg) / overall_avg * 100) if overall_avg != 0 else 0\n",
    "        \n",
    "        # Determine if HIGH, LOW, or AVERAGE\n",
    "        if abs(diff_pct) < 20:\n",
    "            level = \"AVERAGE\"\n",
    "            emoji = \"‚û°Ô∏è\"\n",
    "        elif diff_pct > 0:\n",
    "            level = \"HIGH\" if diff_pct > 50 else \"ABOVE AVG\"\n",
    "            emoji = \"‚¨ÜÔ∏è\"\n",
    "        else:\n",
    "            level = \"LOW\" if diff_pct < -50 else \"BELOW AVG\"\n",
    "            emoji = \"‚¨áÔ∏è\"\n",
    "        \n",
    "        print(f\"   {emoji} {feature:20s}: {avg[feature]:>10.4f} ({level})\")\n",
    "    \n",
    "    # Suggest interpretation based on patterns\n",
    "    print(\"\\nüí≠ Interpretation:\")\n",
    "    \n",
    "    # Check key patterns\n",
    "    high_volatility = avg['Volatility'] > defi_data['Volatility'].mean() * 1.3\n",
    "    low_volatility = avg['Volatility'] < defi_data['Volatility'].mean() * 0.7\n",
    "    high_marketcap = avg['Market_Cap'] > defi_data['Market_Cap'].mean() * 1.3\n",
    "    low_marketcap = avg['Market_Cap'] < defi_data['Market_Cap'].mean() * 0.7\n",
    "    high_liquidity = avg['Liquidity_Score'] > defi_data['Liquidity_Score'].mean() * 1.2\n",
    "    \n",
    "    interpretations = []\n",
    "    \n",
    "    if high_marketcap and high_liquidity and low_volatility:\n",
    "        interpretations.append(\"üèõÔ∏è LARGE-CAP STABLE: Established, liquid, low-risk tokens\")\n",
    "    elif low_marketcap and high_volatility:\n",
    "        interpretations.append(\"üöÄ SMALL-CAP HIGH-RISK: High potential, high volatility\")\n",
    "    elif not high_marketcap and not low_marketcap and not high_volatility:\n",
    "        interpretations.append(\"‚öñÔ∏è MID-CAP BALANCED: Moderate size and risk profile\")\n",
    "    \n",
    "    if avg['ETH_Correlation'] > 0.7:\n",
    "        interpretations.append(\"üìä HIGH ETH CORRELATION: Follows Ethereum closely\")\n",
    "    elif avg['ETH_Correlation'] < 0.3:\n",
    "        interpretations.append(\"üîÑ LOW ETH CORRELATION: Independent price movement\")\n",
    "    \n",
    "    if avg['Mean_Return'] > defi_data['Mean_Return'].mean() * 1.5:\n",
    "        interpretations.append(\"üìà HIGH RETURNS: Outperforming average\")\n",
    "    \n",
    "    if interpretations:\n",
    "        for interp in interpretations:\n",
    "            print(f\"   ‚Ä¢ {interp}\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ MIXED PROFILE: Combination of various characteristics\")\n",
    "    \n",
    "    # Show sample tokens\n",
    "    print(f\"\\nüìã Sample Tokens (for exammple the first 8, or even less):\")\n",
    "    sample_tokens = cluster_data.index[:8].tolist()\n",
    "    print(f\"   {', '.join(sample_tokens)}\")\n",
    "    if len(cluster_data) > 8:\n",
    "        print(f\"   ... and {len(cluster_data) - 8} more\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLUSTER NAMING EXERCISE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n Based on the analysis above, suggest names for each cluster:\")\n",
    "print()\n",
    "\n",
    "# Create summary table for naming\n",
    "naming_summary = []\n",
    "for i in range(optimal_k):\n",
    "    cluster_data = defi_data_clustered[defi_data_clustered['Cluster'] == i]\n",
    "    cluster_features = cluster_data.drop('Cluster', axis=1)\n",
    "    avg = cluster_features.mean()\n",
    "    \n",
    "    # Create a quick profile\n",
    "    vol_level = \"High\" if avg['Volatility'] > defi_data['Volatility'].mean() * 1.2 else \"Low\" if avg['Volatility'] < defi_data['Volatility'].mean() * 0.8 else \"Med\"\n",
    "    cap_level = \"Large\" if avg['Market_Cap'] > defi_data['Market_Cap'].mean() * 1.2 else \"Small\" if avg['Market_Cap'] < defi_data['Market_Cap'].mean() * 0.8 else \"Mid\"\n",
    "    liq_level = \"High\" if avg['Liquidity_Score'] > defi_data['Liquidity_Score'].mean() * 1.1 else \"Low\"\n",
    "    \n",
    "    naming_summary.append({\n",
    "        'Cluster': i,\n",
    "        'Size': len(cluster_data),\n",
    "        'Cap': cap_level,\n",
    "        'Volatility': vol_level,\n",
    "        'Liquidity': liq_level,\n",
    "        'Your Name': '_______________'\n",
    "    })\n",
    "\n",
    "naming_df = pd.DataFrame(naming_summary)\n",
    "display(naming_df)\n",
    "\n",
    "print(\"\\nüí° Naming suggestions:\")\n",
    "print(\"   ‚Ä¢ Focus on 2-3 distinguishing characteristics\")\n",
    "print(\"   ‚Ä¢ Use descriptive terms: Large/Mid/Small, Stable/Volatile, Liquid/Illiquid\")\n",
    "print(\"   ‚Ä¢ Think about portfolio role: Conservative, Growth, Speculative\")\n",
    "print(\"   ‚Ä¢ Examples: 'DeFi Blue Chips', 'High-Risk Growth', 'Stable Giants'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Cluster analysis complete!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüéØ KEY TAKEAWAYS:\")\n",
    "print(f\"   ‚Ä¢ {len(defi_data)} tokens organized into {optimal_k} meaningful clusters\")\n",
    "print(\"   ‚Ä¢ Each cluster has distinct risk/return characteristics\")\n",
    "print(\"   ‚Ä¢ Clusters can guide portfolio construction and diversification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08c710-ec30-49f7-8635-1a4465d3563e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Visualizing Clusters in 2D Space\n",
    "\n",
    "### The Visualization Challenge\n",
    "\n",
    "We have a **problem**: Our tokens exist in **8-dimensional space** (8 features), but:\n",
    "- We can only visualize **2D or 3D**\n",
    "- We need to see clusters to validate they make sense\n",
    "- We want to show cluster separation visually\n",
    "\n",
    "**Solution:** Use **dimensionality reduction** to project into 2D!\n",
    "\n",
    "---\n",
    "\n",
    "## üé® Why Visualize Clusters?\n",
    "\n",
    "### Visual validation helps us:\n",
    "\n",
    "**1. Verify Cluster Separation**\n",
    "```\n",
    "Good clustering:           Bad clustering:\n",
    "  ‚óè‚óè‚óè        ‚óè‚óè‚óè              ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
    "  ‚óè‚óè‚óè        ‚óè‚óè‚óè              ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
    "               ‚óè‚óè‚óè            ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
    "  ‚óè‚óè‚óè        ‚óè‚óè‚óè              (all mixed)\n",
    "Clear groups!             \n",
    "```\n",
    "\n",
    "**2. Identify Outliers**\n",
    "- Tokens far from any cluster center\n",
    "- May be unique opportunities or data errors\n",
    "\n",
    "**3. Detect Overlaps**\n",
    "- Where clusters blend together\n",
    "- May indicate we need different k\n",
    "\n",
    "**4. Communicate Results**\n",
    "- Show stakeholders cluster structure\n",
    "- Easier than explaining 8D centroids!\n",
    "\n",
    "**5. Build Intuition**\n",
    "- See which tokens are similar\n",
    "- Understand cluster boundaries\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Method: PCA for 2D Projection\n",
    "\n",
    "### Why PCA?\n",
    "\n",
    "We already learned PCA reduces dimensions while keeping information. Perfect for visualization!\n",
    "\n",
    "**Process:**\n",
    "```\n",
    "8D Data (original features)\n",
    "    ‚Üì\n",
    "  PCA (keep 2 components)\n",
    "    ‚Üì\n",
    "2D Data (PC1 and PC2)\n",
    "    ‚Üì\n",
    "Plot with cluster colors\n",
    "```\n",
    "\n",
    "**What we're plotting:**\n",
    "- **X-axis:** PC1 (usually \"size\" factor)\n",
    "- **Y-axis:** PC2 (usually \"risk\" factor)\n",
    "- **Colors:** Different color per cluster\n",
    "- **Points:** Each token\n",
    "\n",
    "---\n",
    "\n",
    "##  Understanding the 2D Plot\n",
    "\n",
    "### Components of the Visualization\n",
    "\n",
    "**1. Axes**\n",
    "```\n",
    "      PC2 (Risk Factor)\n",
    "       ‚Üë\n",
    "       |   ‚óè Cluster 0\n",
    "       |       ‚óè Cluster 1\n",
    "       |   ‚óè       ‚óè\n",
    "       |       ‚óè ‚óè\n",
    "       |   ‚óè   ‚óè\n",
    "       |‚óè ‚óè     ‚óè Cluster 2\n",
    "       |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí PC1 (Size Factor)\n",
    "```\n",
    "\n",
    "**2. Point Colors**\n",
    "- Each cluster gets a distinct color\n",
    "- Same-color points belong to same cluster\n",
    "- Helps visualize group membership\n",
    "\n",
    "**3. Point Positions**\n",
    "- **Top-right:** High PC1, High PC2 (large & risky)\n",
    "- **Top-left:** Low PC1, High PC2 (small & risky)\n",
    "- **Bottom-right:** High PC1, Low PC2 (large & stable)\n",
    "- **Bottom-left:** Low PC1, Low PC2 (small & stable)\n",
    "\n",
    "**4. Cluster Boundaries**\n",
    "- Natural groupings visible\n",
    "- Some overlap is normal\n",
    "- Clear separation = good clustering\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What Makes a Good Cluster Visualization?\n",
    "\n",
    "### ‚úÖ Indicators of Good Clustering\n",
    "\n",
    "**1. Visual Separation**\n",
    "```\n",
    "‚úÖ Good: Clear space between cluster groups\n",
    "   ‚óè‚óè‚óè        ‚óè‚óè‚óè        ‚óè‚óè‚óè\n",
    "   ‚óè‚óè‚óè        ‚óè‚óè‚óè        ‚óè‚óè‚óè\n",
    "   \n",
    "‚ùå Bad: Clusters completely overlap\n",
    "   ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
    "   ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
    "```\n",
    "\n",
    "**2. Reasonable Compactness**\n",
    "```\n",
    "‚úÖ Good: Points cluster around centers\n",
    "   ‚óè‚óè‚óè\n",
    "   ‚óè‚óè‚óè  (tight group)\n",
    "   ‚óè‚óè‚óè\n",
    "   \n",
    "‚ùå Bad: Points spread everywhere\n",
    "   ‚óè\n",
    "      ‚óè    ‚óè\n",
    "         ‚óè     ‚óè\n",
    "   ‚óè        ‚óè\n",
    "```\n",
    "\n",
    "**3. Balanced Distribution**\n",
    "```\n",
    "‚úÖ Good: Clusters have similar densities\n",
    "   ‚óè‚óè‚óè  ‚óè‚óè‚óè  ‚óè‚óè‚óè\n",
    "   ‚óè‚óè‚óè  ‚óè‚óè‚óè  ‚óè‚óè‚óè\n",
    "   \n",
    "‚ùå Bad: One huge cluster, others tiny\n",
    "   ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
    "   ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè  ‚óè  ‚óè\n",
    "```\n",
    "\n",
    "**4. Few Outliers**\n",
    "```\n",
    "‚úÖ Good: Most points clearly belong\n",
    "   ‚óè‚óè‚óè\n",
    "   ‚óè‚óè‚óè  ‚óè‚óè‚óè\n",
    "        ‚óè‚óè‚óè\n",
    "   \n",
    "‚ö†Ô∏è Concerning: Many points between clusters\n",
    "   ‚óè‚óè‚óè\n",
    "   ‚óè‚óè‚óè ‚óè ‚óè ‚óè‚óè‚óè\n",
    "       ‚óè ‚óè  ‚óè‚óè‚óè\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Common Visualization Patterns\n",
    "\n",
    "### Pattern 1: Well-Separated Clusters\n",
    "```\n",
    "Plot appearance:\n",
    "   |\n",
    "   |  ‚óè‚óè‚óè (Cluster 0)\n",
    "   |\n",
    "   |          ‚óè‚óè‚óè (Cluster 1)\n",
    "   |\n",
    "   |    ‚óè‚óè‚óè (Cluster 2)\n",
    "   |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   \n",
    "Interpretation:\n",
    "- Clusters are clearly distinct\n",
    "- K-Means worked well\n",
    "- High confidence in assignments\n",
    "```\n",
    "\n",
    "### Pattern 2: Partial Overlap\n",
    "```\n",
    "Plot appearance:\n",
    "   |\n",
    "   |  ‚óè‚óè‚óè‚óè‚óè‚óè (Cluster 0 & 1 overlap)\n",
    "   |    ‚óè‚óè‚óè\n",
    "   |\n",
    "   |         ‚óè‚óè‚óè (Cluster 2 separate)\n",
    "   |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   \n",
    "Interpretation:\n",
    "- Some clusters blend together\n",
    "- May share characteristics\n",
    "- Consider merging overlapping clusters\n",
    "```\n",
    "\n",
    "### Pattern 3: One Dominant Cluster\n",
    "```\n",
    "Plot appearance:\n",
    "   |\n",
    "   |  ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
    "   |  ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè  ‚óè ‚óè\n",
    "   |  ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
    "   |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   \n",
    "Interpretation:\n",
    "- One cluster dominates\n",
    "- Others are outliers/niche groups\n",
    "- May need different k\n",
    "```\n",
    "\n",
    "### Pattern 4: Linear Separation\n",
    "```\n",
    "Plot appearance:\n",
    "   |\n",
    "   |  ‚óè‚óè‚óè  ‚óè‚óè‚óè  ‚óè‚óè‚óè\n",
    "   |  ‚óè‚óè‚óè  ‚óè‚óè‚óè  ‚óè‚óè‚óè\n",
    "   |  ‚óè‚óè‚óè  ‚óè‚óè‚óè  ‚óè‚óè‚óè\n",
    "   |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   \n",
    "Interpretation:\n",
    "- Clusters align along PC1\n",
    "- Main differentiator is size/liquidity\n",
    "- Risk (PC2) less important\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç How to Interpret Your Plot\n",
    "\n",
    "### Step 1: Overall Structure\n",
    "\n",
    "**Some questions:**\n",
    "1. Can we visually identify the clusters?\n",
    "2. Are they reasonably separated?\n",
    "3. How much do they overlap?\n",
    "\n",
    "### Step 2: Cluster Positions\n",
    "\n",
    "**Quadrant analysis:**\n",
    "```\n",
    "      High Risk (PC2)\n",
    "           ‚Üë\n",
    "  Small+   |   Large+\n",
    "  Risky    |   Risky\n",
    "           |\n",
    "‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Size (PC1)\n",
    "           |\n",
    "  Small+   |   Large+\n",
    "  Safe     |   Safe\n",
    "           ‚Üì\n",
    "       Low Risk\n",
    "```\n",
    "\n",
    "**Interpret based on position:**\n",
    "- **Top-right:** Large risky tokens (growth opportunities?)\n",
    "- **Top-left:** Small risky tokens (speculative plays?)\n",
    "- **Bottom-right:** Large stable tokens (blue chips?)\n",
    "- **Bottom-left:** Small stable tokens (overlooked gems?)\n",
    "\n",
    "### Step 3: Token Positions\n",
    "\n",
    "**Look for:**\n",
    "- Tokens on cluster edges (borderline cases)\n",
    "- Tokens between clusters (hard to categorize)\n",
    "- Outlier tokens (very different from others)\n",
    "\n",
    "### Step 4: Validate Against Centroids\n",
    "\n",
    "**Cross-check:**\n",
    "- Do visual clusters match centroid analysis?\n",
    "- Are distinguishing features visible in plot?\n",
    "- Does positioning make economic sense?\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Caveats\n",
    "\n",
    "### 1. Information Loss\n",
    "\n",
    "**Remember:** 2D plot only shows PC1 and PC2\n",
    "- Usually captures 50-70% of variance\n",
    "- Other PCs (PC3, PC4, ...) are hidden\n",
    "- Some cluster separation may be in higher dimensions\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "2D view: Clusters overlap\n",
    "3D view: Actually well-separated in PC3 dimension\n",
    "```\n",
    "\n",
    "### 2. PCA Limitations\n",
    "\n",
    "- PCA projects based on variance, not cluster structure\n",
    "- Best 2D view might not show all cluster separation\n",
    "- Alternative: Use t-SNE or UMAP for better visualization\n",
    "\n",
    "### 3. Projection Artifacts\n",
    "\n",
    "- Distance in 2D ‚â† distance in 8D\n",
    "- Some nearby points in 2D may be far in 8D\n",
    "- Some far points in 2D may be close in 8D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ac594-4a2b-4031-91c7-812ed416dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"VISUALIZING CLUSTERS IN 2D SPACE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Challenge: Our tokens exist in 8-dimensional space\")\n",
    "print(\"   Solution: Use PCA to project into 2D for visualization\")\n",
    "print(\"\\nüîß Process:\")\n",
    "print(\"   1. Apply PCA to reduce 8 features ‚Üí 2 principal components\")\n",
    "print(\"   2. Plot tokens using PC1 (x-axis) and PC2 (y-axis)\")\n",
    "print(\"   3. Color each token by its cluster assignment\")\n",
    "print(\"   4. Analyze the visual pattern\\n\")\n",
    "\n",
    "# Apply PCA to reduce to 2D\n",
    "print(\"Step 1: Applying PCA for 2D projection...\")\n",
    "pca_2d = SimplePCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(defi_data.values, feature_names=defi_data.columns.tolist())\n",
    "\n",
    "# Get variance explained by PC1 and PC2\n",
    "var_pc1 = pca_2d.pca.explained_variance_ratio_[0]\n",
    "var_pc2 = pca_2d.pca.explained_variance_ratio_[1]\n",
    "total_var_2d = var_pc1 + var_pc2\n",
    "\n",
    "print(f\"   ‚úÖ PCA complete!\")\n",
    "print(f\"   ‚Ä¢ PC1 explains: {var_pc1:.1%} of variance\")\n",
    "print(f\"   ‚Ä¢ PC2 explains: {var_pc2:.1%} of variance\")\n",
    "print(f\"   ‚Ä¢ Total captured in 2D: {total_var_2d:.1%}\")\n",
    "\n",
    "if total_var_2d < 0.50:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Warning: Only {total_var_2d:.1%} of variance shown in 2D\")\n",
    "    print(f\"      Much cluster separation may be in higher dimensions\")\n",
    "elif total_var_2d < 0.70:\n",
    "    print(f\"\\n   ‚úÖ Good: {total_var_2d:.1%} of variance visible in 2D\")\n",
    "else:\n",
    "    print(f\"\\n   ‚úÖ Excellent: {total_var_2d:.1%} of variance captured in 2D\")\n",
    "\n",
    "# Show what PC1 and PC2 represent\n",
    "print(\"\\nüìä Understanding the axes:\")\n",
    "loadings_2d = pca_2d.get_loadings()\n",
    "\n",
    "print(\"\\nPC1 (X-axis) top loadings:\")\n",
    "pc1_top = loadings_2d['PC1'].abs().nlargest(3)\n",
    "for feature in pc1_top.index:\n",
    "    loading = loadings_2d.loc[feature, 'PC1']\n",
    "    print(f\"   ‚Ä¢ {feature}: {loading:+.3f}\")\n",
    "\n",
    "print(\"\\nPC2 (Y-axis) top loadings:\")\n",
    "pc2_top = loadings_2d['PC2'].abs().nlargest(3)\n",
    "for feature in pc2_top.index:\n",
    "    loading = loadings_2d.loc[feature, 'PC2']\n",
    "    print(f\"   ‚Ä¢ {feature}: {loading:+.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Step 2: Creating visualization...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create the plot\n",
    "fig = kmeans_model.plot_clusters_2d(\n",
    "    X_2d,\n",
    "    labels=defi_data.index.tolist(),\n",
    "    figsize=(12, 9)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPRETING THE VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze cluster separation in 2D\n",
    "clusters = kmeans_model.kmeans.labels_\n",
    "\n",
    "print(\"\\nüîç Cluster Analysis in 2D Space:\")\n",
    "\n",
    "# Calculate cluster statistics in 2D\n",
    "for i in range(optimal_k):\n",
    "    mask = clusters == i\n",
    "    cluster_points = X_2d[mask]\n",
    "    \n",
    "    # Center of cluster in 2D\n",
    "    center_pc1 = cluster_points[:, 0].mean()\n",
    "    center_pc2 = cluster_points[:, 1].mean()\n",
    "    \n",
    "    # Spread of cluster\n",
    "    std_pc1 = cluster_points[:, 0].std()\n",
    "    std_pc2 = cluster_points[:, 1].std()\n",
    "    \n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    print(f\"   Center: PC1={center_pc1:+.2f}, PC2={center_pc2:+.2f}\")\n",
    "    print(f\"   Spread: PC1¬±{std_pc1:.2f}, PC2¬±{std_pc2:.2f}\")\n",
    "    \n",
    "    # Interpret position\n",
    "    position_desc = []\n",
    "    if abs(center_pc1) > 1.0:\n",
    "        position_desc.append(f\"{'High' if center_pc1 > 0 else 'Low'} PC1 (size)\")\n",
    "    if abs(center_pc2) > 1.0:\n",
    "        position_desc.append(f\"{'High' if center_pc2 > 0 else 'Low'} PC2 (risk)\")\n",
    "    \n",
    "    if position_desc:\n",
    "        print(f\"   Position: {', '.join(position_desc)}\")\n",
    "\n",
    "# Calculate pairwise cluster distances\n",
    "print(\"\\nüìè Cluster Separation (distance between centers):\")\n",
    "print(\"   Larger distance = better separation\\n\")\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Get cluster centers in 2D\n",
    "cluster_centers_2d = []\n",
    "for i in range(optimal_k):\n",
    "    mask = clusters == i\n",
    "    center = X_2d[mask].mean(axis=0)\n",
    "    cluster_centers_2d.append(center)\n",
    "\n",
    "cluster_centers_2d = np.array(cluster_centers_2d)\n",
    "distances = squareform(pdist(cluster_centers_2d, metric='euclidean'))\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    for j in range(i+1, optimal_k):\n",
    "        dist = distances[i, j]\n",
    "        print(f\"   Cluster {i} ‚Üî Cluster {j}: {dist:.2f}\")\n",
    "\n",
    "avg_separation = distances[np.triu_indices_from(distances, k=1)].mean()\n",
    "print(f\"\\n   Average separation: {avg_separation:.2f}\")\n",
    "\n",
    "if avg_separation > 3.0:\n",
    "    print(\"   ‚úÖ Excellent separation - clusters are well-defined\")\n",
    "elif avg_separation > 2.0:\n",
    "    print(\"   ‚úÖ Good separation - clusters are reasonably distinct\")\n",
    "elif avg_separation > 1.0:\n",
    "    print(\"   ‚ö†Ô∏è  Moderate separation - some overlap expected\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Poor separation - clusters may be overlapping significantly\")\n",
    "\n",
    "# Identify outliers (tokens far from their cluster center)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"IDENTIFYING OUTLIERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüéØ Outliers are tokens far from their cluster center\")\n",
    "print(\"   They may represent:\")\n",
    "print(\"   ‚Ä¢ Unique investment opportunities\")\n",
    "print(\"   ‚Ä¢ Data anomalies\")\n",
    "print(\"   ‚Ä¢ Tokens that don't fit any category well\\n\")\n",
    "\n",
    "outliers_found = []\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    mask = clusters == i\n",
    "    cluster_points = X_2d[mask]\n",
    "    cluster_tokens = defi_data.index[mask]\n",
    "    \n",
    "    # Calculate distance from each point to cluster center\n",
    "    center = cluster_points.mean(axis=0)\n",
    "    distances_from_center = np.sqrt(((cluster_points - center)**2).sum(axis=1))\n",
    "    \n",
    "    # Outliers are > 2.5 standard deviations from center\n",
    "    threshold = distances_from_center.mean() + 2.5 * distances_from_center.std()\n",
    "    outlier_mask = distances_from_center > threshold\n",
    "    \n",
    "    if outlier_mask.any():\n",
    "        outlier_indices = np.where(outlier_mask)[0]\n",
    "        for idx in outlier_indices:\n",
    "            token_name = cluster_tokens[idx]\n",
    "            distance = distances_from_center[idx]\n",
    "            outliers_found.append({\n",
    "                'Token': token_name,\n",
    "                'Cluster': i,\n",
    "                'Distance': distance\n",
    "            })\n",
    "\n",
    "if outliers_found:\n",
    "    outliers_df = pd.DataFrame(outliers_found).sort_values('Distance', ascending=False)\n",
    "    print(f\"Found {len(outliers_df)} potential outliers:\\n\")\n",
    "    display(outliers_df.head(10))\n",
    "    \n",
    "    print(\"\\nüí° What to do with outliers:\")\n",
    "    print(\"   ‚Ä¢ Investigate: Check if data is correct\")\n",
    "    print(\"   ‚Ä¢ Analyze: Understand what makes them unique\")\n",
    "    print(\"   ‚Ä¢ Decide: Include/exclude based on investment strategy\")\n",
    "else:\n",
    "    print(\"‚úÖ No significant outliers detected\")\n",
    "    print(\"   All tokens fit reasonably well into their clusters\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"QUADRANT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìç Analyzing token distribution across quadrants:\")\n",
    "print(\"   (Based on PC1 and PC2 coordinates)\\n\")\n",
    "\n",
    "# Define quadrants\n",
    "quadrants = {\n",
    "    'High PC1, High PC2': (X_2d[:, 0] > 0) & (X_2d[:, 1] > 0),\n",
    "    'Low PC1, High PC2': (X_2d[:, 0] < 0) & (X_2d[:, 1] > 0),\n",
    "    'Low PC1, Low PC2': (X_2d[:, 0] < 0) & (X_2d[:, 1] < 0),\n",
    "    'High PC1, Low PC2': (X_2d[:, 0] > 0) & (X_2d[:, 1] < 0)\n",
    "}\n",
    "\n",
    "for quadrant_name, quadrant_mask in quadrants.items():\n",
    "    count = quadrant_mask.sum()\n",
    "    percentage = count / len(defi_data) * 100\n",
    "    print(f\"   {quadrant_name:20s}: {count:2d} tokens ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\nüí° Quadrant interpretation (typical):\")\n",
    "print(\"   ‚Ä¢ High PC1, High PC2:  Large & Risky tokens\")\n",
    "print(\"   ‚Ä¢ Low PC1, High PC2:   Small & Risky tokens\")  \n",
    "print(\"   ‚Ä¢ Low PC1, Low PC2:    Small & Stable tokens\")\n",
    "print(\"   ‚Ä¢ High PC1, Low PC2:   Large & Stable tokens\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VISUAL VALIDATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ Checklist - Does the visualization show:\")\n",
    "\n",
    "# Check 1: Separation\n",
    "separation_ok = avg_separation > 1.5\n",
    "print(f\"   {'‚úÖ' if separation_ok else '‚ùå'} Clear cluster separation\")\n",
    "\n",
    "# Check 2: Balance\n",
    "size_variance = cluster_summary['Size'].std()\n",
    "balance_ok = size_variance < len(defi_data) * 0.2\n",
    "print(f\"   {'‚úÖ' if balance_ok else '‚ö†Ô∏è '} Reasonably balanced cluster sizes\")\n",
    "\n",
    "# Check 3: Coverage\n",
    "coverage_ok = total_var_2d > 0.5\n",
    "print(f\"   {'‚úÖ' if coverage_ok else '‚ö†Ô∏è '} Sufficient variance captured in 2D\")\n",
    "\n",
    "# Check 4: Outliers\n",
    "outliers_ok = len(outliers_found) < len(defi_data) * 0.1  # Less than 10% outliers\n",
    "print(f\"   {'‚úÖ' if outliers_ok else '‚ö†Ô∏è '} Reasonable number of outliers\")\n",
    "\n",
    "# Overall assessment\n",
    "all_ok = separation_ok and balance_ok and coverage_ok and outliers_ok\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if all_ok:\n",
    "    print(\"‚úÖ OVERALL: Clustering looks GOOD!\")\n",
    "    print(\"   Clusters are well-defined and make visual sense\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  OVERALL: Clustering is ACCEPTABLE but has some issues\")\n",
    "    print(\"   Consider adjusting k or investigating outliers\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ 2D visualization complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf2844-c15c-4737-a636-1498407c259c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Portfolio Construction Using Clusters\n",
    "\n",
    "### From Analysis to Action\n",
    "\n",
    "We've done the hard work:\n",
    "- ‚úÖ Applied PCA to understand variance structure\n",
    "- ‚úÖ Found optimal number of clusters with elbow method\n",
    "- ‚úÖ Fitted K-Means and interpreted cluster characteristics\n",
    "- ‚úÖ Visualized clusters in 2D space\n",
    "\n",
    "**Use these insights to build an actual portfolio!** \n",
    "\n",
    "---\n",
    "\n",
    "##  Why Cluster-Based Portfolio Construction?\n",
    "\n",
    "### Traditional Approach (Problems)\n",
    "```\n",
    "‚ùå Manual selection:\n",
    "   ‚Ä¢ Subjective decisions\n",
    "   ‚Ä¢ Unconscious bias toward familiar tokens\n",
    "   ‚Ä¢ May miss diversification opportunities\n",
    "   ‚Ä¢ Hard to justify choices\n",
    "\n",
    "‚ùå Random selection:\n",
    "   ‚Ä¢ No strategy\n",
    "   ‚Ä¢ Likely concentrated risk\n",
    "   ‚Ä¢ Difficult to rebalance\n",
    "```\n",
    "\n",
    "### Cluster-Based Approach (Benefits)\n",
    "```\n",
    "‚úÖ Systematic diversification:\n",
    "   ‚Ä¢ Select from each cluster\n",
    "   ‚Ä¢ Ensures exposure to different market segments\n",
    "   ‚Ä¢ Reduces concentration risk\n",
    "\n",
    "‚úÖ Risk management:\n",
    "   ‚Ä¢ Understand portfolio risk profile\n",
    "   ‚Ä¢ Adjust allocation by cluster\n",
    "   ‚Ä¢ Balance risk/return systematically\n",
    "\n",
    "‚úÖ Transparent methodology:\n",
    "   ‚Ä¢ Clear selection criteria\n",
    "   ‚Ä¢ Easy to explain to stakeholders\n",
    "   ‚Ä¢ Reproducible and auditable\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  Portfolio Construction Strategies\n",
    "\n",
    "### Strategy 1: Equal Weight Across Clusters\n",
    "\n",
    "**Concept:** Select same number of tokens from each cluster\n",
    "```\n",
    "Example with k=4 clusters, 8-token portfolio:\n",
    "\n",
    "Cluster 0 (Large Stable):     2 tokens ‚Üí 25%\n",
    "Cluster 1 (Mid Growth):       2 tokens ‚Üí 25%\n",
    "Cluster 2 (Small High-Risk):  2 tokens ‚Üí 25%\n",
    "Cluster 3 (High Sentiment):   2 tokens ‚Üí 25%\n",
    "                             ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                             8 tokens   100%\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Maximum diversification across segments\n",
    "- ‚úÖ Simple and easy to understand\n",
    "- ‚úÖ Captures all market dynamics\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Ignores risk differences between clusters\n",
    "- ‚ùå May over-allocate to risky segments\n",
    "- ‚ùå Not aligned with risk tolerance\n",
    "\n",
    "**Best for:** Exploratory portfolios, neutral market view\n",
    "\n",
    "---\n",
    "\n",
    "### Strategy 2: Risk-Adjusted Allocation\n",
    "\n",
    "**Concept:** Allocate more capital to lower-risk clusters\n",
    "```\n",
    "Example with k=3 clusters, $100K portfolio:\n",
    "\n",
    "Cluster 0 (Low Vol):    $50K ‚Üí 50% (safe base)\n",
    "Cluster 1 (Med Vol):    $30K ‚Üí 30% (growth)\n",
    "Cluster 2 (High Vol):   $20K ‚Üí 20% (speculation)\n",
    "                       ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                       $100K  100%\n",
    "```\n",
    "\n",
    "**Conservative allocation:**\n",
    "- 60% low-risk cluster\n",
    "- 30% medium-risk cluster\n",
    "- 10% high-risk cluster\n",
    "\n",
    "**Balanced allocation:**\n",
    "- 40% low-risk cluster\n",
    "- 40% medium-risk cluster\n",
    "- 20% high-risk cluster\n",
    "\n",
    "**Aggressive allocation:**\n",
    "- 20% low-risk cluster\n",
    "- 30% medium-risk cluster\n",
    "- 50% high-risk cluster\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Aligned with investor risk tolerance\n",
    "- ‚úÖ Manages downside risk\n",
    "- ‚úÖ Clear risk/return objectives\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Requires accurate risk estimation\n",
    "- ‚ùå May underweight growth opportunities\n",
    "- ‚ùå More complex to explain\n",
    "\n",
    "**Best for:** Risk-aware investors, institutional portfolios\n",
    "\n",
    "---\n",
    "\n",
    "### Strategy 3: Quality-Weighted Selection\n",
    "\n",
    "**Concept:** Select best tokens from each cluster using a quality metric\n",
    "```\n",
    "For each cluster:\n",
    "1. Calculate quality score (e.g., Sharpe ratio = Return/Volatility)\n",
    "2. Rank tokens by quality\n",
    "3. Select top N tokens\n",
    "\n",
    "Example:\n",
    "Cluster 0: Select top 2 by Sharpe ratio\n",
    "Cluster 1: Select top 2 by Sharpe ratio\n",
    "Cluster 2: Select top 2 by Sharpe ratio\n",
    "```\n",
    "\n",
    "**Common quality metrics:**\n",
    "- **Sharpe Ratio:** Return / Volatility (risk-adjusted return)\n",
    "- **Sortino Ratio:** Return / Downside Volatility (downside risk only)\n",
    "- **Return/MaxDrawdown:** Return / Maximum loss\n",
    "- **Liquidity-Adjusted Return:** Return √ó Liquidity Score\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Selects best performers within each segment\n",
    "- ‚úÖ Quantitative selection criteria\n",
    "- ‚úÖ Balances return and risk\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Past performance ‚â† future performance\n",
    "- ‚ùå May miss contrarian opportunities\n",
    "- ‚ùå Requires clean historical data\n",
    "\n",
    "**Best for:** Performance-focused portfolios, backtesting\n",
    "\n",
    "---\n",
    "\n",
    "### Strategy 4: Market-Cap Weighted Within Clusters\n",
    "\n",
    "**Concept:** Weight tokens by market cap within each cluster\n",
    "```\n",
    "Cluster 0 (selected tokens):\n",
    "  TOKEN_1: $500M market cap ‚Üí 50% of cluster allocation\n",
    "  TOKEN_2: $300M market cap ‚Üí 30%\n",
    "  TOKEN_3: $200M market cap ‚Üí 20%\n",
    "\n",
    "Similar weighting for other clusters\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Reflects market importance\n",
    "- ‚úÖ More liquidity for large positions\n",
    "- ‚úÖ Lower rebalancing costs\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Biased toward large tokens\n",
    "- ‚ùå Misses small-cap opportunities\n",
    "- ‚ùå Can create concentration risk\n",
    "\n",
    "**Best for:** Large portfolios, passive strategies\n",
    "\n",
    "---\n",
    "\n",
    "### Strategy 5: Minimum Variance Portfolio\n",
    "\n",
    "**Concept:** Optimize weights to minimize portfolio volatility\n",
    "```\n",
    "Mathematical optimization:\n",
    "  Minimize: Portfolio Variance\n",
    "  Subject to:\n",
    "    ‚Ä¢ Sum of weights = 1\n",
    "    ‚Ä¢ At least 1 token from each cluster\n",
    "    ‚Ä¢ Individual weight limits (e.g., max 20%)\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Mathematically optimal for risk minimization\n",
    "- ‚úÖ Uses covariance structure\n",
    "- ‚úÖ Can incorporate constraints\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Computationally intensive\n",
    "- ‚ùå Requires correlation estimates\n",
    "- ‚ùå May concentrate in few tokens\n",
    "- ‚ùå Sensitive to input errors\n",
    "\n",
    "**Best for:** Advanced portfolios, institutional investors\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Our Implementation: Sharpe Ratio Selection\n",
    "\n",
    "### What We'll Build\n",
    "\n",
    "We'll use **Strategy 3 (Quality-Weighted)** with **Sharpe ratio** as our quality metric.\n",
    "\n",
    "**Why Sharpe Ratio?**\n",
    "```\n",
    "Sharpe Ratio = Mean Return / Volatility\n",
    "\n",
    "Interpretation:\n",
    "- High Sharpe = Good return per unit of risk\n",
    "- Low Sharpe = Poor return for the risk taken\n",
    "- Negative Sharpe = Losing money\n",
    "\n",
    "Example:\n",
    "Token A: Return=5%, Vol=10% ‚Üí Sharpe=0.50\n",
    "Token B: Return=8%, Vol=20% ‚Üí Sharpe=0.40\n",
    "‚Üí Token A is better risk-adjusted!\n",
    "```\n",
    "\n",
    "**Our Process:**\n",
    "```\n",
    "1. Calculate Sharpe ratio for each token\n",
    "2. For each cluster:\n",
    "   a. Rank tokens by Sharpe ratio\n",
    "   b. Select top 2 tokens\n",
    "3. Equal weight across selected tokens\n",
    "4. Analyze resulting portfolio characteristics\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  Portfolio Construction Checklist\n",
    "\n",
    "### Before Building:\n",
    "\n",
    "**Understand cluster characteristics**\n",
    "  - Know what each cluster represents\n",
    "  - Identify risk levels\n",
    "\n",
    "**Define investment objectives**\n",
    "  - Risk tolerance (conservative/balanced/aggressive)\n",
    "  - Return expectations\n",
    "  - Time horizon\n",
    "\n",
    "**Choose allocation strategy**\n",
    "  - Equal weight, risk-adjusted, or quality-weighted\n",
    "  - Decide number of tokens per cluster\n",
    "\n",
    "**Set constraints**\n",
    "  - Minimum/maximum position sizes\n",
    "  - Liquidity requirements\n",
    "  - Sector limits (if applicable)\n",
    "\n",
    "### After Building:\n",
    "\n",
    "**Validate portfolio characteristics**\n",
    "  - Total expected return\n",
    "  - Total expected volatility\n",
    "  - Diversification (not too concentrated)\n",
    "\n",
    "**Check cluster representation**\n",
    "  - At least 1 token from each cluster\n",
    "  - No cluster dominates (unless intended)\n",
    "\n",
    "**Review individual positions**\n",
    "  - Each token makes economic sense\n",
    "  - No data errors or outliers\n",
    "\n",
    "**Document rationale**\n",
    "  - Why this strategy?\n",
    "  - Why these specific tokens?\n",
    "  - What's the exit/rebalancing plan?\n",
    "\n",
    "---\n",
    "\n",
    "##  Portfolio Management Best Practices\n",
    "\n",
    "### Rebalancing Strategy\n",
    "\n",
    "**When to rebalance:**\n",
    "- üîÑ **Time-based:** Every month/quarter\n",
    "- üîÑ **Threshold-based:** When weights drift > 5%\n",
    "- üîÑ **Cluster-based:** When tokens change clusters\n",
    "\n",
    "**How to rebalance:**\n",
    "```\n",
    "1. Reassign tokens to current clusters\n",
    "2. Check if any token changed cluster\n",
    "3. Sell tokens that left target clusters\n",
    "4. Buy replacement tokens from new clusters\n",
    "5. Reweight to target allocation\n",
    "```\n",
    "\n",
    "### Monitoring Metrics\n",
    "\n",
    "**Track regularly:**\n",
    "- üìä **Portfolio return** vs benchmarks\n",
    "- üìä **Portfolio volatility** vs expectations\n",
    "- üìä **Cluster drift** (tokens changing clusters)\n",
    "- üìä **Individual token performance** within clusters\n",
    "- üìä **Correlation changes** between tokens\n",
    "\n",
    "### Risk Management\n",
    "\n",
    "**Set rules:**\n",
    "- üõ°Ô∏è **Stop losses:** Exit if token drops > X%\n",
    "- üõ°Ô∏è **Position limits:** Max 20% in any single token\n",
    "- üõ°Ô∏è **Cluster limits:** Max 40% in any single cluster\n",
    "- üõ°Ô∏è **Liquidity minimums:** Only trade tokens above min volume\n",
    "- üõ°Ô∏è **Diversification floors:** Minimum 8-10 tokens\n",
    "\n",
    "---\n",
    "\n",
    "##  Expected Outcomes\n",
    "\n",
    "### What You'll Get From Cluster-Based Portfolios:\n",
    "\n",
    "**1. Better Diversification**\n",
    "- Exposure to different market segments\n",
    "- Reduced correlation between holdings\n",
    "- Lower concentration risk\n",
    "\n",
    "**2. Systematic Process**\n",
    "- Repeatable methodology\n",
    "- Less emotional decision-making\n",
    "- Clear rationale for holdings\n",
    "\n",
    "**3. Risk Awareness**\n",
    "- Understand portfolio risk profile\n",
    "- Know which clusters drive performance\n",
    "- Easier to adjust risk exposure\n",
    "\n",
    "**4. Performance Attribution**\n",
    "- Know which clusters perform best\n",
    "- Identify successful strategies\n",
    "- Learn from mistakes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed893b38-6dbb-4c09-b77f-6a313c979b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BUILDING A CLUSTER-BASED DIVERSIFIED PORTFOLIO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüéØ Portfolio Construction Strategy:\")\n",
    "print(\"   ‚Ä¢ Method: Quality-weighted selection using Sharpe ratio\")\n",
    "print(\"   ‚Ä¢ Tokens per cluster: 2 (for balanced representation)\")\n",
    "print(\"   ‚Ä¢ Weighting: Equal weight across selected tokens\")\n",
    "print(\"   ‚Ä¢ Total portfolio size: Varies by number of clusters\\n\")\n",
    "\n",
    "print(\"üí° Why Sharpe Ratio?\")\n",
    "print(\"   Sharpe = Mean_Return / Volatility\")\n",
    "print(\"   ‚Üí Measures return per unit of risk\")\n",
    "print(\"   ‚Üí Higher is better (more return for the risk)\\n\")\n",
    "\n",
    "# Calculate Sharpe ratio for all tokens\n",
    "print(\"Step 1: Calculating Sharpe ratio for all tokens...\")\n",
    "defi_data_clustered['Sharpe_Ratio'] = (\n",
    "    defi_data_clustered['Mean_Return'] / \n",
    "    (defi_data_clustered['Volatility'] + 1e-10)  # Add small value to avoid division by zero\n",
    ")\n",
    "\n",
    "print(\"   ‚úÖ Sharpe ratios calculated\\n\")\n",
    "\n",
    "# Show distribution of Sharpe ratios\n",
    "print(\"üìä Sharpe Ratio Distribution:\")\n",
    "sharpe_stats = defi_data_clustered['Sharpe_Ratio'].describe()\n",
    "print(f\"   Mean:   {sharpe_stats['mean']:>8.3f}\")\n",
    "print(f\"   Median: {sharpe_stats['50%']:>8.3f}\")\n",
    "print(f\"   Min:    {sharpe_stats['min']:>8.3f}\")\n",
    "print(f\"   Max:    {sharpe_stats['max']:>8.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Step 2: Selecting top tokens from each cluster...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Portfolio parameters\n",
    "tokens_per_cluster = 2\n",
    "portfolio = []\n",
    "\n",
    "print()\n",
    "for i in range(optimal_k):\n",
    "    cluster_data = defi_data_clustered[defi_data_clustered['Cluster'] == i].copy()\n",
    "    \n",
    "    print(f\"\\nCluster {i} ({len(cluster_data)} tokens):\")\n",
    "    \n",
    "    # Sort by Sharpe ratio and select top N\n",
    "    cluster_data_sorted = cluster_data.sort_values('Sharpe_Ratio', ascending=False)\n",
    "    selected = cluster_data_sorted.head(tokens_per_cluster)\n",
    "    \n",
    "    # Add to portfolio\n",
    "    for idx, token in enumerate(selected.index):\n",
    "        token_data = selected.loc[token]\n",
    "        \n",
    "        portfolio.append({\n",
    "            'Token': token,\n",
    "            'Cluster': i,\n",
    "            'Mean_Return': token_data['Mean_Return'],\n",
    "            'Volatility': token_data['Volatility'],\n",
    "            'Sharpe_Ratio': token_data['Sharpe_Ratio'],\n",
    "            'Market_Cap': token_data['Market_Cap'],\n",
    "            'Liquidity_Score': token_data['Liquidity_Score']\n",
    "        })\n",
    "        \n",
    "        print(f\"   {idx+1}. {token:15s} - Sharpe: {token_data['Sharpe_Ratio']:>6.3f} \"\n",
    "              f\"(Return: {token_data['Mean_Return']:>7.4f}, Vol: {token_data['Volatility']:>6.4f})\")\n",
    "\n",
    "# Create portfolio DataFrame\n",
    "portfolio_df = pd.DataFrame(portfolio)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PORTFOLIO CONSTRUCTED!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n‚úÖ Selected {len(portfolio_df)} tokens for the portfolio\")\n",
    "print(f\"   ({tokens_per_cluster} tokens √ó {optimal_k} clusters)\")\n",
    "\n",
    "print(\"\\nüìã Complete Portfolio:\")\n",
    "display(portfolio_df)\n",
    "\n",
    "# Calculate equal weights\n",
    "portfolio_df['Weight'] = 1.0 / len(portfolio_df)\n",
    "portfolio_df['Weight_Pct'] = portfolio_df['Weight'] * 100\n",
    "\n",
    "print(\"\\nüí∞ Portfolio Weights:\")\n",
    "weight_display = portfolio_df[['Token', 'Cluster', 'Weight_Pct']].copy()\n",
    "weight_display['Weight_Pct'] = weight_display['Weight_Pct'].apply(lambda x: f\"{x:.2f}%\")\n",
    "display(weight_display)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PORTFOLIO STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate portfolio-level statistics\n",
    "portfolio_return = (portfolio_df['Mean_Return'] * portfolio_df['Weight']).sum()\n",
    "portfolio_volatility = np.sqrt((portfolio_df['Volatility']**2 * portfolio_df['Weight']**2).sum())\n",
    "portfolio_sharpe = portfolio_return / portfolio_volatility if portfolio_volatility > 0 else 0\n",
    "\n",
    "# For comparison, calculate equally-weighted whole portfolio stats\n",
    "equal_weight_return = defi_data['Mean_Return'].mean()\n",
    "equal_weight_vol = defi_data['Volatility'].mean()  # Simplified\n",
    "equal_weight_sharpe = equal_weight_return / equal_weight_vol if equal_weight_vol > 0 else 0\n",
    "\n",
    "print(\"\\nüìä Portfolio Performance Metrics:\")\n",
    "print(f\"\\n   Expected Daily Return:   {portfolio_return:>8.4f} ({portfolio_return*100:.2f}%)\")\n",
    "print(f\"   Expected Daily Volatility: {portfolio_volatility:>8.4f} ({portfolio_volatility*100:.2f}%)\")\n",
    "print(f\"   Portfolio Sharpe Ratio:    {portfolio_sharpe:>8.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Annualized Estimates (√ó252 trading days):\")\n",
    "annual_return = portfolio_return * 252\n",
    "annual_vol = portfolio_volatility * np.sqrt(252)\n",
    "print(f\"   Annual Return:      {annual_return:>8.2%}\")\n",
    "print(f\"   Annual Volatility:  {annual_vol:>8.2%}\")\n",
    "print(f\"   Annual Sharpe:      {portfolio_sharpe:>8.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Benchmark Comparison (Equal-weight all tokens):\")\n",
    "print(f\"   Benchmark Return:    {equal_weight_return:>8.4f}\")\n",
    "print(f\"   Benchmark Volatility: {equal_weight_vol:>8.4f}\")\n",
    "print(f\"   Benchmark Sharpe:    {equal_weight_sharpe:>8.3f}\")\n",
    "\n",
    "improvement = portfolio_sharpe - equal_weight_sharpe\n",
    "if improvement > 0:\n",
    "    print(f\"\\n   ‚úÖ Our portfolio Sharpe is {improvement:.3f} BETTER than benchmark\")\n",
    "    print(f\"      ({(improvement/equal_weight_sharpe*100):.1f}% improvement)\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Benchmark Sharpe is {abs(improvement):.3f} better\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLUSTER REPRESENTATION IN PORTFOLIO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze cluster representation\n",
    "cluster_representation = portfolio_df.groupby('Cluster').agg({\n",
    "    'Token': 'count',\n",
    "    'Weight': 'sum',\n",
    "    'Mean_Return': 'mean',\n",
    "    'Volatility': 'mean',\n",
    "    'Sharpe_Ratio': 'mean'\n",
    "}).rename(columns={'Token': 'Num_Tokens'})\n",
    "\n",
    "cluster_representation['Weight_Pct'] = cluster_representation['Weight'] * 100\n",
    "\n",
    "print(\"\\nüìä Portfolio Composition by Cluster:\")\n",
    "display(cluster_representation.round(4))\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "for i in range(optimal_k):\n",
    "    if i in cluster_representation.index:\n",
    "        num_tokens = cluster_representation.loc[i, 'Num_Tokens']\n",
    "        weight_pct = cluster_representation.loc[i, 'Weight_Pct']\n",
    "        avg_return = cluster_representation.loc[i, 'Mean_Return']\n",
    "        avg_vol = cluster_representation.loc[i, 'Volatility']\n",
    "        \n",
    "        print(f\"\\n   Cluster {i}: {num_tokens} tokens ({weight_pct:.1f}% of portfolio)\")\n",
    "        print(f\"   ‚Ä¢ Average return: {avg_return:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Average volatility: {avg_vol:.4f}\")\n",
    "\n",
    "# Check if representation is balanced\n",
    "weight_std = cluster_representation['Weight_Pct'].std()\n",
    "if weight_std < 5:\n",
    "    print(\"\\n   ‚úÖ Well-balanced across clusters\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Somewhat imbalanced (std dev: {weight_std:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Portfolio composition by cluster\n",
    "ax1 = axes[0, 0]\n",
    "cluster_counts = portfolio_df['Cluster'].value_counts().sort_index()\n",
    "colors_clusters = plt.cm.Set3(np.linspace(0, 1, optimal_k))\n",
    "bars1 = ax1.bar(cluster_counts.index, cluster_counts.values, \n",
    "                alpha=0.7, color=[colors_clusters[i] for i in cluster_counts.index],\n",
    "                edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i, (cluster, count) in enumerate(cluster_counts.items()):\n",
    "    ax1.text(cluster, count + 0.1, f'{count} tokens', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax1.set_xlabel('Cluster', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Number of Tokens', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Portfolio Distribution Across Clusters', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(range(optimal_k))\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Risk-Return scatter\n",
    "ax2 = axes[0, 1]\n",
    "for i in range(optimal_k):\n",
    "    cluster_tokens = portfolio_df[portfolio_df['Cluster'] == i]\n",
    "    ax2.scatter(cluster_tokens['Volatility'], cluster_tokens['Mean_Return'],\n",
    "               s=100, alpha=0.6, label=f'Cluster {i}', \n",
    "               color=colors_clusters[i], edgecolors='black', linewidth=1)\n",
    "\n",
    "# Add token labels\n",
    "for idx, row in portfolio_df.iterrows():\n",
    "    ax2.annotate(row['Token'], \n",
    "                (row['Volatility'], row['Mean_Return']),\n",
    "                fontsize=7, alpha=0.7, ha='right')\n",
    "\n",
    "ax2.set_xlabel('Volatility (Risk)', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Mean Return', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Portfolio Risk-Return Profile', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Sharpe ratio comparison\n",
    "ax3 = axes[1, 0]\n",
    "portfolio_df_sorted = portfolio_df.sort_values('Sharpe_Ratio', ascending=True)\n",
    "colors_sharpe = ['green' if x > portfolio_sharpe else 'orange' \n",
    "                 for x in portfolio_df_sorted['Sharpe_Ratio']]\n",
    "\n",
    "ax3.barh(range(len(portfolio_df_sorted)), portfolio_df_sorted['Sharpe_Ratio'],\n",
    "        alpha=0.7, color=colors_sharpe, edgecolor='black', linewidth=1)\n",
    "ax3.set_yticks(range(len(portfolio_df_sorted)))\n",
    "ax3.set_yticklabels(portfolio_df_sorted['Token'], fontsize=9)\n",
    "ax3.set_xlabel('Sharpe Ratio', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Individual Token Sharpe Ratios', fontsize=12, fontweight='bold')\n",
    "ax3.axvline(x=portfolio_sharpe, color='red', linestyle='--', \n",
    "           linewidth=2, label=f'Portfolio avg: {portfolio_sharpe:.3f}')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 4: Weight allocation pie chart\n",
    "ax4 = axes[1, 1]\n",
    "cluster_weights = portfolio_df.groupby('Cluster')['Weight'].sum()\n",
    "ax4.pie(cluster_weights.values, labels=[f'Cluster {i}' for i in cluster_weights.index],\n",
    "       autopct='%1.1f%%', startangle=90, colors=colors_clusters,\n",
    "       wedgeprops={'edgecolor': 'black', 'linewidth': 1.5})\n",
    "ax4.set_title('Portfolio Allocation by Cluster', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PORTFOLIO RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüéØ Portfolio Characteristics:\")\n",
    "\n",
    "# Classify portfolio risk level\n",
    "if portfolio_volatility < defi_data['Volatility'].quantile(0.33):\n",
    "    risk_level = \"CONSERVATIVE\"\n",
    "    emoji = \"üõ°Ô∏è\"\n",
    "elif portfolio_volatility < defi_data['Volatility'].quantile(0.67):\n",
    "    risk_level = \"BALANCED\"\n",
    "    emoji = \"‚öñÔ∏è\"\n",
    "else:\n",
    "    risk_level = \"AGGRESSIVE\"\n",
    "    emoji = \"üöÄ\"\n",
    "\n",
    "print(f\"\\n   {emoji} Risk Profile: {risk_level}\")\n",
    "print(f\"   Portfolio volatility: {portfolio_volatility:.4f}\")\n",
    "print(f\"   Market average: {defi_data['Volatility'].mean():.4f}\")\n",
    "\n",
    "# Provide recommendations\n",
    "print(\"\\nüí° Management Recommendations:\")\n",
    "\n",
    "print(f\"\\n   1. REBALANCING:\")\n",
    "print(f\"      ‚Ä¢ Frequency: Monthly or when weights drift > 5%\")\n",
    "print(f\"      ‚Ä¢ Method: Re-run clustering, update selections\")\n",
    "\n",
    "print(f\"\\n   2. MONITORING:\")\n",
    "print(f\"      ‚Ä¢ Track individual token performance\")\n",
    "print(f\"      ‚Ä¢ Watch for tokens changing clusters\")\n",
    "print(f\"      ‚Ä¢ Monitor portfolio Sharpe ratio\")\n",
    "\n",
    "print(f\"\\n   3. RISK MANAGEMENT:\")\n",
    "if portfolio_sharpe > 0.3:\n",
    "    print(f\"      ‚úÖ Good risk-adjusted returns (Sharpe={portfolio_sharpe:.3f})\")\n",
    "    print(f\"      ‚Üí Maintain current allocation\")\n",
    "else:\n",
    "    print(f\"      ‚ö†Ô∏è  Low risk-adjusted returns (Sharpe={portfolio_sharpe:.3f})\")\n",
    "    print(f\"      ‚Üí Consider increasing quality threshold\")\n",
    "\n",
    "print(f\"\\n   4. DIVERSIFICATION:\")\n",
    "cluster_concentration = (cluster_representation['Weight']**2).sum()\n",
    "print(f\"      ‚Ä¢ Cluster concentration: {cluster_concentration:.3f}\")\n",
    "if cluster_concentration < 0.30:\n",
    "    print(f\"      ‚úÖ Well diversified across clusters\")\n",
    "elif cluster_concentration < 0.40:\n",
    "    print(f\"      ‚ö†Ô∏è  Moderate concentration\")\n",
    "else:\n",
    "    print(f\"      ‚ùå High concentration - consider rebalancing\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Portfolio construction complete!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìã PORTFOLIO SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Total tokens: {len(portfolio_df)}\")\n",
    "print(f\"   ‚Ä¢ Clusters represented: {optimal_k}/{optimal_k}\")\n",
    "print(f\"   ‚Ä¢ Expected return: {portfolio_return*252:.2%} annually\")\n",
    "print(f\"   ‚Ä¢ Expected volatility: {annual_vol:.2%} annually\")\n",
    "print(f\"   ‚Ä¢ Sharpe ratio: {portfolio_sharpe:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96eaa04-d951-4377-90fb-abe96229d8c2",
   "metadata": {},
   "source": [
    "# Part 3: Factor Analysis (FA)\n",
    "\n",
    "## What is Factor Analysis and Why Do We Need It?\n",
    "\n",
    "### The Problem: Hidden Market Forces\n",
    "\n",
    "Once again, imagine you're tracking 50 DeFi tokens with 20 observable features each:\n",
    "- Price returns (daily, weekly, monthly)\n",
    "- Volatility measures\n",
    "- Trading volumes\n",
    "- TVL and market cap\n",
    "- Liquidity metrics\n",
    "- Sentiment scores\n",
    "- ... and more!\n",
    "\n",
    "**But what if these features are all influenced by a few hidden \"factors\"?** \n",
    "\n",
    "### The Solution: Factor Analysis\n",
    "\n",
    "**Factor Analysis (FA)** helps us by:\n",
    "- Identifying hidden **latent factors** that drive observable features\n",
    "- Discovering the underlying economic forces (like \"market sentiment\" or \"systemic risk\")\n",
    "- Explaining correlations between features through common factors\n",
    "- Reducing noise and isolating the true signals\n",
    "\n",
    "\n",
    "**Factor Analysis finds the latent factors that explain why features move together.**\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Latent Factors**: Hidden variables that cause observed correlations\n",
    "   - Example: In DeFi, a \"Market Risk Factor\" might drive both volatility and returns\n",
    "\n",
    "2. **Factor Loadings**: How much each feature is influenced by each factor\n",
    "   - Example: Returns might have high loading on \"Risk Factor\" but low on \"Size Factor\"\n",
    "\n",
    "3. **Uniqueness**: Feature variance NOT explained by common factors\n",
    "   - Example: Token-specific news that affects only one token\n",
    "\n",
    "4. **Common Variance**: Feature variance explained by shared factors\n",
    "   - Example: All tokens react to ETH price movement (common factor)\n",
    "\n",
    "---\n",
    "\n",
    "## The Maths\n",
    "\n",
    "**Step 1: Start with observed features**\n",
    "```\n",
    "Each feature = (Common factors) + (Unique factor) + (Error)\n",
    "\n",
    "Return = Œ≤‚ÇÅ*Factor1 + Œ≤‚ÇÇ*Factor2 + ... + Unique_Return + Error\n",
    "Volume = Œ≤‚ÇÅ*Factor1 + Œ≤‚ÇÇ*Factor2 + ... + Unique_Volume + Error\n",
    "...\n",
    "```\n",
    "\n",
    "**Step 2: Factor Model Structure**\n",
    "```\n",
    "X = L √ó F + U\n",
    "\n",
    "Where:\n",
    "X = Observable features (Returns, Volume, TVL, ...)\n",
    "L = Factor loadings (how factors influence features)\n",
    "F = Latent factors (hidden market forces)\n",
    "U = Unique factors (token-specific effects)\n",
    "```\n",
    "\n",
    "**Step 3: Estimate factors and loadings**\n",
    "- Factor Analysis calculates which factors best explain feature correlations\n",
    "- Uses eigenvalue decomposition or maximum likelihood estimation\n",
    "\n",
    "**Step 4: Interpret the factors**\n",
    "```\n",
    "Factor 1: High loadings on Volume, TVL, Market Cap\n",
    "          ‚Üí \"Size Factor\" (large vs small protocols)\n",
    "\n",
    "Factor 2: High loadings on Returns, Volatility\n",
    "          ‚Üí \"Risk Factor\" (risky vs stable tokens)\n",
    "\n",
    "Factor 3: High loadings on ETH correlation, Sentiment\n",
    "          ‚Üí \"Market Sentiment Factor\" (bullish vs bearish)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Factor Analysis vs. PCA: What's the Difference?\n",
    "\n",
    "| Aspect | PCA | Factor Analysis |\n",
    "|--------|-----|-----------------|\n",
    "| **Goal** | Reduce dimensions | Find latent causes |\n",
    "| **Focus** | Total variance | Common variance only |\n",
    "| **Model** | Descriptive (no causal model) | Causal (factors ‚Üí features) |\n",
    "| **Variance** | Explains all variance | Explains only shared variance |\n",
    "| **Uniqueness** | No concept of uniqueness | Models unique variance |\n",
    "| **Interpretation** | Components are combinations | Factors are underlying causes |\n",
    "\n",
    "**Simple Analogy:**\n",
    "- **PCA**: \"Let me summarize this data efficiently\"\n",
    "- **FA**: \"Let me find what's causing these patterns\"\n",
    "\n",
    "---\n",
    "\n",
    "## üí° When Should You Use Factor Analysis in DeFi?\n",
    "\n",
    "‚úÖ **USE FA when you want to:**\n",
    "- Identify hidden market forces driving token behavior\n",
    "- Understand causal relationships between features\n",
    "- Model systematic vs. idiosyncratic risk\n",
    "- Test economic theories (Does a \"DeFi Risk Factor\" exist?)\n",
    "- Build factor-based trading strategies\n",
    "- Explain feature correlations through latent causes\n",
    "\n",
    "‚ùå **USE PCA instead when you want to:**\n",
    "- Simply reduce dimensions for visualization\n",
    "- Create uncorrelated features for ML models\n",
    "- Don't care about causality, just variance explanation\n",
    "- Need faster computation\n",
    "\n",
    "---\n",
    "\n",
    "## ü¶Å Real DeFi Applications of Factor Analysis\n",
    "\n",
    "1. **Systematic Risk Decomposition**\n",
    "   - Separate market-wide risk (factors) from token-specific risk (uniqueness)\n",
    "   - Build hedging strategies based on factor exposures\n",
    "\n",
    "2. **Factor-Based Portfolio Construction**\n",
    "   - Identify \"Size Factor,\" \"Momentum Factor,\" \"Value Factor\" in DeFi\n",
    "   - Construct portfolios with desired factor exposures (like Fama-French models)\n",
    "\n",
    "3. **Risk Attribution**\n",
    "   - Decompose portfolio risk into factor risks\n",
    "   - Understand which factors contribute most to portfolio volatility\n",
    "\n",
    "4. **Market Structure Analysis**\n",
    "   - Discover how many independent factors drive the DeFi market\n",
    "   - Test if DeFi is dominated by few factors (high correlation) or many (diversification opportunity)\n",
    "\n",
    "5. **Regime Detection**\n",
    "   - Monitor factor loadings over time\n",
    "   - Detect when market structure changes (e.g., from \"momentum-driven\" to \"value-driven\")\n",
    "\n",
    "6. **Alpha Generation**\n",
    "   - Find tokens with high unique variance (less influenced by common factors)\n",
    "   - Potential for uncorrelated returns and alpha\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key FA Concepts for Finance\n",
    "\n",
    "### 1. **Factor Scores**\n",
    "- Each token gets a score on each factor\n",
    "- High score on \"Risk Factor\" = token is risky\n",
    "- Use scores for portfolio optimization\n",
    "\n",
    "### 2. **Communality**\n",
    "- Proportion of feature variance explained by ALL factors\n",
    "- High communality = feature is well-explained by common factors\n",
    "- Low communality = feature is mostly unique/idiosyncratic\n",
    "\n",
    "### 3. **Uniqueness**\n",
    "- Proportion NOT explained by factors (1 - communality)\n",
    "- High uniqueness = token has unique characteristics\n",
    "- Potentially offers diversification benefits!\n",
    "\n",
    "### 4. **Factor Rotation**\n",
    "- Make factors easier to interpret\n",
    "- **Varimax**: Maximizes loading variance (simple structure)\n",
    "- **Promax**: Allows correlated factors (more realistic for finance)\n",
    "\n",
    "---\n",
    "\n",
    "## üìà The Factor Analysis Process in DeFi\n",
    "\n",
    "**Step 1: Prepare Data**\n",
    "- Standardize features (mean=0, std=1)\n",
    "- Handle missing values\n",
    "- Check correlation matrix (FA needs correlated features!)\n",
    "\n",
    "**Step 2: Determine Number of Factors**\n",
    "- Scree plot (like PCA)\n",
    "- Kaiser criterion (eigenvalues > 1)\n",
    "- Parallel analysis\n",
    "- Economic theory (e.g., Fama-French uses 3-5 factors)\n",
    "\n",
    "**Step 3: Fit Factor Model**\n",
    "- Extract factors using maximum likelihood or principal factors\n",
    "- Rotate factors for better interpretation\n",
    "\n",
    "**Step 4: Interpret Factors**\n",
    "- Examine loadings to understand each factor\n",
    "- Name factors based on high-loading features\n",
    "- Validate with economic intuition\n",
    "\n",
    "**Step 5: Calculate Factor Scores**\n",
    "- Transform tokens into factor space\n",
    "- Use scores for portfolio decisions\n",
    "\n",
    "**Step 6: Analyze Uniqueness**\n",
    "- Identify tokens with high unique variance\n",
    "- Consider for diversification\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Assumptions and Limitations\n",
    "\n",
    "### Assumptions:\n",
    "\n",
    "‚úÖ **Features are correlated** (FA needs correlation!)\n",
    "‚úÖ **Linear relationships** between factors and features\n",
    "‚úÖ **Normally distributed** features (for ML estimation)\n",
    "‚úÖ **Factors are latent causes** (causal interpretation makes sense)\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "‚ùå **Factor interpretation is subjective** (rotation choices matter)\n",
    "‚ùå **Requires more data than PCA** (estimate more parameters)\n",
    "‚ùå **Sensitive to outliers** (especially with ML estimation)\n",
    "‚ùå **Assumes factors are few** (doesn't work well with many factors)\n",
    "\n",
    "### When FA Might Not Work Well:\n",
    "- Features are already uncorrelated (no common factors!)\n",
    "- Sample size is small (need n >> number of features)\n",
    "- Features have non-linear relationships\n",
    "- You just want dimension reduction (use PCA instead)\n",
    "\n",
    "---\n",
    "\n",
    "##  Example: Fama-French Factors in Traditional Finance\n",
    "\n",
    "**Traditional Equity Markets use these factors:**\n",
    "1. **Market Factor**: Overall market return (beta)\n",
    "2. **Size Factor (SMB)**: Small minus Big stocks\n",
    "3. **Value Factor (HML)**: High minus Low book-to-market\n",
    "4. **Momentum Factor**: Winners minus Losers\n",
    "5. **Quality Factor**: Profitable minus Unprofitable\n",
    "\n",
    "**In DeFi, we might find similar factors:**\n",
    "1. **DeFi Market Factor**: Overall DeFi ecosystem movement\n",
    "2. **Protocol Size Factor**: Large TVL vs. Small TVL\n",
    "3. **DeFi Category Factor**: DEX vs. Lending vs. Yield\n",
    "4. **Risk Factor**: Stable vs. Volatile protocols\n",
    "5. **Innovation Factor**: Established vs. New protocols\n",
    "\n",
    "---\n",
    "\n",
    "## Statistical Validation\n",
    "\n",
    "**How do we know our factors are real?**\n",
    "\n",
    "1. **Bartlett's Test of Sphericity**\n",
    "   - Tests if features are correlated enough for FA\n",
    "   - p-value < 0.05 = good for FA\n",
    "\n",
    "2. **KMO (Kaiser-Meyer-Olkin) Test**\n",
    "   - Measures sampling adequacy\n",
    "   - KMO > 0.6 = acceptable for FA\n",
    "   - KMO > 0.8 = great for FA\n",
    "\n",
    "3. **Communalities Check**\n",
    "   - All communalities > 0.5 = factors explain features well\n",
    "   - Some < 0.3 = might need different factors or features\n",
    "\n",
    "---\n",
    "\n",
    "## üé® Visualization Techniques\n",
    "\n",
    "**Factor Analysis offers unique visualizations:**\n",
    "\n",
    "1. **Factor Loading Plot**: Show feature-factor relationships\n",
    "2. **Communality Bar Chart**: See how well each feature is explained\n",
    "3. **Factor Score Scatter**: Plot tokens in factor space\n",
    "4. **Uniqueness Heatmap**: Identify unique vs. common drivers\n",
    "5. **Loading Heatmap**: Matrix of all loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739854c-71c0-44fe-9065-fae0c1702141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFactorAnalysis:\n",
    "    \"\"\"\n",
    "    Factor Analysis for DeFi Portfolio Analysis\n",
    "    \n",
    "    This class wraps sklearn's FactorAnalysis with helpful methods for:\n",
    "    - Fitting FA to DeFi data\n",
    "    - Identifying latent market factors\n",
    "    - Analyzing factor loadings and uniqueness\n",
    "    - Calculating factor scores for tokens\n",
    "    - Visualizing results\n",
    "    - Testing model assumptions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_factors=None, rotation='varimax'):\n",
    "        \"\"\"\n",
    "        Initialize Factor Analysis\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_factors : int or None\n",
    "            Number of latent factors to extract\n",
    "            - None = use eigenvalue > 1 criterion\n",
    "            - int = extract specific number (e.g., 3)\n",
    "            - Tip: Start with None, then choose based on scree plot\n",
    "        rotation : str, optional\n",
    "            Type of factor rotation for interpretation\n",
    "            - 'varimax': Orthogonal rotation (uncorrelated factors)\n",
    "            - 'promax': Oblique rotation (allows correlated factors)\n",
    "            - 'none': No rotation\n",
    "        \"\"\"\n",
    "        self.n_factors = n_factors\n",
    "        self.rotation = rotation\n",
    "        self.scaler = StandardScaler()  # For standardizing features\n",
    "        self.fa = None  # Will hold the fitted FA model\n",
    "        self.feature_names = None  # To remember original feature names\n",
    "        self.loadings_rotated = None  # Rotated loadings (if rotation applied)\n",
    "        \n",
    "    def fit(self, X, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit Factor Analysis model to data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data matrix (e.g., 50 tokens √ó 8 features)\n",
    "        feature_names : list of str, optional\n",
    "            Names of features for better interpretation\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        # Step 1: Standardize features (CRITICAL for FA!)\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Step 2: Determine number of factors if not specified\n",
    "        if self.n_factors is None:\n",
    "            # Use Kaiser criterion: eigenvalues > 1\n",
    "            correlation_matrix = np.corrcoef(X_scaled.T)\n",
    "            eigenvalues = np.linalg.eigvals(correlation_matrix)\n",
    "            self.n_factors = np.sum(eigenvalues > 1.0)\n",
    "            print(f\"\\nüìä Kaiser criterion suggests {self.n_factors} factors (eigenvalues > 1)\")\n",
    "        \n",
    "        # Step 3: Fit Factor Analysis\n",
    "        self.fa = FactorAnalysis(n_components=self.n_factors, random_state=42)\n",
    "        self.fa.fit(X_scaled)\n",
    "        \n",
    "        # Step 4: Apply rotation if specified\n",
    "        if self.rotation == 'varimax':\n",
    "            self.loadings_rotated = self._varimax_rotation(self.fa.components_.T)\n",
    "            print(f\"   ‚úÖ Applied Varimax rotation for better interpretability\")\n",
    "        else:\n",
    "            self.loadings_rotated = self.fa.components_.T\n",
    "        \n",
    "        # Step 5: Save feature names\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n‚úÖ Factor Analysis fitted successfully!\")\n",
    "        print(f\"   Factors extracted: {self.n_factors}\")\n",
    "        print(f\"   Log-likelihood: {self.fa.score(X_scaled):.2f}\")\n",
    "        \n",
    "        # Calculate and display communalities\n",
    "        communalities = self._calculate_communalities()\n",
    "        mean_communality = np.mean(communalities)\n",
    "        print(f\"   Mean communality: {mean_communality:.2%} (variance explained by factors)\")\n",
    "        print(f\"   Mean uniqueness: {(1-mean_communality):.2%} (unique variance)\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data to factor score space\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data to transform\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        factor_scores : array, shape (n_samples, n_factors)\n",
    "            Factor scores for each observation\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.fa.transform(X_scaled)\n",
    "    \n",
    "    def fit_transform(self, X, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit Factor Analysis and transform data in one step\n",
    "        \"\"\"\n",
    "        self.fit(X, feature_names)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def get_loadings(self, rotated=True):\n",
    "        \"\"\"\n",
    "        Get factor loadings (how features relate to factors)\n",
    "        \n",
    "        Loadings tell us which features are influenced by which factors\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        rotated : bool, default=True\n",
    "            Return rotated loadings if rotation was applied\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with features as rows, factors as columns\n",
    "        \n",
    "        Interpretation:\n",
    "        - High positive loading: Feature increases with factor\n",
    "        - High negative loading: Feature decreases with factor  \n",
    "        - Near-zero loading: Feature unrelated to factor\n",
    "        \"\"\"\n",
    "        if rotated and self.loadings_rotated is not None:\n",
    "            loadings_matrix = self.loadings_rotated\n",
    "        else:\n",
    "            loadings_matrix = self.fa.components_.T\n",
    "        \n",
    "        loadings = pd.DataFrame(\n",
    "            loadings_matrix,\n",
    "            columns=[f'Factor{i+1}' for i in range(self.n_factors)],\n",
    "            index=self.feature_names if self.feature_names else range(len(loadings_matrix))\n",
    "        )\n",
    "        return loadings\n",
    "    \n",
    "    def get_communalities(self):\n",
    "        \"\"\"\n",
    "        Get communalities for each feature\n",
    "        \n",
    "        Communality = variance explained by ALL factors\n",
    "        Range: 0 to 1\n",
    "        - High (>0.7): Feature well-explained by common factors\n",
    "        - Moderate (0.4-0.7): Partial common variance\n",
    "        - Low (<0.4): Mostly unique/idiosyncratic variance\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Series with feature communalities\n",
    "        \"\"\"\n",
    "        communalities = self._calculate_communalities()\n",
    "        \n",
    "        return pd.Series(\n",
    "            communalities,\n",
    "            index=self.feature_names if self.feature_names else range(len(communalities)),\n",
    "            name='Communality'\n",
    "        )\n",
    "    \n",
    "    def get_uniqueness(self):\n",
    "        \"\"\"\n",
    "        Get uniqueness for each feature\n",
    "        \n",
    "        Uniqueness = 1 - Communality = variance NOT explained by factors\n",
    "        \n",
    "        High uniqueness = feature has token-specific behavior\n",
    "        (Good for diversification!)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Series with feature uniqueness\n",
    "        \"\"\"\n",
    "        communalities = self.get_communalities()\n",
    "        uniqueness = 1 - communalities\n",
    "        uniqueness.name = 'Uniqueness'\n",
    "        return uniqueness\n",
    "    \n",
    "    def get_factor_summary(self):\n",
    "        \"\"\"\n",
    "        Get summary statistics for each factor\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with variance explained by each factor\n",
    "        \"\"\"\n",
    "        # Calculate variance explained by each factor\n",
    "        loadings = self.get_loadings(rotated=False)\n",
    "        variance_explained = np.sum(loadings**2, axis=0) / len(loadings)\n",
    "        \n",
    "        # Create cumulative variance\n",
    "        cumulative_variance = np.cumsum(variance_explained)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Factor': [f'Factor{i+1}' for i in range(self.n_factors)],\n",
    "            'Variance_Explained': variance_explained,\n",
    "            'Cumulative_Variance': cumulative_variance,\n",
    "            'Percentage': variance_explained * 100\n",
    "        })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def test_assumptions(self, X):\n",
    "        \"\"\"\n",
    "        Test if data is suitable for Factor Analysis\n",
    "        \n",
    "        Returns KMO and Bartlett's test results\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Bartlett's test of sphericity\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FACTOR ANALYSIS ASSUMPTION TESTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Correlation matrix\n",
    "        corr_matrix = np.corrcoef(X_scaled.T)\n",
    "        \n",
    "        # Check if any correlations exist\n",
    "        avg_corr = np.mean(np.abs(corr_matrix[np.triu_indices_from(corr_matrix, k=1)]))\n",
    "        print(f\"\\nüìä Average absolute correlation: {avg_corr:.3f}\")\n",
    "        \n",
    "        if avg_corr < 0.3:\n",
    "            print(\"   ‚ö†Ô∏è  WARNING: Low correlations - FA may not be appropriate\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Sufficient correlations for Factor Analysis\")\n",
    "        \n",
    "        # Simple KMO approximation (full KMO requires specialized library)\n",
    "        det_corr = np.linalg.det(corr_matrix)\n",
    "        print(f\"\\nüìä Correlation matrix determinant: {det_corr:.6f}\")\n",
    "        \n",
    "        if det_corr < 0.00001:\n",
    "            print(\"   ‚úÖ Strong correlations present (good for FA)\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Weak correlations (consider if FA is needed)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    def _calculate_communalities(self):\n",
    "        \"\"\"\n",
    "        Calculate communalities (internal method)\n",
    "        \"\"\"\n",
    "        loadings = self.loadings_rotated if self.loadings_rotated is not None else self.fa.components_.T\n",
    "        communalities = np.sum(loadings**2, axis=1)\n",
    "        return communalities\n",
    "    \n",
    "    def _varimax_rotation(self, loadings, gamma=1.0, max_iter=100, tol=1e-5):\n",
    "        \"\"\"\n",
    "        Apply Varimax rotation to factor loadings\n",
    "        \n",
    "        Varimax maximizes variance of squared loadings\n",
    "        This creates a \"simple structure\" - each feature loads highly on few factors\n",
    "        \"\"\"\n",
    "        n_features, n_factors = loadings.shape\n",
    "        rotation_matrix = np.eye(n_factors)\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            # Compute rotated loadings\n",
    "            rotated = loadings @ rotation_matrix\n",
    "            \n",
    "            # Compute gradient for Varimax\n",
    "            u, s, vh = np.linalg.svd(loadings.T @ (rotated**3 - gamma * rotated @ np.diag(np.sum(rotated**2, axis=0)) / n_features))\n",
    "            rotation_matrix_new = u @ vh\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.allclose(rotation_matrix, rotation_matrix_new, atol=tol):\n",
    "                break\n",
    "            \n",
    "            rotation_matrix = rotation_matrix_new\n",
    "        \n",
    "        return loadings @ rotation_matrix\n",
    "    \n",
    "    def plot_scree(self, X, max_factors=None, figsize=(14, 5)):\n",
    "        \"\"\"\n",
    "        Create scree plot to help choose optimal number of factors\n",
    "        \n",
    "        Shows eigenvalues to determine how many factors to extract\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        correlation_matrix = np.corrcoef(X_scaled.T)\n",
    "        eigenvalues = np.linalg.eigvals(correlation_matrix)\n",
    "        eigenvalues = np.sort(eigenvalues)[::-1]  # Sort descending\n",
    "        \n",
    "        if max_factors is None:\n",
    "            max_factors = len(eigenvalues)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Left plot: Eigenvalues (scree plot)\n",
    "        axes[0].plot(range(1, max_factors+1), eigenvalues[:max_factors], \n",
    "                    'bo-', linewidth=2, markersize=8)\n",
    "        axes[0].axhline(y=1.0, color='red', linestyle='--', linewidth=2, \n",
    "                       label='Kaiser criterion (eigenvalue = 1)')\n",
    "        axes[0].set_xlabel('Factor Number', fontsize=12)\n",
    "        axes[0].set_ylabel('Eigenvalue', fontsize=12)\n",
    "        axes[0].set_title('Scree Plot: Factor Selection', fontsize=13, fontweight='bold')\n",
    "        axes[0].legend(fontsize=10)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Right plot: Cumulative variance\n",
    "        cumulative_variance = np.cumsum(eigenvalues / np.sum(eigenvalues))\n",
    "        axes[1].plot(range(1, max_factors+1), cumulative_variance[:max_factors], \n",
    "                    'go-', linewidth=2, markersize=8)\n",
    "        axes[1].axhline(y=0.80, color='orange', linestyle='--', linewidth=2, label='80% threshold')\n",
    "        axes[1].set_xlabel('Number of Factors', fontsize=12)\n",
    "        axes[1].set_ylabel('Cumulative Proportion of Variance', fontsize=12)\n",
    "        axes[1].set_title('Cumulative Variance Explained', fontsize=13, fontweight='bold')\n",
    "        axes[1].legend(fontsize=10)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_ylim([0, 1.05])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_loadings(self, n_factors=None, n_top_features=10, figsize=(15, 5)):\n",
    "        \"\"\"\n",
    "        Visualize factor loadings for interpretation\n",
    "        \n",
    "        Shows which features are most important for each factor\n",
    "        \"\"\"\n",
    "        loadings = self.get_loadings(rotated=True)\n",
    "        \n",
    "        if n_factors is None:\n",
    "            n_factors = min(3, self.n_factors)\n",
    "        else:\n",
    "            n_factors = min(n_factors, self.n_factors)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, n_factors, figsize=figsize)\n",
    "        if n_factors == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i in range(n_factors):\n",
    "            factor_col = f'Factor{i+1}'\n",
    "            \n",
    "            # Get top features by absolute loading\n",
    "            top_features = loadings[factor_col].abs().nlargest(n_top_features)\n",
    "            sorted_loadings = loadings.loc[top_features.index, factor_col].sort_values()\n",
    "            \n",
    "            # Color code: red=negative, blue=positive\n",
    "            colors = ['red' if x < 0 else 'blue' for x in sorted_loadings.values]\n",
    "            \n",
    "            axes[i].barh(range(len(sorted_loadings)), sorted_loadings.values, \n",
    "                        color=colors, alpha=0.7)\n",
    "            axes[i].set_yticks(range(len(sorted_loadings)))\n",
    "            axes[i].set_yticklabels(sorted_loadings.index, fontsize=9)\n",
    "            axes[i].set_xlabel('Factor Loading', fontsize=11)\n",
    "            axes[i].set_title(f'{factor_col}\\n(Latent Factor)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "            axes[i].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "            axes[i].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_communalities(self, figsize=(12, 6)):\n",
    "        \"\"\"\n",
    "        Visualize communalities and uniqueness for each feature\n",
    "        \"\"\"\n",
    "        communalities = self.get_communalities()\n",
    "        uniqueness = self.get_uniqueness()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Left plot: Communalities\n",
    "        communalities_sorted = communalities.sort_values(ascending=True)\n",
    "        colors_comm = ['green' if x > 0.7 else 'orange' if x > 0.4 else 'red' \n",
    "                       for x in communalities_sorted.values]\n",
    "        \n",
    "        axes[0].barh(range(len(communalities_sorted)), communalities_sorted.values, \n",
    "                    color=colors_comm, alpha=0.7)\n",
    "        axes[0].set_yticks(range(len(communalities_sorted)))\n",
    "        axes[0].set_yticklabels(communalities_sorted.index, fontsize=9)\n",
    "        axes[0].set_xlabel('Communality', fontsize=11)\n",
    "        axes[0].set_title('Feature Communalities\\n(Common Variance)', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].axvline(x=0.7, color='green', linestyle='--', alpha=0.5, label='High (>0.7)')\n",
    "        axes[0].axvline(x=0.4, color='orange', linestyle='--', alpha=0.5, label='Moderate (>0.4)')\n",
    "        axes[0].legend(fontsize=8)\n",
    "        axes[0].set_xlim([0, 1])\n",
    "        axes[0].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Right plot: Uniqueness\n",
    "        uniqueness_sorted = uniqueness.sort_values(ascending=False)\n",
    "        colors_uniq = ['purple' if x > 0.6 else 'blue' if x > 0.3 else 'lightblue' \n",
    "                       for x in uniqueness_sorted.values]\n",
    "        \n",
    "        axes[1].barh(range(len(uniqueness_sorted)), uniqueness_sorted.values, \n",
    "                    color=colors_uniq, alpha=0.7)\n",
    "        axes[1].set_yticks(range(len(uniqueness_sorted)))\n",
    "        axes[1].set_yticklabels(uniqueness_sorted.index, fontsize=9)\n",
    "        axes[1].set_xlabel('Uniqueness', fontsize=11)\n",
    "        axes[1].set_title('Feature Uniqueness\\n(Unique Variance)', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].axvline(x=0.6, color='purple', linestyle='--', alpha=0.5, label='High (>0.6)')\n",
    "        axes[1].axvline(x=0.3, color='blue', linestyle='--', alpha=0.5, label='Moderate (>0.3)')\n",
    "        axes[1].legend(fontsize=8)\n",
    "        axes[1].set_xlim([0, 1])\n",
    "        axes[1].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_factor_scores(self, factor_scores, labels=None, figsize=(12, 6)):\n",
    "        \"\"\"\n",
    "        Visualize tokens in factor space (2D projection)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        factor_scores : array-like\n",
    "            Factor scores from transform()\n",
    "        labels : array-like, optional\n",
    "            Cluster labels for color coding\n",
    "        \"\"\"\n",
    "        if factor_scores.shape[1] < 2:\n",
    "            print(\"‚ö†Ô∏è  Need at least 2 factors for 2D visualization\")\n",
    "            return None\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Plot Factor1 vs Factor2\n",
    "        if labels is not None:\n",
    "            scatter = axes[0].scatter(factor_scores[:, 0], factor_scores[:, 1], \n",
    "                                     c=labels, cmap='tab10', s=100, alpha=0.6)\n",
    "            plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "        else:\n",
    "            axes[0].scatter(factor_scores[:, 0], factor_scores[:, 1], \n",
    "                          c='blue', s=100, alpha=0.6)\n",
    "        \n",
    "        axes[0].set_xlabel('Factor 1 Score', fontsize=11)\n",
    "        axes[0].set_ylabel('Factor 2 Score', fontsize=11)\n",
    "        axes[0].set_title('Tokens in Factor Space\\n(Factor 1 vs Factor 2)', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "        axes[0].axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot Factor1 vs Factor3 (if exists)\n",
    "        if factor_scores.shape[1] >= 3:\n",
    "            if labels is not None:\n",
    "                scatter = axes[1].scatter(factor_scores[:, 0], factor_scores[:, 2], \n",
    "                                         c=labels, cmap='tab10', s=100, alpha=0.6)\n",
    "                plt.colorbar(scatter, ax=axes[1], label='Cluster')\n",
    "            else:\n",
    "                axes[1].scatter(factor_scores[:, 0], factor_scores[:, 2], \n",
    "                              c='green', s=100, alpha=0.6)\n",
    "            \n",
    "            axes[1].set_xlabel('Factor 1 Score', fontsize=11)\n",
    "            axes[1].set_ylabel('Factor 3 Score', fontsize=11)\n",
    "            axes[1].set_title('Tokens in Factor Space\\n(Factor 1 vs Factor 3)', \n",
    "                             fontsize=12, fontweight='bold')\n",
    "            axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "            axes[1].axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'Need 3+ factors\\nfor this plot', \n",
    "                        ha='center', va='center', fontsize=14, \n",
    "                        transform=axes[1].transAxes)\n",
    "            axes[1].set_title('Factor 1 vs Factor 3', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ SimpleFactorAnalysis class created successfully!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAvailable methods:\")\n",
    "print(\"  ‚Ä¢ fit(X, feature_names) - Fit Factor Analysis to data\")\n",
    "print(\"  ‚Ä¢ transform(X) - Get factor scores\")\n",
    "print(\"  ‚Ä¢ fit_transform(X, feature_names) - Fit and transform in one step\")\n",
    "print(\"  ‚Ä¢ get_loadings() - Get factor loadings\")\n",
    "print(\"  ‚Ä¢ get_communalities() - Get common variance for each feature\")\n",
    "print(\"  ‚Ä¢ get_uniqueness() - Get unique variance for each feature\")\n",
    "print(\"  ‚Ä¢ get_factor_summary() - Get variance explained by factors\")\n",
    "print(\"  ‚Ä¢ test_assumptions(X) - Test if FA is appropriate\")\n",
    "print(\"  ‚Ä¢ plot_scree(X) - Visualize eigenvalues for factor selection\")\n",
    "print(\"  ‚Ä¢ plot_loadings() - Visualize factor loadings\")\n",
    "print(\"  ‚Ä¢ plot_communalities() - Visualize communalities and uniqueness\")\n",
    "print(\"  ‚Ä¢ plot_factor_scores() - Visualize tokens in factor space\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e799e9-47b2-4980-af9f-1061090f889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FITTING FACTOR ANALYSIS TO DeFi PORTFOLIO DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Test if our data is suitable for Factor Analysis\n",
    "print(\"\\nüî¨ STEP 1: Testing Factor Analysis Assumptions\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "fa_model = SimpleFactorAnalysis()\n",
    "fa_model.test_assumptions(defi_data.values)\n",
    "\n",
    "# Step 2: Determine optimal number of factors\n",
    "print(\"\\nüìä STEP 2: Determining Optimal Number of Factors\")\n",
    "print(\"-\"*70)\n",
    "print(\"\\nüéØ Creating scree plot to guide factor selection...\")\n",
    "\n",
    "# Create scree plot\n",
    "fig = fa_model.plot_scree(defi_data.values, max_factors=8, figsize=(14, 5))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° How to interpret the scree plot:\")\n",
    "print(\"   ‚Ä¢ Left plot shows eigenvalues for each potential factor\")\n",
    "print(\"   ‚Ä¢ Kaiser criterion: Keep factors with eigenvalue > 1.0 (red line)\")\n",
    "print(\"   ‚Ä¢ Look for the 'elbow' where eigenvalues flatten out\")\n",
    "print(\"   ‚Ä¢ Right plot shows cumulative variance explained\")\n",
    "print(\"   ‚Ä¢ Goal: Explain 70-80%+ of common variance\")\n",
    "\n",
    "# Step 3: Fit Factor Analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: Fitting Factor Analysis Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüîß Initializing Factor Analysis model...\")\n",
    "print(\"   Using automatic factor selection (Kaiser criterion)\")\n",
    "print(\"   Applying Varimax rotation for interpretability\")\n",
    "\n",
    "# Initialize FA (will auto-detect number of factors)\n",
    "fa_model = SimpleFactorAnalysis(n_factors=None, rotation='varimax')\n",
    "\n",
    "# Fit FA and transform data\n",
    "print(\"\\n‚öôÔ∏è  Fitting Factor Analysis and transforming data...\")\n",
    "print(\"   This will:\")\n",
    "print(\"   1. Standardize all features (mean=0, std=1)\")\n",
    "print(\"   2. Determine optimal number of factors\")\n",
    "print(\"   3. Extract latent factors using maximum likelihood\")\n",
    "print(\"   4. Apply Varimax rotation\")\n",
    "print(\"   5. Calculate factor scores for each token\")\n",
    "print(\"   6. Compute communalities and uniqueness\")\n",
    "\n",
    "factor_scores = fa_model.fit_transform(\n",
    "    defi_data.values,\n",
    "    feature_names=defi_data.columns.tolist()\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRANSFORMATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show dimensionality transformation\n",
    "print(f\"\\nüìä Factor Space Transformation:\")\n",
    "print(f\"   Original shape: {defi_data.shape[0]} tokens √ó {defi_data.shape[1]} features\")\n",
    "print(f\"   Factor scores:  {factor_scores.shape[0]} tokens √ó {factor_scores.shape[1]} factors\")\n",
    "print(f\"   Reduction: {defi_data.shape[1]} ‚Üí {factor_scores.shape[1]} latent dimensions\")\n",
    "\n",
    "# Get factor summary\n",
    "factor_summary = fa_model.get_factor_summary()\n",
    "\n",
    "print(f\"\\nüìà Variance Explained by Each Factor:\")\n",
    "print(\"=\"*70)\n",
    "display(factor_summary.round(4))\n",
    "\n",
    "# Get communalities\n",
    "communalities = fa_model.get_communalities()\n",
    "uniqueness = fa_model.get_uniqueness()\n",
    "\n",
    "print(f\"\\nüîç Communality Analysis:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nCommunality = Variance explained by ALL factors\")\n",
    "print(\"Uniqueness  = Variance NOT explained by factors (token-specific)\\n\")\n",
    "\n",
    "# Create summary table\n",
    "comm_summary = pd.DataFrame({\n",
    "    'Feature': communalities.index,\n",
    "    'Communality': communalities.values,\n",
    "    'Uniqueness': uniqueness.values,\n",
    "    'Interpretation': ['High common' if c > 0.7 else 'Moderate' if c > 0.4 else 'Mostly unique' \n",
    "                      for c in communalities.values]\n",
    "})\n",
    "comm_summary = comm_summary.sort_values('Communality', ascending=False)\n",
    "display(comm_summary.round(4))\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(f\"   ‚Ä¢ Mean communality: {communalities.mean():.1%}\")\n",
    "print(f\"   ‚Ä¢ Mean uniqueness: {uniqueness.mean():.1%}\")\n",
    "\n",
    "# Identify features with high communality\n",
    "high_comm = communalities[communalities > 0.7]\n",
    "if len(high_comm) > 0:\n",
    "    print(f\"\\n   ‚úÖ Features with HIGH communality (>70%):\")\n",
    "    for feat in high_comm.index:\n",
    "        print(f\"      ‚Ä¢ {feat}: {communalities[feat]:.1%} (driven by common factors)\")\n",
    "\n",
    "# Identify features with high uniqueness\n",
    "high_uniq = uniqueness[uniqueness > 0.6]\n",
    "if len(high_uniq) > 0:\n",
    "    print(f\"\\n   üéØ Features with HIGH uniqueness (>60%):\")\n",
    "    for feat in high_uniq.index:\n",
    "        print(f\"      ‚Ä¢ {feat}: {uniqueness[feat]:.1%} (token-specific behavior)\")\n",
    "    print(f\"      ‚Üí These features offer diversification potential!\")\n",
    "\n",
    "# Show sample of factor scores\n",
    "print(f\"\\nüìã Sample Factor Scores (First 5 Tokens):\")\n",
    "print(\"=\"*70)\n",
    "factor_scores_df = pd.DataFrame(\n",
    "    factor_scores[:5],\n",
    "    columns=[f'Factor{i+1}' for i in range(factor_scores.shape[1])],\n",
    "    index=defi_data.index[:5]\n",
    ")\n",
    "display(factor_scores_df.round(3))\n",
    "\n",
    "print(\"\\nüí° Understanding Factor Scores:\")\n",
    "print(\"   ‚Ä¢ Each row represents a token\")\n",
    "print(\"   ‚Ä¢ Each column represents a latent factor\")\n",
    "print(\"   ‚Ä¢ Positive score = token has high exposure to that factor\")\n",
    "print(\"   ‚Ä¢ Negative score = token has low exposure to that factor\")\n",
    "print(\"   ‚Ä¢ Score magnitude = strength of relationship\")\n",
    "\n",
    "# Compare variance retention: Total vs Common\n",
    "total_var = factor_summary['Cumulative_Variance'].iloc[-1]\n",
    "print(f\"\\nüìä Variance Analysis:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Factor Analysis explains: {total_var:.1%} of common variance\")\n",
    "print(f\"   Average communality: {communalities.mean():.1%}\")\n",
    "print(f\"   Average uniqueness: {uniqueness.mean():.1%}\")\n",
    "\n",
    "if total_var >= 0.80:\n",
    "    print(f\"\\n   ‚úÖ Excellent! Factors capture {total_var:.1%} of common patterns\")\n",
    "elif total_var >= 0.70:\n",
    "    print(f\"\\n   ‚úÖ Good! Factors capture {total_var:.1%} of common patterns\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Only {total_var:.1%} of common variance - consider more factors\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Factor Analysis Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüëâ Next: Visualize and interpret the factor loadings\")\n",
    "\n",
    "# STEP 4: Visualize Factor Loadings\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: Analyzing Factor Loadings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüé® Creating factor loading visualizations...\")\n",
    "fig = fa_model.plot_loadings(n_factors=min(3, fa_model.n_factors), \n",
    "                              n_top_features=8, \n",
    "                              figsize=(15, 5))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Factor Loading Analysis:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get loadings\n",
    "loadings = fa_model.get_loadings(rotated=True)\n",
    "display(loadings.round(3))\n",
    "\n",
    "print(\"\\nüí° How to Interpret Loadings:\")\n",
    "print(\"   ‚Ä¢ |Loading| > 0.7 = STRONG relationship (feature defines this factor)\")\n",
    "print(\"   ‚Ä¢ |Loading| = 0.4-0.7 = MODERATE relationship (contributes)\")\n",
    "print(\"   ‚Ä¢ |Loading| < 0.4 = WEAK relationship (minimal influence)\")\n",
    "print(\"   ‚Ä¢ Positive (+) = feature increases with factor\")\n",
    "print(\"   ‚Ä¢ Negative (-) = feature decreases with factor\")\n",
    "\n",
    "# Analyze and name each factor\n",
    "print(\"\\nüè∑Ô∏è  FACTOR INTERPRETATION:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(loadings.shape[1]):\n",
    "    factor_name = f'Factor{i+1}'\n",
    "    factor_loadings = loadings[factor_name].abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\n{factor_name}:\")\n",
    "    print(f\"   Variance explained: {factor_summary['Percentage'].iloc[i]:.1f}%\")\n",
    "    print(f\"\\n   Top features (by absolute loading):\")\n",
    "    \n",
    "    for j, (feat, loading) in enumerate(loadings[factor_name].items(), 1):\n",
    "        if abs(loading) > 0.4:  # Only show significant loadings\n",
    "            direction = \"‚Üë\" if loading > 0 else \"‚Üì\"\n",
    "            strength = \"STRONG\" if abs(loading) > 0.7 else \"MODERATE\"\n",
    "            print(f\"      {j}. {feat:20s} {direction} {loading:+.3f}  ({strength})\")\n",
    "    \n",
    "    # Suggest interpretation based on loadings\n",
    "    high_loadings = factor_loadings.head(3).index.tolist()\n",
    "    \n",
    "    # Simple naming heuristics\n",
    "    if any('Volume' in f or 'TVL' in f or 'Market_Cap' in f for f in high_loadings):\n",
    "        interpretation = \"üí∞ SIZE FACTOR (Protocol/Token Scale)\"\n",
    "    elif any('Return' in f or 'Volatility' in f for f in high_loadings):\n",
    "        interpretation = \"üìà RISK FACTOR (Risk/Return Profile)\"\n",
    "    elif any('ETH' in f or 'Sentiment' in f for f in high_loadings):\n",
    "        interpretation = \"üåä MARKET SENTIMENT FACTOR\"\n",
    "    elif any('Liquidity' in f for f in high_loadings):\n",
    "        interpretation = \"üíß LIQUIDITY FACTOR\"\n",
    "    else:\n",
    "        interpretation = \"‚ùì MIXED FACTOR (Multiple influences)\"\n",
    "    \n",
    "    print(f\"\\n   Suggested interpretation: {interpretation}\")\n",
    "    print(f\"   \" + \"-\"*60)\n",
    "\n",
    "# STEP 5: Visualize Communalities\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: Communality and Uniqueness Visualization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüé® Creating communality plots...\")\n",
    "fig = fa_model.plot_communalities(figsize=(14, 6))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Communality Insights:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Features with high common variance\n",
    "print(\"\\nüîó Features STRONGLY influenced by common factors:\")\n",
    "for feat in communalities[communalities > 0.7].sort_values(ascending=False).index:\n",
    "    print(f\"   ‚Ä¢ {feat}: {communalities[feat]:.1%} common | {uniqueness[feat]:.1%} unique\")\n",
    "    print(f\"     ‚Üí Driven by systematic market forces\")\n",
    "\n",
    "# Features with high uniqueness\n",
    "print(\"\\n‚≠ê Features with HIGH UNIQUENESS (diversification opportunities):\")\n",
    "for feat in uniqueness[uniqueness > 0.5].sort_values(ascending=False).index:\n",
    "    print(f\"   ‚Ä¢ {feat}: {uniqueness[feat]:.1%} unique | {communalities[feat]:.1%} common\")\n",
    "    print(f\"     ‚Üí Token-specific behavior, less correlated with market\")\n",
    "\n",
    "# STEP 6: Visualize tokens in factor space\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: Visualizing Tokens in Factor Space\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüé® Creating factor score scatter plots...\")\n",
    "fig = fa_model.plot_factor_scores(factor_scores, figsize=(14, 6))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Factor Space Insights:\")\n",
    "print(\"=\"*70)\n",
    "print(\"   ‚Ä¢ Each point represents a token\")\n",
    "print(\"   ‚Ä¢ Position shows token's exposure to each factor\")\n",
    "print(\"   ‚Ä¢ Tokens close together have similar factor profiles\")\n",
    "print(\"   ‚Ä¢ Tokens far apart are fundamentally different\")\n",
    "print(\"   ‚Ä¢ Origin (0,0) represents average exposure to all factors\")\n",
    "\n",
    "# Calculate which tokens have extreme factor scores\n",
    "print(\"\\nüéØ Tokens with Extreme Factor Exposures:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(min(3, factor_scores.shape[1])):\n",
    "    print(f\"\\nFactor{i+1}:\")\n",
    "    factor_col = factor_scores[:, i]\n",
    "    \n",
    "    # Highest exposure\n",
    "    high_idx = np.argmax(factor_col)\n",
    "    print(f\"   Highest: {defi_data.index[high_idx]} (score: {factor_col[high_idx]:.2f})\")\n",
    "    \n",
    "    # Lowest exposure\n",
    "    low_idx = np.argmin(factor_col)\n",
    "    print(f\"   Lowest:  {defi_data.index[low_idx]} (score: {factor_col[low_idx]:.2f})\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FACTOR ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_stats = {\n",
    "    'Metric': [\n",
    "        'Tokens Analyzed',\n",
    "        'Original Features',\n",
    "        'Latent Factors Extracted',\n",
    "        'Avg Communality',\n",
    "        'Avg Uniqueness',\n",
    "        'Common Variance Explained',\n",
    "        'Dimensionality Reduction'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{defi_data.shape[0]}\",\n",
    "        f\"{defi_data.shape[1]}\",\n",
    "        f\"{fa_model.n_factors}\",\n",
    "        f\"{communalities.mean():.1%}\",\n",
    "        f\"{uniqueness.mean():.1%}\",\n",
    "        f\"{total_var:.1%}\",\n",
    "        f\"{defi_data.shape[1]/fa_model.n_factors:.1f}x\"\n",
    "    ]\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\n What We Learned from Factor Analysis:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   ‚úÖ Identified {fa_model.n_factors} latent factors driving DeFi token behavior\")\n",
    "print(f\"   ‚úÖ {communalities.mean():.1%} of variance is common (systematic risk)\")\n",
    "print(f\"   ‚úÖ {uniqueness.mean():.1%} of variance is unique (idiosyncratic risk)\")\n",
    "print(f\"   ‚úÖ Each token has specific factor exposures (betas)\")\n",
    "print(f\"   ‚úÖ Can build factor-neutral or factor-tilted portfolios\")\n",
    "\n",
    "print(\"\\n Investment Applications:\")\n",
    "print(\"=\"*70)\n",
    "print(\"   1. RISK DECOMPOSITION:\")\n",
    "print(\"      ‚Ä¢ Separate systematic risk (factors) from unique risk\")\n",
    "print(\"      ‚Ä¢ Understand which factors drive portfolio volatility\")\n",
    "print()\n",
    "print(\"   2. PORTFOLIO CONSTRUCTION:\")\n",
    "print(\"      ‚Ä¢ Build factor-balanced portfolios\")\n",
    "print(\"      ‚Ä¢ Tilt towards desired factor exposures\")\n",
    "print(\"      ‚Ä¢ Select tokens with high uniqueness for diversification\")\n",
    "print()\n",
    "print(\"   3. HEDGING STRATEGIES:\")\n",
    "print(\"      ‚Ä¢ Identify factor exposures to hedge\")\n",
    "print(\"      ‚Ä¢ Find tokens with offsetting factor profiles\")\n",
    "print()\n",
    "print(\"   4. ALPHA GENERATION:\")\n",
    "print(\"      ‚Ä¢ Find tokens with high uniqueness (less market-driven)\")\n",
    "print(\"      ‚Ä¢ Potential for uncorrelated returns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ FACTOR ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüëâ Next: Compare Factor Analysis with PCA results\")\n",
    "\n",
    "# BONUS: Quick comparison FA vs PCA\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÑ FACTOR ANALYSIS vs PCA COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Quick Comparison for Your Data:\")\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Method',\n",
    "        'Components/Factors',\n",
    "        'Primary Goal',\n",
    "        'Variance Type',\n",
    "        'Has Uniqueness Concept',\n",
    "        'Rotation Applied',\n",
    "        'Interpretability'\n",
    "    ],\n",
    "    'PCA (if you ran it)': [\n",
    "        'Principal Components',\n",
    "        '5 (your choice)',\n",
    "        'Dimension reduction',\n",
    "        'Total variance',\n",
    "        'No',\n",
    "        'Optional',\n",
    "        'Components = combinations'\n",
    "    ],\n",
    "    'Factor Analysis (just ran)': [\n",
    "        'Latent Factors',\n",
    "        f'{fa_model.n_factors} (auto-detected)',\n",
    "        'Find causes',\n",
    "        'Common variance only',\n",
    "        'Yes',\n",
    "        'Varimax',\n",
    "        'Factors = underlying causes'\n",
    "    ]\n",
    "}\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\n When to Use Each Method:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUse FACTOR ANALYSIS when:\")\n",
    "print(\"   ‚Ä¢ You want to understand causal relationships\")\n",
    "print(\"   ‚Ä¢ You need to separate systematic vs unique risk\")\n",
    "print(\"   ‚Ä¢ You're building factor-based strategies\")\n",
    "print(\"   ‚Ä¢ You care about economic interpretation\")\n",
    "print()\n",
    "print(\"Use PCA when:\")\n",
    "print(\"   ‚Ä¢ You just need dimension reduction\")\n",
    "print(\"   ‚Ä¢ You want uncorrelated features for ML\")\n",
    "print(\"   ‚Ä¢ Speed is critical (PCA is faster)\")\n",
    "print(\"   ‚Ä¢ You don't need causal interpretation\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509410f6-ed06-4004-b0dd-46bf19971f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
