{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# Week 8: Deep Learning for Sequences - RNN/LSTM in Finance\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to Week 8! This week we dive into the exciting world of **Deep Learning for Sequential Data**.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Recurrent Neural Networks (RNN)**: Understanding networks with memory\n",
    "- **Long Short-Term Memory (LSTM)**: Advanced RNNs that solve long-term dependency problems\n",
    "- **Sequence Modeling**: Predicting future values from historical patterns\n",
    "- **Performance Evaluation**: Measuring and comparing model improvements\n",
    "\n",
    "**Why this matters in Finance:**\n",
    "- Financial data is inherently sequential (time series)\n",
    "- Prices, volumes, and returns have temporal dependencies\n",
    "- Traditional ML ignores the order of data points\n",
    "- LSTMs can capture complex patterns across different time horizons\n",
    "\n",
    "**By the end of this notebook, you'll be able to:**\n",
    "\n",
    "‚úÖ Understand how RNNs maintain memory of past information  \n",
    "‚úÖ Build and train an LSTM model for price prediction  \n",
    "‚úÖ Compare LSTM performance against baseline models  \n",
    "‚úÖ Interpret and visualize LSTM predictions  \n",
    "‚úÖ Apply sequence modeling to real DeFi/crypto data  \n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Setup: Installing Required Libraries\n",
    "\n",
    "Before we begin, we need to import our tools:\n",
    "\n",
    "- **NumPy & Pandas**: Data manipulation and numerical operations\n",
    "- **Matplotlib & Seaborn**: Visualization\n",
    "- **TensorFlow/Keras**: Deep learning framework for building neural networks\n",
    "- **Scikit-learn**: Preprocessing and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, SimpleRNN\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Confirm successful import\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ ALL LIBRARIES IMPORTED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nLibraries loaded:\")\n",
    "print(\"  ‚Ä¢ NumPy version:\", np.__version__)\n",
    "print(\"  ‚Ä¢ Pandas version:\", pd.__version__)\n",
    "print(\"  ‚Ä¢ TensorFlow version:\", tf.__version__)\n",
    "print(\"  ‚Ä¢ Keras: Ready for Deep Learning\")\n",
    "print(\"  ‚Ä¢ GPU Available:\", \"Yes\" if len(tf.config.list_physical_devices('GPU')) > 0 else \"No (CPU only)\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-part1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding Sequential Data and the Need for RNNs\n",
    "\n",
    "## üéØ What Makes Financial Data Sequential?\n",
    "\n",
    "### The Problem with Traditional Neural Networks\n",
    "\n",
    "Imagine you're trying to predict tomorrow's Bitcoin price. Traditional neural networks treat each data point **independently**:\n",
    "\n",
    "```\n",
    "Traditional Neural Network:\n",
    "Price_t-3 ‚Üí NN ‚Üí Prediction_t+1\n",
    "Price_t-2 ‚Üí NN ‚Üí Prediction_t+1\n",
    "Price_t-1 ‚Üí NN ‚Üí Prediction_t+1\n",
    "```\n",
    "\n",
    "**What's wrong with this?**\n",
    "- Each prediction ignores the **order** of data\n",
    "- No memory of what happened before\n",
    "- Can't capture trends, momentum, or patterns across time\n",
    "\n",
    "### The Sequential Nature of Finance\n",
    "\n",
    "Financial data has **temporal dependencies**:\n",
    "1. **Short-term momentum**: Yesterday's rise often continues today\n",
    "2. **Trend patterns**: Prices move in trends (bull/bear markets)\n",
    "3. **Volatility clustering**: High volatility days cluster together\n",
    "4. **Seasonal patterns**: Weekly/monthly regularities\n",
    "5. **Mean reversion**: Prices eventually return to equilibrium\n",
    "\n",
    "**We need a model that \"remembers\" the sequence!**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Enter Recurrent Neural Networks (RNNs)\n",
    "\n",
    "### The Core Idea: Neural Networks with Memory\n",
    "\n",
    "**RNNs process sequential data one step at a time, maintaining a \"hidden state\" (memory):**\n",
    "\n",
    "```\n",
    "RNN with Memory:\n",
    "Price_t-3 ‚Üí [RNN] ‚Üí hidden_state_1\n",
    "Price_t-2 ‚Üí [RNN + hidden_state_1] ‚Üí hidden_state_2\n",
    "Price_t-1 ‚Üí [RNN + hidden_state_2] ‚Üí hidden_state_3\n",
    "            [hidden_state_3] ‚Üí Prediction_t+1\n",
    "```\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Hidden State (h_t)**: The \"memory\" that gets passed from one time step to the next\n",
    "   - Contains information about all previous time steps\n",
    "   - Updated at each new observation\n",
    "\n",
    "2. **Recurrent Connection**: Output feeds back as input\n",
    "   - Same weights applied at each time step\n",
    "   - Learns patterns that work across different time positions\n",
    "\n",
    "3. **Unrolling Through Time**: Conceptually, an RNN for T time steps is like a very deep network with T layers\n",
    "\n",
    "---\n",
    "\n",
    "## üìê RNN Mathematics (Simplified)\n",
    "\n",
    "At each time step t, an RNN does:\n",
    "\n",
    "**Step 1:** Combine current input (x_t) with previous hidden state (h_t-1)\n",
    "```\n",
    "h_t = tanh(W_hh * h_t-1 + W_xh * x_t + b_h)\n",
    "```\n",
    "\n",
    "**Step 2:** Generate output from hidden state\n",
    "```\n",
    "y_t = W_hy * h_t + b_y\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **x_t**: Current input (e.g., today's price features)\n",
    "- **h_t**: Hidden state (memory) at time t\n",
    "- **y_t**: Output/prediction at time t\n",
    "- **W**: Weight matrices (learned during training)\n",
    "- **tanh**: Activation function (keeps values between -1 and 1)\n",
    "\n",
    "### Intuition:\n",
    "Think of the hidden state as a **summary of everything seen so far**, constantly updated with new information.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è The Vanishing Gradient Problem\n",
    "\n",
    "### Why Basic RNNs Struggle\n",
    "\n",
    "**The Problem:**\n",
    "When training RNNs through backpropagation, gradients need to flow backward through many time steps. With standard RNNs:\n",
    "\n",
    "- Gradients get multiplied repeatedly\n",
    "- They either **vanish** (‚Üí 0) or **explode** (‚Üí ‚àû)\n",
    "- Network can't learn long-term dependencies\n",
    "\n",
    "**Example in Finance:**\n",
    "```\n",
    "Imagine trying to predict Bitcoin price in 2024 based on:\n",
    "- Yesterday's price (easy for RNN)\n",
    "- Last week's trend (harder for RNN)\n",
    "- The halving event 6 months ago (impossible for basic RNN!)\n",
    "```\n",
    "\n",
    "Basic RNNs **forget long-term information** due to vanishing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Long Short-Term Memory (LSTM): The Solution\n",
    "\n",
    "### What Makes LSTM Special?\n",
    "\n",
    "**LSTMs are specifically designed to remember information for long periods!**\n",
    "\n",
    "They solve the vanishing gradient problem through a clever architecture with **gates** that control information flow.\n",
    "\n",
    "### The LSTM Cell Architecture\n",
    "\n",
    "An LSTM cell has two states:\n",
    "1. **Cell State (C_t)**: Long-term memory highway\n",
    "2. **Hidden State (h_t)**: Short-term working memory\n",
    "\n",
    "And three gates:\n",
    "1. **Forget Gate (f_t)**: What to forget from long-term memory\n",
    "2. **Input Gate (i_t)**: What new information to store\n",
    "3. **Output Gate (o_t)**: What to output/remember short-term\n",
    "\n",
    "### LSTM Intuition with a Trading Example\n",
    "\n",
    "**Imagine you're a trader analyzing Bitcoin:**\n",
    "\n",
    "1. **Cell State (Long-term memory):**\n",
    "   - \"Bitcoin tends to pump after halvings\" (stored long-term)\n",
    "   - \"Institutional adoption is increasing\" (stored long-term)\n",
    "   - \"Regulatory news impacts price\" (stored long-term)\n",
    "\n",
    "2. **Forget Gate:**\n",
    "   - \"That FUD tweet from 3 months ago? Not relevant anymore\" ‚Üí FORGET\n",
    "   - \"The bull market pattern? Still relevant\" ‚Üí KEEP\n",
    "\n",
    "3. **Input Gate:**\n",
    "   - \"New: Major exchange got hacked\" ‚Üí IMPORTANT, STORE THIS\n",
    "   - \"Minor: Some random price fluctuation\" ‚Üí IGNORE\n",
    "\n",
    "4. **Output Gate:**\n",
    "   - \"Based on long-term bull trend + recent hack news ‚Üí Predict temporary dip\"\n",
    "\n",
    "---\n",
    "\n",
    "## üìä LSTM Mathematics (Step-by-Step)\n",
    "\n",
    "At each time step t, LSTM performs these operations:\n",
    "\n",
    "**Step 1: Forget Gate** (What to forget from cell state)\n",
    "```\n",
    "f_t = œÉ(W_f ¬∑ [h_t-1, x_t] + b_f)\n",
    "```\n",
    "- Output: Values between 0 (completely forget) and 1 (completely remember)\n",
    "\n",
    "**Step 2: Input Gate** (What new information to store)\n",
    "```\n",
    "i_t = œÉ(W_i ¬∑ [h_t-1, x_t] + b_i)\n",
    "CÃÉ_t = tanh(W_C ¬∑ [h_t-1, x_t] + b_C)\n",
    "```\n",
    "- i_t: How much to update\n",
    "- CÃÉ_t: Candidate values to add\n",
    "\n",
    "**Step 3: Update Cell State** (Update long-term memory)\n",
    "```\n",
    "C_t = f_t * C_t-1 + i_t * CÃÉ_t\n",
    "```\n",
    "- Forget old info (f_t * C_t-1)\n",
    "- Add new info (i_t * CÃÉ_t)\n",
    "\n",
    "**Step 4: Output Gate** (What to output)\n",
    "```\n",
    "o_t = œÉ(W_o ¬∑ [h_t-1, x_t] + b_o)\n",
    "h_t = o_t * tanh(C_t)\n",
    "```\n",
    "- Decides what parts of cell state to reveal\n",
    "\n",
    "Where:\n",
    "- **œÉ**: Sigmoid function (outputs 0 to 1)\n",
    "- **tanh**: Hyperbolic tangent (outputs -1 to 1)\n",
    "- **¬∑**: Matrix multiplication\n",
    "- **‚àó**: Element-wise multiplication\n",
    "\n",
    "### The Key Insight:\n",
    "The cell state (C_t) flows through time with only minor modifications, allowing gradients to flow backward easily. This solves the vanishing gradient problem!\n",
    "\n",
    "---\n",
    "\n",
    "## üí° When to Use LSTMs in Finance?\n",
    "\n",
    "‚úÖ **USE LSTMs when you have:**\n",
    "- Time series data with temporal dependencies\n",
    "- Need to capture long-term patterns (weeks/months)\n",
    "- Multiple time-varying features\n",
    "- Non-linear relationships in sequential data\n",
    "- Sufficient data (typically 1000+ observations)\n",
    "\n",
    "‚ùå **BE CAREFUL when:**\n",
    "- You have very little data (< 500 observations)\n",
    "- Relationships are clearly linear (simpler models may work)\n",
    "- You need real-time, ultra-fast predictions\n",
    "- Interpretability is critical (LSTMs are black boxes)\n",
    "- Data has strong non-stationarity\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Real-World Finance Applications of LSTM\n",
    "\n",
    "1. **Price Prediction**: Forecast stock/crypto prices using historical patterns\n",
    "2. **Volatility Forecasting**: Predict future volatility for option pricing\n",
    "3. **Trading Signal Generation**: Generate buy/sell signals from patterns\n",
    "4. **Risk Management**: Predict Value-at-Risk (VaR) dynamically\n",
    "5. **Market Regime Detection**: Identify shifts between bull/bear markets\n",
    "6. **Sentiment Analysis**: Process sequential text data (news, tweets)\n",
    "7. **Portfolio Optimization**: Dynamic rebalancing based on predicted returns\n",
    "8. **Fraud Detection**: Identify unusual transaction patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üìö LSTM vs Traditional Methods\n",
    "\n",
    "| Aspect | Traditional ML (RF, XGBoost) | LSTM |\n",
    "|--------|------------------------------|------|\n",
    "| **Sequential Memory** | ‚ùå No (treats data independently) | ‚úÖ Yes (maintains hidden state) |\n",
    "| **Long-term Dependencies** | ‚ùå Hard to capture | ‚úÖ Designed for this |\n",
    "| **Feature Engineering** | Manual (lag features, rolling stats) | Automatic (learns patterns) |\n",
    "| **Training Time** | Fast (minutes) | Slow (hours) |\n",
    "| **Data Requirements** | Moderate (100s) | High (1000s+) |\n",
    "| **Interpretability** | High (feature importance) | Low (black box) |\n",
    "| **Overfitting Risk** | Moderate | High (needs regularization) |\n",
    "\n",
    "**Bottom Line:** LSTMs excel when you have enough data and complex temporal patterns. For simpler problems, traditional ML may be better!\n",
    "\n",
    "---\n",
    "\n",
    "Now let's build our first LSTM model! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-gen-theory",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Data Preparation for Sequence Modeling\n",
    "\n",
    "## üéØ Creating Realistic Financial Time Series Data\n",
    "\n",
    "Before we can train an LSTM, we need to understand how to prepare sequential data. We'll:\n",
    "\n",
    "1. Generate synthetic crypto price data with realistic properties\n",
    "2. Create sequences (look-back windows) for training\n",
    "3. Split data properly for time series (no random shuffling!)\n",
    "4. Scale features appropriately\n",
    "\n",
    "### Key Concept: Look-back Windows\n",
    "\n",
    "LSTMs need **sequences** of data as input:\n",
    "\n",
    "```\n",
    "If look_back = 10 days:\n",
    "Input:  [Price_t-10, Price_t-9, ..., Price_t-1]  (10 previous days)\n",
    "Output: Price_t                                   (tomorrow's price)\n",
    "```\n",
    "\n",
    "We'll generate multiple such sequences from our time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDataGenerator:\n",
    "    \"\"\"\n",
    "    Generate synthetic financial time series data with realistic properties\n",
    "    for LSTM training and evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_samples=2000, start_price=100, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize the data generator.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_samples : int\n",
    "            Number of time steps to generate\n",
    "        start_price : float\n",
    "            Initial price value\n",
    "        seed : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_samples = n_samples\n",
    "        self.start_price = start_price\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def generate_crypto_prices(self):\n",
    "        \"\"\"\n",
    "        Generate synthetic crypto prices with trend, momentum, and noise.\n",
    "        \n",
    "        Mimics real crypto behavior:\n",
    "        - Geometric Brownian Motion (random walk)\n",
    "        - Trend component (bull/bear cycles)\n",
    "        - Momentum component (short-term persistence)\n",
    "        - Volatility clustering\n",
    "        \"\"\"\n",
    "        print(\"üîß Generating synthetic crypto price data...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Initialize\n",
    "        prices = np.zeros(self.n_samples)\n",
    "        prices[0] = self.start_price\n",
    "        \n",
    "        # Parameters\n",
    "        base_volatility = 0.02  # 2% daily volatility\n",
    "        trend_strength = 0.0003  # Slight upward bias\n",
    "        momentum_strength = 0.3  # How much yesterday influences today\n",
    "        \n",
    "        print(f\"üìä Parameters:\")\n",
    "        print(f\"   ‚Ä¢ Samples: {self.n_samples}\")\n",
    "        print(f\"   ‚Ä¢ Starting price: ${self.start_price}\")\n",
    "        print(f\"   ‚Ä¢ Base volatility: {base_volatility*100:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Trend strength: {trend_strength*100:.2f}%\")\n",
    "        print(f\"   ‚Ä¢ Momentum: {momentum_strength*100:.0f}%\")\n",
    "        \n",
    "        # Generate price series\n",
    "        for t in range(1, self.n_samples):\n",
    "            # Random shock (white noise)\n",
    "            shock = np.random.randn() * base_volatility\n",
    "            \n",
    "            # Trend component (long-term drift)\n",
    "            trend = trend_strength\n",
    "            \n",
    "            # Momentum component (yesterday's return influences today)\n",
    "            if t > 1:\n",
    "                prev_return = (prices[t-1] - prices[t-2]) / prices[t-2]\n",
    "                momentum = momentum_strength * prev_return\n",
    "            else:\n",
    "                momentum = 0\n",
    "            \n",
    "            # Combine components\n",
    "            total_return = trend + momentum + shock\n",
    "            \n",
    "            # Update price\n",
    "            prices[t] = prices[t-1] * (1 + total_return)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        dates = pd.date_range(start='2020-01-01', periods=self.n_samples, freq='D')\n",
    "        df = pd.DataFrame({'Date': dates, 'Price': prices})\n",
    "        df.set_index('Date', inplace=True)\n",
    "        \n",
    "        # Calculate additional features\n",
    "        df['Returns'] = df['Price'].pct_change()\n",
    "        df['Log_Returns'] = np.log(df['Price'] / df['Price'].shift(1))\n",
    "        df['Volatility_5d'] = df['Returns'].rolling(window=5).std()\n",
    "        df['MA_10'] = df['Price'].rolling(window=10).mean()\n",
    "        df['MA_50'] = df['Price'].rolling(window=50).mean()\n",
    "        \n",
    "        # Drop NaN rows\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        print(\"\\n‚úÖ Data generation complete!\")\n",
    "        print(f\"   ‚Ä¢ Final shape: {df.shape}\")\n",
    "        print(f\"   ‚Ä¢ Price range: ${df['Price'].min():.2f} - ${df['Price'].max():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Mean daily return: {df['Returns'].mean()*100:.3f}%\")\n",
    "        print(f\"   ‚Ä¢ Volatility (std): {df['Returns'].std()*100:.2f}%\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def plot_price_series(self, df, figsize=(14, 10)):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualization of the generated price series.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(3, 1, figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Price with Moving Averages\n",
    "        axes[0].plot(df.index, df['Price'], label='Price', linewidth=1.5, alpha=0.8)\n",
    "        axes[0].plot(df.index, df['MA_10'], label='MA(10)', linewidth=1, alpha=0.7)\n",
    "        axes[0].plot(df.index, df['MA_50'], label='MA(50)', linewidth=1, alpha=0.7)\n",
    "        axes[0].set_title('Generated Crypto Price Series with Moving Averages', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_ylabel('Price ($)', fontsize=11)\n",
    "        axes[0].legend(loc='best')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Daily Returns\n",
    "        axes[1].plot(df.index, df['Returns']*100, color='steelblue', alpha=0.6, linewidth=0.8)\n",
    "        axes[1].axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        axes[1].set_title('Daily Returns (%)', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_ylabel('Return (%)', fontsize=11)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Rolling Volatility\n",
    "        axes[2].plot(df.index, df['Volatility_5d']*100, color='coral', linewidth=1.5)\n",
    "        axes[2].set_title('5-Day Rolling Volatility', fontsize=14, fontweight='bold')\n",
    "        axes[2].set_ylabel('Volatility (%)', fontsize=11)\n",
    "        axes[2].set_xlabel('Date', fontsize=11)\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Generate data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: DATA GENERATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "data_gen = FinancialDataGenerator(n_samples=2000, start_price=100, seed=42)\n",
    "crypto_df = data_gen.generate_crypto_prices()\n",
    "\n",
    "print(\"\\nüìä First few rows of generated data:\")\n",
    "display(crypto_df.head(10))\n",
    "\n",
    "print(\"\\nüìà Statistical Summary:\")\n",
    "display(crypto_df.describe())\n",
    "\n",
    "# Visualize\n",
    "print(\"\\nüé® Creating visualizations...\")\n",
    "fig = data_gen.plot_price_series(crypto_df)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"=\"*70)\n",
    "print(\"   ‚Ä¢ Price shows realistic random walk behavior\")\n",
    "print(\"   ‚Ä¢ Moving averages smooth out short-term noise\")\n",
    "print(\"   ‚Ä¢ Returns fluctuate around zero (mean-reverting)\")\n",
    "print(\"   ‚Ä¢ Volatility clusters (high volatility periods persist)\")\n",
    "print(\"   ‚Ä¢ This mimics real crypto market behavior!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sequence-prep-theory",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Creating Sequences for LSTM Training\n",
    "\n",
    "### Understanding the Sequence Creation Process\n",
    "\n",
    "**The Challenge:** LSTMs need 3D input data with shape `(samples, timesteps, features)`\n",
    "\n",
    "**Our 1D price series:**\n",
    "```\n",
    "[100, 102, 101, 103, 105, 104, 106, ...]\n",
    "```\n",
    "\n",
    "**Must become 3D sequences:**\n",
    "```\n",
    "Sequence 1: [[100, 102, 101, 103, 105]] ‚Üí Target: 104\n",
    "Sequence 2: [[102, 101, 103, 105, 104]] ‚Üí Target: 106\n",
    "Sequence 3: [[101, 103, 105, 104, 106]] ‚Üí Target: ...\n",
    "...\n",
    "```\n",
    "\n",
    "### Important Considerations:\n",
    "\n",
    "1. **Look-back Period**: How many past time steps to use\n",
    "   - Too short: Miss long-term patterns\n",
    "   - Too long: Overfitting, computational cost\n",
    "   - Common choices: 10-60 days for daily data\n",
    "\n",
    "2. **Time Series Split**: NEVER randomly shuffle!\n",
    "   - Train on past data only\n",
    "   - Validate on intermediate period\n",
    "   - Test on most recent data\n",
    "\n",
    "3. **Scaling**: Critical for neural networks\n",
    "   - Fit scaler on training data only\n",
    "   - Transform validation and test using training scaler\n",
    "   - Prevents data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sequence-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePreparator:\n",
    "    \"\"\"\n",
    "    Prepare sequential data for LSTM training.\n",
    "    Handles sequence creation, train/val/test split, and scaling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, look_back=30, train_ratio=0.7, val_ratio=0.15):\n",
    "        \"\"\"\n",
    "        Initialize the sequence preparator.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        look_back : int\n",
    "            Number of previous time steps to use as input\n",
    "        train_ratio : float\n",
    "            Proportion of data for training\n",
    "        val_ratio : float\n",
    "            Proportion of data for validation\n",
    "        \"\"\"\n",
    "        self.look_back = look_back\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.test_ratio = 1 - train_ratio - val_ratio\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    def create_sequences(self, data, look_back):\n",
    "        \"\"\"\n",
    "        Create sequences from 1D time series data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : np.array\n",
    "            1D array of values\n",
    "        look_back : int\n",
    "            Number of previous time steps\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X, y : np.arrays\n",
    "            X: 3D array (samples, timesteps, features)\n",
    "            y: 2D array (samples, 1) - targets\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(len(data) - look_back):\n",
    "            # Get sequence of 'look_back' previous values\n",
    "            sequence = data[i:(i + look_back)]\n",
    "            # Get the next value as target\n",
    "            target = data[i + look_back]\n",
    "            \n",
    "            X.append(sequence)\n",
    "            y.append(target)\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Reshape X to (samples, timesteps, features)\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def prepare_data(self, df, target_column='Price'):\n",
    "        \"\"\"\n",
    "        Complete data preparation pipeline.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dictionary with train, val, test splits (scaled and unscaled)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 2: SEQUENCE PREPARATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Extract target values\n",
    "        data = df[target_column].values.reshape(-1, 1)\n",
    "        \n",
    "        print(f\"\\nüìä Data Shape: {data.shape}\")\n",
    "        print(f\"   ‚Ä¢ Total samples: {len(data)}\")\n",
    "        print(f\"   ‚Ä¢ Look-back period: {self.look_back} days\")\n",
    "        print(f\"   ‚Ä¢ Effective sequences: {len(data) - self.look_back}\")\n",
    "        \n",
    "        # Split indices (time series split - no shuffling!)\n",
    "        total_len = len(data)\n",
    "        train_end = int(total_len * self.train_ratio)\n",
    "        val_end = int(total_len * (self.train_ratio + self.val_ratio))\n",
    "        \n",
    "        print(f\"\\nüîÄ Time Series Split:\")\n",
    "        print(f\"   ‚Ä¢ Train: 0 to {train_end} ({self.train_ratio*100:.0f}%)\")\n",
    "        print(f\"   ‚Ä¢ Val:   {train_end} to {val_end} ({self.val_ratio*100:.0f}%)\")\n",
    "        print(f\"   ‚Ä¢ Test:  {val_end} to {total_len} ({self.test_ratio*100:.0f}%)\")\n",
    "        \n",
    "        # Split data\n",
    "        train_data = data[:train_end]\n",
    "        val_data = data[train_end:val_end]\n",
    "        test_data = data[val_end:]\n",
    "        \n",
    "        # Fit scaler on training data only (IMPORTANT!)\n",
    "        print(\"\\n‚öñÔ∏è  Scaling data...\")\n",
    "        train_scaled = self.scaler.fit_transform(train_data)\n",
    "        val_scaled = self.scaler.transform(val_data)\n",
    "        test_scaled = self.scaler.transform(test_data)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Scaler fitted on training data\")\n",
    "        print(f\"   ‚Ä¢ Min: {self.scaler.data_min_[0]:.2f}, Max: {self.scaler.data_max_[0]:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Scaled range: [0, 1]\")\n",
    "        \n",
    "        # Create sequences\n",
    "        print(\"\\nüîÑ Creating sequences...\")\n",
    "        X_train, y_train = self.create_sequences(train_scaled, self.look_back)\n",
    "        X_val, y_val = self.create_sequences(val_scaled, self.look_back)\n",
    "        X_test, y_test = self.create_sequences(test_scaled, self.look_back)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Sequences Created:\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train.shape} | y_train: {y_train.shape}\")\n",
    "        print(f\"   ‚Ä¢ X_val:   {X_val.shape} | y_val: {y_val.shape}\")\n",
    "        print(f\"   ‚Ä¢ X_test:  {X_test.shape} | y_test: {y_test.shape}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Return everything\n",
    "        return {\n",
    "            'X_train': X_train, 'y_train': y_train,\n",
    "            'X_val': X_val, 'y_val': y_val,\n",
    "            'X_test': X_test, 'y_test': y_test,\n",
    "            'train_data': train_data,\n",
    "            'val_data': val_data,\n",
    "            'test_data': test_data,\n",
    "            'scaler': self.scaler\n",
    "        }\n",
    "    \n",
    "    def visualize_splits(self, df, train_end_idx, val_end_idx, figsize=(14, 5)):\n",
    "        \"\"\"\n",
    "        Visualize the train/val/test split.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot full data\n",
    "        ax.plot(df.index, df['Price'], color='gray', alpha=0.3, label='Full Data')\n",
    "        \n",
    "        # Highlight splits\n",
    "        train_data = df.iloc[:train_end_idx]\n",
    "        val_data = df.iloc[train_end_idx:val_end_idx]\n",
    "        test_data = df.iloc[val_end_idx:]\n",
    "        \n",
    "        ax.plot(train_data.index, train_data['Price'], color='blue', linewidth=2, label='Train')\n",
    "        ax.plot(val_data.index, val_data['Price'], color='orange', linewidth=2, label='Validation')\n",
    "        ax.plot(test_data.index, test_data['Price'], color='green', linewidth=2, label='Test')\n",
    "        \n",
    "        ax.set_title('Train / Validation / Test Split (Time Series)', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Date', fontsize=11)\n",
    "        ax.set_ylabel('Price ($)', fontsize=11)\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Prepare sequences\n",
    "seq_prep = SequencePreparator(look_back=30, train_ratio=0.7, val_ratio=0.15)\n",
    "data_dict = seq_prep.prepare_data(crypto_df, target_column='Price')\n",
    "\n",
    "# Visualize splits\n",
    "print(\"\\nüé® Visualizing data splits...\")\n",
    "train_end = int(len(crypto_df) * 0.7)\n",
    "val_end = int(len(crypto_df) * 0.85)\n",
    "fig = seq_prep.visualize_splits(crypto_df, train_end, val_end)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Points About Sequence Preparation:\")\n",
    "print(\"=\"*70)\n",
    "print(\"   1. ‚è∞ Temporal Order Preserved:\")\n",
    "print(\"      ‚Ä¢ Train on past, validate on intermediate, test on future\")\n",
    "print(\"      ‚Ä¢ NO random shuffling (unlike regular ML!)\")\n",
    "print()\n",
    "print(\"   2. üî¢ Sequence Structure:\")\n",
    "print(\"      ‚Ä¢ Each sample is a window of 30 consecutive days\")\n",
    "print(\"      ‚Ä¢ Target is the price on day 31\")\n",
    "print(\"      ‚Ä¢ Sliding window creates many overlapping sequences\")\n",
    "print()\n",
    "print(\"   3. ‚öñÔ∏è  Scaling:\")\n",
    "print(\"      ‚Ä¢ Scaler fitted ONLY on training data\")\n",
    "print(\"      ‚Ä¢ Same scaler applied to val and test\")\n",
    "print(\"      ‚Ä¢ Prevents data leakage from future\")\n",
    "print()\n",
    "print(\"   4. üìä Shape Transformation:\")\n",
    "print(f\"      ‚Ä¢ Original: (samples, 1)\")\n",
    "print(f\"      ‚Ä¢ After sequences: (samples, timesteps, features)\")\n",
    "print(f\"      ‚Ä¢ Ready for LSTM input!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-theory",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Baseline Models - Know Your Competition!\n",
    "\n",
    "## üéØ Why Build Baseline Models?\n",
    "\n",
    "**Before celebrating LSTM performance, we MUST establish baselines:**\n",
    "\n",
    "### The Principle:\n",
    "> \"A complex model is only valuable if it beats simple alternatives!\"\n",
    "\n",
    "### Common Baselines in Finance:\n",
    "\n",
    "1. **Naive Forecast**: \"Tomorrow = Today\"\n",
    "   - Surprisingly hard to beat in random walks!\n",
    "   - Also called \"persistence model\"\n",
    "\n",
    "2. **Simple Moving Average**: Average of last N days\n",
    "   - Smooths noise\n",
    "   - Widely used in practice\n",
    "\n",
    "3. **Linear Regression**: Fit a trend line\n",
    "   - Assumes linear relationships\n",
    "   - Fast and interpretable\n",
    "\n",
    "### Why This Matters:\n",
    "- If LSTM barely beats naive forecast ‚Üí Not worth the complexity\n",
    "- If LSTM beats all baselines ‚Üí Strong evidence of value\n",
    "- Helps quantify the \"performance lift\" from deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Evaluation Metrics\n",
    "\n",
    "We'll use multiple metrics because each captures different aspects:\n",
    "\n",
    "1. **MAE (Mean Absolute Error)**: Average prediction error in dollars\n",
    "   - Easy to interpret: \"Off by $X on average\"\n",
    "   - Not sensitive to outliers\n",
    "\n",
    "2. **RMSE (Root Mean Squared Error)**: Penalizes large errors more\n",
    "   - Standard metric in ML\n",
    "   - Sensitive to outliers\n",
    "\n",
    "3. **MAPE (Mean Absolute Percentage Error)**: Error as percentage\n",
    "   - Scale-independent\n",
    "   - Easy to understand: \"Off by X%\"\n",
    "\n",
    "4. **R¬≤ Score**: Proportion of variance explained\n",
    "   - 1.0 = perfect predictions\n",
    "   - 0.0 = no better than mean\n",
    "   - < 0.0 = worse than mean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModels:\n",
    "    \"\"\"\n",
    "    Baseline models for time series forecasting.\n",
    "    Essential for evaluating if LSTM provides real value.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.predictions = {}\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def calculate_metrics(self, y_true, y_pred, model_name):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive evaluation metrics.\n",
    "        \"\"\"\n",
    "        # Ensure correct shapes\n",
    "        y_true = y_true.flatten()\n",
    "        y_pred = y_pred.flatten()\n",
    "        \n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        self.metrics[model_name] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'R2': r2\n",
    "        }\n",
    "        \n",
    "        return mae, rmse, mape, r2\n",
    "    \n",
    "    def naive_forecast(self, X, y):\n",
    "        \"\"\"\n",
    "        Naive forecast: Tomorrow's price = Today's price\n",
    "        (Use the last value from each sequence)\n",
    "        \"\"\"\n",
    "        # Last value in each sequence is our prediction\n",
    "        predictions = X[:, -1, 0]  # Last timestep of each sequence\n",
    "        self.predictions['Naive'] = predictions\n",
    "        return predictions\n",
    "    \n",
    "    def moving_average_forecast(self, X, window=5):\n",
    "        \"\"\"\n",
    "        Moving average of last 'window' days as prediction.\n",
    "        \"\"\"\n",
    "        predictions = np.mean(X[:, -window:, 0], axis=1)\n",
    "        self.predictions['MA'] = predictions\n",
    "        return predictions\n",
    "    \n",
    "    def linear_regression_forecast(self, X_train, y_train, X_test):\n",
    "        \"\"\"\n",
    "        Linear regression using the sequence values as features.\n",
    "        \"\"\"\n",
    "        # Flatten sequences for linear regression\n",
    "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # Train\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train_flat, y_train)\n",
    "        self.models['LinearRegression'] = lr\n",
    "        \n",
    "        # Predict\n",
    "        predictions = lr.predict(X_test_flat)\n",
    "        self.predictions['LinearRegression'] = predictions\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate_all_baselines(self, X_train, y_train, X_test, y_test, scaler):\n",
    "        \"\"\"\n",
    "        Train and evaluate all baseline models.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 3: BASELINE MODELS EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. Naive Forecast\n",
    "        print(\"\\n1Ô∏è‚É£ Naive Forecast (Persistence Model)\")\n",
    "        print(\"-\" * 70)\n",
    "        pred_naive = self.naive_forecast(X_test, y_test)\n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        pred_naive_original = scaler.inverse_transform(pred_naive.reshape(-1, 1))\n",
    "        y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "        \n",
    "        mae, rmse, mape, r2 = self.calculate_metrics(\n",
    "            y_test_original, pred_naive_original, 'Naive'\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚Ä¢ MAE:  ${mae:.2f}\")\n",
    "        print(f\"   ‚Ä¢ RMSE: ${rmse:.2f}\")\n",
    "        print(f\"   ‚Ä¢ MAPE: {mape:.2f}%\")\n",
    "        print(f\"   ‚Ä¢ R¬≤:   {r2:.4f}\")\n",
    "        results['Naive'] = pred_naive_original\n",
    "        \n",
    "        # 2. Moving Average\n",
    "        print(\"\\n2Ô∏è‚É£ Moving Average (5-day)\")\n",
    "        print(\"-\" * 70)\n",
    "        pred_ma = self.moving_average_forecast(X_test, window=5)\n",
    "        pred_ma_original = scaler.inverse_transform(pred_ma.reshape(-1, 1))\n",
    "        \n",
    "        mae, rmse, mape, r2 = self.calculate_metrics(\n",
    "            y_test_original, pred_ma_original, 'MovingAverage'\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚Ä¢ MAE:  ${mae:.2f}\")\n",
    "        print(f\"   ‚Ä¢ RMSE: ${rmse:.2f}\")\n",
    "        print(f\"   ‚Ä¢ MAPE: {mape:.2f}%\")\n",
    "        print(f\"   ‚Ä¢ R¬≤:   {r2:.4f}\")\n",
    "        results['MovingAverage'] = pred_ma_original\n",
    "        \n",
    "        # 3. Linear Regression\n",
    "        print(\"\\n3Ô∏è‚É£ Linear Regression\")\n",
    "        print(\"-\" * 70)\n",
    "        pred_lr = self.linear_regression_forecast(X_train, y_train, X_test)\n",
    "        pred_lr_original = scaler.inverse_transform(pred_lr.reshape(-1, 1))\n",
    "        \n",
    "        mae, rmse, mape, r2 = self.calculate_metrics(\n",
    "            y_test_original, pred_lr_original, 'LinearRegression'\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚Ä¢ MAE:  ${mae:.2f}\")\n",
    "        print(f\"   ‚Ä¢ RMSE: ${rmse:.2f}\")\n",
    "        print(f\"   ‚Ä¢ MAPE: {mape:.2f}%\")\n",
    "        print(f\"   ‚Ä¢ R¬≤:   {r2:.4f}\")\n",
    "        results['LinearRegression'] = pred_lr_original\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ BASELINE EVALUATION COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return results, y_test_original\n",
    "    \n",
    "    def plot_baseline_comparison(self, results, y_true, figsize=(14, 6)):\n",
    "        \"\"\"\n",
    "        Visualize baseline predictions vs actual.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Predictions vs Actual\n",
    "        plot_range = slice(0, 100)  # First 100 points for clarity\n",
    "        axes[0].plot(y_true[plot_range], label='Actual', linewidth=2, alpha=0.8, color='black')\n",
    "        \n",
    "        colors = ['blue', 'orange', 'green']\n",
    "        for i, (name, pred) in enumerate(results.items()):\n",
    "            axes[0].plot(pred[plot_range], label=name, linewidth=1.5, alpha=0.7, color=colors[i])\n",
    "        \n",
    "        axes[0].set_title('Baseline Predictions vs Actual (First 100 points)', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].set_xlabel('Time Step')\n",
    "        axes[0].set_ylabel('Price ($)')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Metrics Comparison\n",
    "        metrics_df = pd.DataFrame(self.metrics).T\n",
    "        metrics_df[['MAE', 'RMSE']].plot(kind='bar', ax=axes[1], color=['steelblue', 'coral'])\n",
    "        axes[1].set_title('Baseline Model Performance (MAE & RMSE)', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].set_xlabel('Model')\n",
    "        axes[1].set_ylabel('Error ($)')\n",
    "        axes[1].legend(['MAE', 'RMSE'])\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Evaluate baseline models\n",
    "baseline = BaselineModels()\n",
    "baseline_results, y_test_original = baseline.evaluate_all_baselines(\n",
    "    data_dict['X_train'], \n",
    "    data_dict['y_train'],\n",
    "    data_dict['X_test'], \n",
    "    data_dict['y_test'],\n",
    "    data_dict['scaler']\n",
    ")\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\nüìä Baseline Models Summary:\")\n",
    "metrics_df = pd.DataFrame(baseline.metrics).T\n",
    "metrics_df = metrics_df.round(4)\n",
    "display(metrics_df)\n",
    "\n",
    "# Visualize\n",
    "print(\"\\nüé® Creating comparison visualizations...\")\n",
    "fig = baseline.plot_baseline_comparison(baseline_results, y_test_original)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Baseline Insights:\")\n",
    "print(\"=\"*70)\n",
    "print(\"   ‚Ä¢ Naive forecast is often surprisingly good for financial data\")\n",
    "print(\"   ‚Ä¢ Moving average smooths out noise but lags behind trends\")\n",
    "print(\"   ‚Ä¢ Linear regression captures overall trend but misses non-linearities\")\n",
    "print(\"   ‚Ä¢ These are the benchmarks our LSTM must beat!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best baseline\n",
    "best_baseline = min(baseline.metrics.items(), key=lambda x: x[1]['RMSE'])\n",
    "print(f\"\\nüèÜ Best Baseline: {best_baseline[0]}\")\n",
    "print(f\"   ‚Ä¢ RMSE: ${best_baseline[1]['RMSE']:.2f}\")\n",
    "print(f\"\\nüëâ Our LSTM needs to beat this to be worthwhile!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lstm-theory",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Building and Training LSTM Models\n",
    "\n",
    "## üèóÔ∏è LSTM Architecture Design\n",
    "\n",
    "### Key Decisions When Building LSTMs:\n",
    "\n",
    "1. **Number of LSTM Layers**: \n",
    "   - Single layer: Simple patterns\n",
    "   - Multiple layers: Complex hierarchical patterns\n",
    "   - More layers ‚â† always better (overfitting risk)\n",
    "\n",
    "2. **Number of Units (Neurons)**:\n",
    "   - Too few: Underfitting (can't capture patterns)\n",
    "   - Too many: Overfitting (memorizes training data)\n",
    "   - Common choices: 50-200 for financial data\n",
    "\n",
    "3. **Dropout Regularization**:\n",
    "   - Randomly \"drops\" neurons during training\n",
    "   - Prevents overfitting\n",
    "   - Common values: 0.2-0.3 (20-30%)\n",
    "\n",
    "4. **Return Sequences**:\n",
    "   - True: Output at each timestep (for stacked LSTMs)\n",
    "   - False: Output only at end (for prediction)\n",
    "\n",
    "---\n",
    "\n",
    "## üé® Our LSTM Architecture\n",
    "\n",
    "We'll build two models for comparison:\n",
    "\n",
    "### Model 1: Simple LSTM\n",
    "```\n",
    "Input (30 timesteps, 1 feature)\n",
    "    ‚Üì\n",
    "LSTM Layer (50 units)\n",
    "    ‚Üì\n",
    "Dropout (20%)\n",
    "    ‚Üì\n",
    "Dense Layer (1 unit)\n",
    "    ‚Üì\n",
    "Output (price prediction)\n",
    "```\n",
    "\n",
    "### Model 2: Deep LSTM (Stacked)\n",
    "```\n",
    "Input (30 timesteps, 1 feature)\n",
    "    ‚Üì\n",
    "LSTM Layer 1 (50 units, return_sequences=True)\n",
    "    ‚Üì\n",
    "Dropout (20%)\n",
    "    ‚Üì\n",
    "LSTM Layer 2 (50 units)\n",
    "    ‚Üì\n",
    "Dropout (20%)\n",
    "    ‚Üì\n",
    "Dense Layer (1 unit)\n",
    "    ‚Üì\n",
    "Output (price prediction)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Training Configuration\n",
    "\n",
    "### Loss Function: Mean Squared Error (MSE)\n",
    "- Standard for regression problems\n",
    "- Penalizes large errors\n",
    "\n",
    "### Optimizer: Adam\n",
    "- Adaptive learning rate\n",
    "- Works well for LSTMs\n",
    "- Learning rate: 0.001 (default)\n",
    "\n",
    "### Callbacks:\n",
    "1. **Early Stopping**: Stop if validation loss stops improving\n",
    "   - Patience: Wait 15 epochs before stopping\n",
    "   - Prevents overfitting\n",
    "\n",
    "2. **Learning Rate Reduction**: Reduce LR when stuck\n",
    "   - Factor: 0.5 (halve the learning rate)\n",
    "   - Patience: 10 epochs\n",
    "   - Helps fine-tune towards end of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModels:\n",
    "    \"\"\"\n",
    "    Build, train, and evaluate LSTM models for time series forecasting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.histories = {}\n",
    "        self.predictions = {}\n",
    "    \n",
    "    def build_simple_lstm(self, input_shape, units=50, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Build a simple LSTM model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_shape : tuple\n",
    "            (timesteps, features)\n",
    "        units : int\n",
    "            Number of LSTM units\n",
    "        dropout : float\n",
    "            Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(units=units, input_shape=input_shape, name='LSTM_Layer'),\n",
    "            Dropout(dropout, name='Dropout'),\n",
    "            Dense(1, name='Output_Layer')\n",
    "        ], name='Simple_LSTM')\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_deep_lstm(self, input_shape, units=50, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Build a stacked (deep) LSTM model.\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(units=units, return_sequences=True, input_shape=input_shape, \n",
    "                 name='LSTM_Layer_1'),\n",
    "            Dropout(dropout, name='Dropout_1'),\n",
    "            LSTM(units=units, name='LSTM_Layer_2'),\n",
    "            Dropout(dropout, name='Dropout_2'),\n",
    "            Dense(1, name='Output_Layer')\n",
    "        ], name='Deep_LSTM')\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, model, X_train, y_train, X_val, y_val, \n",
    "                   epochs=100, batch_size=32, verbose=1):\n",
    "        \"\"\"\n",
    "        Train LSTM model with callbacks.\n",
    "        \"\"\"\n",
    "        # Callbacks\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=0.00001,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def plot_training_history(self, history, model_name, figsize=(14, 5)):\n",
    "        \"\"\"\n",
    "        Plot training and validation loss curves.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Loss\n",
    "        axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "        axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "        axes[0].set_title(f'{model_name}: Loss Curves', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss (MSE)')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: MAE\n",
    "        axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "        axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "        axes[1].set_title(f'{model_name}: MAE Curves', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('MAE')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test, scaler, model_name):\n",
    "        \"\"\"\n",
    "        Evaluate model and calculate metrics.\n",
    "        \"\"\"\n",
    "        # Predict\n",
    "        y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "        y_true = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        metrics = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'R2': r2\n",
    "        }\n",
    "        \n",
    "        self.predictions[model_name] = y_pred\n",
    "        \n",
    "        return y_pred, y_true, metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: BUILDING AND TRAINING LSTM MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize\n",
    "lstm_models = LSTMModels()\n",
    "\n",
    "# Extract data\n",
    "X_train = data_dict['X_train']\n",
    "y_train = data_dict['y_train']\n",
    "X_val = data_dict['X_val']\n",
    "y_val = data_dict['y_val']\n",
    "X_test = data_dict['X_test']\n",
    "y_test = data_dict['y_test']\n",
    "scaler = data_dict['scaler']\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (timesteps, features)\n",
    "\n",
    "print(f\"\\nüìä Input Shape: {input_shape}\")\n",
    "print(f\"   ‚Ä¢ Timesteps (look-back): {input_shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Features: {input_shape[1]}\")\n",
    "\n",
    "# ============= Model 1: Simple LSTM =============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 1: SIMPLE LSTM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Building Simple LSTM...\")\n",
    "simple_lstm = lstm_models.build_simple_lstm(input_shape, units=50, dropout=0.2)\n",
    "\n",
    "print(\"\\nüìã Model Architecture:\")\n",
    "simple_lstm.summary()\n",
    "\n",
    "print(\"\\nüéì Training Simple LSTM...\")\n",
    "print(\"-\" * 70)\n",
    "history_simple = lstm_models.train_model(\n",
    "    simple_lstm, X_train, y_train, X_val, y_val,\n",
    "    epochs=100, batch_size=32, verbose=1\n",
    ")\n",
    "lstm_models.histories['Simple_LSTM'] = history_simple\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"   ‚Ä¢ Total epochs: {len(history_simple.history['loss'])}\")\n",
    "print(f\"   ‚Ä¢ Final training loss: {history_simple.history['loss'][-1]:.6f}\")\n",
    "print(f\"   ‚Ä¢ Final validation loss: {history_simple.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nüé® Plotting training history...\")\n",
    "fig = lstm_models.plot_training_history(history_simple, 'Simple LSTM')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nüìä Evaluating on Test Set...\")\n",
    "pred_simple, y_true, metrics_simple = lstm_models.evaluate_model(\n",
    "    simple_lstm, X_test, y_test, scaler, 'Simple_LSTM'\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Test Set Performance:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   ‚Ä¢ MAE:  ${metrics_simple['MAE']:.2f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: ${metrics_simple['RMSE']:.2f}\")\n",
    "print(f\"   ‚Ä¢ MAPE: {metrics_simple['MAPE']:.2f}%\")\n",
    "print(f\"   ‚Ä¢ R¬≤:   {metrics_simple['R2']:.4f}\")\n",
    "\n",
    "# ============= Model 2: Deep LSTM =============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 2: DEEP LSTM (STACKED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Building Deep LSTM...\")\n",
    "deep_lstm = lstm_models.build_deep_lstm(input_shape, units=50, dropout=0.2)\n",
    "\n",
    "print(\"\\nüìã Model Architecture:\")\n",
    "deep_lstm.summary()\n",
    "\n",
    "print(\"\\nüéì Training Deep LSTM...\")\n",
    "print(\"-\" * 70)\n",
    "history_deep = lstm_models.train_model(\n",
    "    deep_lstm, X_train, y_train, X_val, y_val,\n",
    "    epochs=100, batch_size=32, verbose=1\n",
    ")\n",
    "lstm_models.histories['Deep_LSTM'] = history_deep\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"   ‚Ä¢ Total epochs: {len(history_deep.history['loss'])}\")\n",
    "print(f\"   ‚Ä¢ Final training loss: {history_deep.history['loss'][-1]:.6f}\")\n",
    "print(f\"   ‚Ä¢ Final validation loss: {history_deep.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nüé® Plotting training history...\")\n",
    "fig = lstm_models.plot_training_history(history_deep, 'Deep LSTM')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nüìä Evaluating on Test Set...\")\n",
    "pred_deep, _, metrics_deep = lstm_models.evaluate_model(\n",
    "    deep_lstm, X_test, y_test, scaler, 'Deep_LSTM'\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Test Set Performance:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   ‚Ä¢ MAE:  ${metrics_deep['MAE']:.2f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: ${metrics_deep['RMSE']:.2f}\")\n",
    "print(f\"   ‚Ä¢ MAPE: {metrics_deep['MAPE']:.2f}%\")\n",
    "print(f\"   ‚Ä¢ R¬≤:   {metrics_deep['R2']:.4f}\")\n",
    "\n",
    "# Store models\n",
    "lstm_models.models['Simple_LSTM'] = simple_lstm\n",
    "lstm_models.models['Deep_LSTM'] = deep_lstm\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ALL LSTM MODELS TRAINED AND EVALUATED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-theory",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Performance Comparison & Analysis\n",
    "\n",
    "## üéØ The Million Dollar Question:\n",
    "\n",
    "**\"Is the LSTM worth the complexity?\"**\n",
    "\n",
    "We need to compare:\n",
    "1. Baseline models (simple, interpretable)\n",
    "2. LSTM models (complex, powerful)\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "‚úÖ **LSTM adds value if:**\n",
    "- Significantly lower RMSE/MAE than baselines\n",
    "- Higher R¬≤ score\n",
    "- Captures patterns baselines miss\n",
    "- Performance improvement justifies training time\n",
    "\n",
    "‚ùå **LSTM may not be worth it if:**\n",
    "- Only marginally better than naive forecast\n",
    "- Overfits (great on train, poor on test)\n",
    "- Too computationally expensive\n",
    "- Can't be deployed in production\n",
    "\n",
    "### Key Metrics for Comparison:\n",
    "\n",
    "1. **Absolute Performance**: Which model has lowest error?\n",
    "2. **Relative Improvement**: How much better is LSTM?\n",
    "3. **Consistency**: Does LSTM beat baselines on all metrics?\n",
    "4. **Practical Significance**: Is the improvement meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: COMPREHENSIVE PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine all metrics\n",
    "all_metrics = baseline.metrics.copy()\n",
    "all_metrics['Simple_LSTM'] = metrics_simple\n",
    "all_metrics['Deep_LSTM'] = metrics_deep\n",
    "\n",
    "# Create comprehensive comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_metrics).T\n",
    "comparison_df = comparison_df[['MAE', 'RMSE', 'MAPE', 'R2']]  # Reorder columns\n",
    "\n",
    "print(\"\\nüìä COMPLETE PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nüèÜ BEST PERFORMERS BY METRIC\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_mae = comparison_df['MAE'].idxmin()\n",
    "best_rmse = comparison_df['RMSE'].idxmin()\n",
    "best_mape = comparison_df['MAPE'].idxmin()\n",
    "best_r2 = comparison_df['R2'].idxmax()\n",
    "\n",
    "print(f\"   ‚Ä¢ Lowest MAE:  {best_mae} (${comparison_df.loc[best_mae, 'MAE']:.2f})\")\n",
    "print(f\"   ‚Ä¢ Lowest RMSE: {best_rmse} (${comparison_df.loc[best_rmse, 'RMSE']:.2f})\")\n",
    "print(f\"   ‚Ä¢ Lowest MAPE: {best_mape} ({comparison_df.loc[best_mape, 'MAPE']:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Highest R¬≤:  {best_r2} ({comparison_df.loc[best_r2, 'R2']:.4f})\")\n",
    "\n",
    "# Calculate improvement over best baseline\n",
    "print(\"\\nüìà LSTM IMPROVEMENT OVER BEST BASELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_models = ['Naive', 'MovingAverage', 'LinearRegression']\n",
    "best_baseline_rmse = comparison_df.loc[baseline_models, 'RMSE'].min()\n",
    "best_baseline_name = comparison_df.loc[baseline_models, 'RMSE'].idxmin()\n",
    "\n",
    "simple_lstm_improvement = ((best_baseline_rmse - metrics_simple['RMSE']) / best_baseline_rmse) * 100\n",
    "deep_lstm_improvement = ((best_baseline_rmse - metrics_deep['RMSE']) / best_baseline_rmse) * 100\n",
    "\n",
    "print(f\"\\nBest Baseline: {best_baseline_name} (RMSE: ${best_baseline_rmse:.2f})\")\n",
    "print(f\"\\nSimple LSTM:\")\n",
    "print(f\"   ‚Ä¢ RMSE: ${metrics_simple['RMSE']:.2f}\")\n",
    "print(f\"   ‚Ä¢ Improvement: {simple_lstm_improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\nDeep LSTM:\")\n",
    "print(f\"   ‚Ä¢ RMSE: ${metrics_deep['RMSE']:.2f}\")\n",
    "print(f\"   ‚Ä¢ Improvement: {deep_lstm_improvement:+.2f}%\")\n",
    "\n",
    "# Visualize comparison\n",
    "print(\"\\nüé® Creating comprehensive visualizations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: RMSE Comparison (Bar Chart)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "colors = ['steelblue', 'steelblue', 'steelblue', 'coral', 'darkred']\n",
    "bars = ax1.bar(comparison_df.index, comparison_df['RMSE'], color=colors, alpha=0.7)\n",
    "ax1.set_title('RMSE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('RMSE ($)')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Highlight best\n",
    "best_idx = comparison_df['RMSE'].argmin()\n",
    "bars[best_idx].set_edgecolor('gold')\n",
    "bars[best_idx].set_linewidth(3)\n",
    "\n",
    "# Plot 2: MAE Comparison (Bar Chart)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "bars = ax2.bar(comparison_df.index, comparison_df['MAE'], color=colors, alpha=0.7)\n",
    "ax2.set_title('MAE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('MAE ($)')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plot 3: R¬≤ Comparison (Bar Chart)\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "bars = ax3.bar(comparison_df.index, comparison_df['R2'], color=colors, alpha=0.7)\n",
    "ax3.set_title('R¬≤ Score Comparison (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('R¬≤ Score')\n",
    "ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plot 4: MAPE Comparison (Bar Chart)\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "bars = ax4.bar(comparison_df.index, comparison_df['MAPE'], color=colors, alpha=0.7)\n",
    "ax4.set_title('MAPE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('MAPE (%)')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plot 5: Predictions vs Actual (First 100 points)\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "plot_range = slice(0, 100)\n",
    "\n",
    "ax5.plot(y_true[plot_range], label='Actual', linewidth=2.5, color='black', alpha=0.8)\n",
    "ax5.plot(baseline_results['Naive'][plot_range], label='Naive', linewidth=1.5, alpha=0.6)\n",
    "ax5.plot(pred_simple[plot_range], label='Simple LSTM', linewidth=1.5, alpha=0.7)\n",
    "ax5.plot(pred_deep[plot_range], label='Deep LSTM', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax5.set_title('Predictions vs Actual (First 100 Test Points)', fontsize=12, fontweight='bold')\n",
    "ax5.set_xlabel('Time Step')\n",
    "ax5.set_ylabel('Price ($)')\n",
    "ax5.legend(loc='best', fontsize=10)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comprehensive Model Performance Comparison', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéì FINAL VERDICT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if simple_lstm_improvement > 5 or deep_lstm_improvement > 5:\n",
    "    print(\"\\n‚úÖ LSTM ADDS SIGNIFICANT VALUE\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"   ‚Ä¢ LSTM models show >5% improvement over best baseline\")\n",
    "    print(f\"   ‚Ä¢ Deep learning is justified for this problem\")\n",
    "    print(f\"   ‚Ä¢ Recommendation: Deploy LSTM in production\")\n",
    "elif simple_lstm_improvement > 0 or deep_lstm_improvement > 0:\n",
    "    print(\"\\n‚ö†Ô∏è  LSTM ADDS MODERATE VALUE\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"   ‚Ä¢ LSTM models show modest improvement\")\n",
    "    print(f\"   ‚Ä¢ Consider trade-off: complexity vs. performance\")\n",
    "    print(f\"   ‚Ä¢ Recommendation: Use LSTM if resources permit\")\n",
    "else:\n",
    "    print(\"\\n‚ùå LSTM DOES NOT ADD VALUE\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"   ‚Ä¢ LSTM doesn't outperform simple baselines\")\n",
    "    print(f\"   ‚Ä¢ Stick with simpler models (Naive, Linear Regression)\")\n",
    "    print(f\"   ‚Ä¢ Recommendation: More data or better features needed\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"=\"*70)\n",
    "print(\"   1. Always establish strong baselines before celebrating LSTM results\")\n",
    "print(\"   2. Small improvements may not justify deployment complexity\")\n",
    "print(\"   3. Deep LSTM vs Simple LSTM trade-off: performance vs. complexity\")\n",
    "print(\"   4. Consider your business context: How valuable is each % improvement?\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Week 8 Summary: What We Learned\n",
    "\n",
    "## Key Concepts Covered:\n",
    "\n",
    "### 1. Sequential Data & RNNs\n",
    "- ‚úÖ Financial data has temporal dependencies\n",
    "- ‚úÖ Traditional ML ignores sequence order\n",
    "- ‚úÖ RNNs maintain \"memory\" through hidden states\n",
    "- ‚úÖ Vanishing gradient problem limits basic RNNs\n",
    "\n",
    "### 2. LSTM Architecture\n",
    "- ‚úÖ Gates control information flow\n",
    "- ‚úÖ Cell state acts as long-term memory\n",
    "- ‚úÖ Solves vanishing gradient problem\n",
    "- ‚úÖ Can capture patterns across long time horizons\n",
    "\n",
    "### 3. Practical Implementation\n",
    "- ‚úÖ Sequence creation with look-back windows\n",
    "- ‚úÖ Proper time series train/val/test split\n",
    "- ‚úÖ Scaling data without leakage\n",
    "- ‚úÖ Building models with Keras/TensorFlow\n",
    "\n",
    "### 4. Model Evaluation\n",
    "- ‚úÖ Baseline models are ESSENTIAL\n",
    "- ‚úÖ Multiple metrics provide different insights\n",
    "- ‚úÖ Performance improvement must justify complexity\n",
    "- ‚úÖ Visualizations help understand model behavior\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps & Extensions\n",
    "\n",
    "**Try these on your own:**\n",
    "\n",
    "1. **Add More Features**: \n",
    "   - Include volume, volatility, moving averages\n",
    "   - Use multiple cryptocurrencies (multivariate LSTM)\n",
    "\n",
    "2. **Experiment with Architecture**:\n",
    "   - Try different numbers of LSTM units\n",
    "   - Adjust dropout rates\n",
    "   - Add more layers or Dense layers\n",
    "\n",
    "3. **Alternative Models**:\n",
    "   - Try GRU (simpler alternative to LSTM)\n",
    "   - Explore bidirectional LSTMs\n",
    "   - Test Transformer models (attention mechanism)\n",
    "\n",
    "4. **Production Considerations**:\n",
    "   - Model saving and loading\n",
    "   - Real-time prediction pipeline\n",
    "   - Monitoring model drift\n",
    "   - Retraining strategies\n",
    "\n",
    "5. **Advanced Topics**:\n",
    "   - Sequence-to-sequence models\n",
    "   - Multi-step ahead forecasting\n",
    "   - Uncertainty quantification\n",
    "   - Attention mechanisms\n",
    "\n",
    "---\n",
    "\n",
    "## üíº Real-World Applications\n",
    "\n",
    "**Where LSTMs excel in Finance:**\n",
    "\n",
    "1. **Trading Strategies**: Generate buy/sell signals from patterns\n",
    "2. **Risk Management**: Forecast volatility and Value-at-Risk\n",
    "3. **Portfolio Optimization**: Dynamic allocation based on predicted returns\n",
    "4. **Sentiment Analysis**: Process sequential text data (news, tweets)\n",
    "5. **Fraud Detection**: Identify unusual transaction sequences\n",
    "6. **Credit Scoring**: Analyze payment history patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Final Thoughts\n",
    "\n",
    "**Remember:**\n",
    "- üß† LSTMs are powerful but not magic\n",
    "- ‚öñÔ∏è  Always compare against simple baselines\n",
    "- üìä More data usually helps more than complex architecture\n",
    "- üîç Understand WHY your model works, not just THAT it works\n",
    "- üí° In finance, consistent small improvements compound over time\n",
    "\n",
    "**The real skill** is knowing when to use deep learning and when simpler methods suffice!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Further Reading\n",
    "\n",
    "- Original LSTM Paper: Hochreiter & Schmidhuber (1997)\n",
    "- \"Understanding LSTM Networks\" by Chris Olah\n",
    "- \"Deep Learning\" by Goodfellow, Bengio, Courville\n",
    "- \"Advances in Financial Machine Learning\" by Marcos L√≥pez de Prado\n",
    "\n",
    "---\n",
    "\n",
    "# üéâ Congratulations!\n",
    "\n",
    "You've completed Week 8 and learned how to apply state-of-the-art deep learning to financial time series!\n",
    "\n",
    "**Keep practicing, keep learning, and keep building! üöÄ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save your trained models\n",
    "print(\"\\nüíæ Saving trained models...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save models\n",
    "lstm_models.models['Simple_LSTM'].save('simple_lstm_model.h5')\n",
    "lstm_models.models['Deep_LSTM'].save('deep_lstm_model.h5')\n",
    "\n",
    "print(\"‚úÖ Models saved successfully!\")\n",
    "print(\"   ‚Ä¢ simple_lstm_model.h5\")\n",
    "print(\"   ‚Ä¢ deep_lstm_model.h5\")\n",
    "print(\"\\nüí° You can load these models later with: model = keras.models.load_model('model_name.h5')\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüéä WEEK 8 COMPLETE! üéä\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Thank you for learning with us!\")\n",
    "print(\"Next week: Advanced Topics in Deep Learning for Finance\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
